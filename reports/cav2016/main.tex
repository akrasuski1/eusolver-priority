\documentclass{llncs}

\sloppy

\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{alltt}
\usepackage{paralist}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{mdwtab}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[square,numbers,sort&compress,comma,sectionbib]{natbib}
\renewcommand{\bibname}{References}
\captionsetup{compatibility=false}

\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\newcommand\comment[1]{}
\newcommand\arsays[1]{{\bf AR: #1}}
\newcommand\ausays[1]{{\bf AU: #1}}

\newcommand\Integers{\mathbb{Z}}
\newcommand\tuple[1]{\langle #1 \rangle}
\newcommand\True{\mathit{True}}
\newcommand\None{\mathit{None}}
\newcommand\Points{\mathit{Pts}}
\newcommand\Point{\mathit{Pt}}
\newcommand\Verify{\mathit{Verify}}
\newcommand\CexInput{\mathit{CexPt}}
\newcommand\Predicates{\mathit{Preds}}
\newcommand\Expr{e}
\newcommand\Pred{p}
\newcommand\Terms{\mathit{Terms}}
\newcommand\Term{t}
\newcommand\Cover{\mathit{Cover}}
\newcommand\Powerset[1]{\mathbf{2}^{#1}}
\newcommand\Spec{\Phi}
\newcommand\Size{K}
\newcommand\Grammar{G}
\newcommand\sem[1]{[\![ #1 ]\!]}
\newcommand\SynthFun{f}
\newcommand\range{\mathit{range}}
\newcommand\FormalParameters{\mathit{Params}}
\newcommand\Productions{\mathit{Prodn}}
\newcommand\Prob[1]{\mathbb{P}(#1)}
\newcommand\NonTerminals{\mathcal{N}}
\newcommand\NonTerminal{N}
\newcommand\StartSymbol{S}
\newcommand\Symbols{\mathit{Symbols}}
\newcommand\Rules{\mathcal{R}}
\newcommand\Rule{R}
\newcommand\Theory{\mathcal{T}}
\newcommand\RewritesTo{\rightarrow}
\newcommand\ITE[3]{\mathtt{if}~#1~\mathtt{then}~#2~\mathtt{else}~#3}

\newcommand\DecisionTree{\mathit{DT}}
\newcommand\DTtoExpr[1]{\mathit{Expr}(#1)}
\newcommand\NodesInternal{V_I}
\newcommand\Nodes{V}
\newcommand\node{v}
\newcommand\NodesLeaf{V_L}
\newcommand\EdgesYes{E_Y}
\newcommand\EdgesNo{E_N}
\newcommand\Edges{E}
\newcommand\Attribute{\mathcal{A}}
\newcommand\Label{\mathcal{L}}
\newcommand{\sygus}{{\sffamily\fontsize{8.5}{10}\selectfont SyGuS}\xspace}

% Make floats and equations behave sensibly
\setlength{\intextsep}{1pt}
\setlength{\textfloatsep}{1pt}
\setlength{\floatsep}{1pt}
\abovedisplayskip=6pt plus 3pt minus 9pt
\abovedisplayshortskip=0pt plus 3pt
\belowdisplayskip=6pt plus 3pt minus 9pt
\belowdisplayshortskip=7pt plus 3pt minus 4pt
\setlength\abovecaptionskip{1pt}
\setlength\belowcaptionskip{1pt}

\begin{document}

\title{Scaling Enumerative Program Synthesis via Divide and Conquer}
% \author{Rajeev Alur \and Arjun Radhakrishna \and Abhishek Udupa}
\author{}
% \institute{University of Pennsylvania}
\institute{}
\maketitle
\vspace*{-6ex}

\begin{abstract}
  Given a semantic constraint specified by a logical formula and
  syntactic constraint specified by a context-free grammar, the
  Syntax-Guided Synthesis (SyGuS) problem is to find an expression
  that satisfies both the syntactic and semantic constraints.
  An enumerative approach to solve this problem is to systematically
  generates all expressions from the syntactic space with some pruning,
  and has proved to be surprisingly competitive in the newly started
  competition of SyGuS solvers.  It performs well on small to medium sized
  benchmarks, produces succinct expressions, and has the ability to
  generalize from input-output examples.  However, its performance
  degrades drastically with the size of the smallest solution. To overcome
  this limitation, in this paper we propose an alternative approach to
  solve SyGuS instances.

  The key idea  is to employ a divide-and-conquer approach by
  separately enumerating (a) smaller expressions that are correct on
  subsets of inputs, and (b) predicates on inputs that distinguish these
  subsets.  These smaller expressions and predicates are then combined
  using decision trees to obtain an expression that is correct on all
  inputs.  We view the problem of combining expressions and predicates as
  a multi-label decision tree learning problem. We propose a novel
  technique of associating a probability distribution over the set of
  labels that a sample can be labeled with. This enables us to use
  standard information-gain based heuristics to learn a compact decision
  tree.

  We report a prototype implementation and evaluate it on the benchmarks
  from the SyGuS 2015 competition. Our tool is able to match the running
  times and the succinctness of solutions of both standard enumerative
  solver as well as the latest white-box solvers in most cases.  Further,
  our solver is able to solve a large number of instances from the ICFP
  class of benchmarks, which were previously unsolved by all existing
  solvers.
\end{abstract}

\arsays{Change main algo to check for distinctness on points}

\section{Introduction}
\label{sec:intro}

\section{Illustrative Examples}
\label{sec:example}

\arsays{Make an example where all pieces are generated at a small size,
but the full expression takes much longer}


\section{Problem Statement and Background}
\label{sec:problem}

Let us fix the function to be synthesized $\SynthFun$ and its formal
parameters $\FormalParameters$.
We write $\range(\SynthFun)$ to denote the range of $\SynthFun$.
The term {\em point} denotes a valuation of $\FormalParameters$, i.e., a
point is an input to $\SynthFun$.

\begin{example}
  For the running example in this section, we consider a function to be
  synthesized $\SynthFun$ of type $\Integers \times \Integers \to
  \Integers$ with the formal parameters $\FormalParameters = \{ x, y \}$.
  Points are valuations of $x$ and $y$. 
  For example, $\{ x \mapsto 1, y \mapsto 2 \}$ is a point.
\end{example}

\paragraph{Specifications}
Satisfiability modulo theory formulae have become the standard formalism
for specifying semantic constraints for synthesis.
In the rest of this paper, we fix an arbitrary theory $\Theory$ and 
denote by $\Theory[\Symbols]$, the set of $\Theory$ terms over the
symbols $\Symbols$.
A {\em specification} $\Spec$ is a logical formula in a theory $\Theory$
over standard theory symbols and the function to be synthesized
$\SynthFun$.
We say that an expression $\Expr$ satisfies a specification $\Spec$ if
instantiating the function to be synthesized $\SynthFun$ by $\Expr$
makes the formula $\Spec$ valid.

\begin{example}
  \label{ex:running:spec}
  Continuing the running example, we define a specification $\Spec
  \equiv \forall x, y : \SynthFun(x, y) \geq x \wedge \SynthFun(x, y)
  \geq y \wedge f(x, y) = x \vee f(x, y) = y$.
  The specification states that the function to be synthesized
  $\SynthFun$ maps each pair $x$ and $y$ to a value that is at least as
  great as each of them and equal to one of them.
  The function that returns the maximum of the two input values is the
  only possible function that satisfies this specification.
\end{example}

\paragraph{Grammars}
An {\em expression grammar} $\Grammar$ is a tuple $\tuple {
\NonTerminals, \StartSymbol, \Rules }$ where:
\begin{inparaenum}[(a)]
\item the set $\NonTerminals$ is a set of non-terminal symbols,
\item the non-terminal $\StartSymbol \in \NonTerminals$ is the initial non-terminal,
\item $\Rules \subseteq \NonTerminals \times
  \Theory[\NonTerminals \cup \FormalParameters]$ is a finite set
  of rewrite rules that map $\NonTerminals$ to $\Theory$-expressions
  over non-terminals and formal parameters.
\end{inparaenum}
We say that an expression $\Expr$ {\em rewrites to} an incomplete
expression $\Expr'$ (written as $\Expr \RewritesTo_\Grammar \Expr'$) if
there exists a rule $\Rule = (\NonTerminal, \Expr'') \in \Rules$ and
$\Expr'$ is obtained by replacing one occurrence of $\NonTerminal$ in
$\Expr$ by $\Expr''$.
Let $\RewritesTo_\Grammar^*$ be the transitive closure of $\RewritesTo$.
We say that an expression $\Expr \in \Theory[\FormalParameters]$
is {\em generated} by the grammar $\Grammar$ (written as $\Expr \in
\sem{\Grammar}$) if $\StartSymbol \RewritesTo_\Grammar^* \Expr$.
Note that we implicitly assume that all terms generated by the grammar
have the right type, i.e., are of the type $\range(\SynthFun)$.

\begin{example}
  \label{ex:running:grammar}
  For the running example, we choose the following grammar.
  The non-terminals are given by $\NonTerminals = \{ \StartSymbol, T, C \}$ with
  the initial non-terminal bein $\StartSymbol$.
  The rules are $\{ (\StartSymbol, T), (\StartSymbol,
        \mathtt{if}~C~\mathtt{then}~\StartSymbol~\mathtt{else}~\StartSymbol)
        \} \cup \{ (T, x), (T, y), (T, 1), (T, - T), (T, T + T) \} \cup
        \{ (C, T \leq T), (C, C \wedge C), (C, \neg C) \}$.
  This is the standard linear integer arithmetic grammar used for many
  SyGuS problems.
  Written in standard BCNF notation, this grammar is equivalent to:
    \vspace{-1ex}
  \begin{alltt}
    S ::= T | if (C) then T else T
    T ::= x | y | 1 | T + T | - T
    C ::= T \(\leq\) T | T < T | C \(\wedge\) C | \(\neg\) C
  \end{alltt}
    \vspace{-4ex}
\end{example}

\paragraph{The Syntax-Guided Synthesis Problem}
An instance of the SyGuS problem is given by a pair $\tuple { \Spec,
\Grammar }$ of specification and grammar.
An expression $\Expr$ is a solution to the instance if $\Expr \models
\Spec$ and $\Expr \in \sem{\Grammar}$.

\begin{example}
  Continuing the running example, for the specification $\Spec$ from
  Example~\ref{ex:running:spec} and the grammar from
  Example~\ref{ex:running:grammar}, one of the solution expressions is
  given by $\SynthFun(x, y) \equiv \mathtt{if}~x > y~\mathtt{then}~x~\mathtt{else}~y$.
\end{example}

From our definitions, it is clear that we restrict ourselves to a
version of the SyGuS problem where there is exactly one unknown function
to be synthesized, and the grammar does not contain {\tt let} rules.
Further, we assume that our specifications are {\em
  point-wise}\footnote{For brief discussion on different syntactic and
semantic notions of point-wise specifications, see the appendix}.
Intuitively, if a specification is point-wise, if it just relates the
input point to its output, and not the outputs of different inputs.

Here, we use a simple syntactic notion of point-wise specifications for
convenience.
However, our techniques can be generalized to any notion of point-wise
specifications.
Formally, we say that a specification is {\em plainly separable} if can
be rewritten into a conjunctive normal form where each clause is either
\begin{inparaenum}[(a)]
\item a tautology, or
\item every appearing application of the function to be synthesized
  $\SynthFun$ have the same arguments.
\end{inparaenum}

\begin{example}
  The specification for our running example $\Spec \equiv \SynthFun(x,
  y) \geq x \wedge \SynthFun(x, y) \geq y \wedge \SynthFun(x, y) = x
  \vee \SynthFun(x, y) = y$ is plainly separable.
  For example, this implies that the value of $\SynthFun(1, 2)$ can be
  chosen irrespective of the value of $\SynthFun$ on any other point.
  On the other hand, a specification such as $\SynthFun(x, y) = 1
  \implies \SynthFun(x + 1, y) = 1$ is neither plainly separable nor
  point-wise.
  The value of $\SynthFun(1, 2)$ cannot be chosen independently of the
  value of $\SynthFun(0, 2)$.
\end{example}

Point-wise (and in turn, simply separable) specifications allow us to
define the notion of an expression $\Expr$ satisfying a specification
$\Spec$ on a point $\Point$.
\arsays{Formally, we say that $\Expr \models \Spec \downharpoonleft
\Point$ if \dots}
We extend this definition to sets of points as follows: $\Expr \models
\Spec \downharpoonleft \Points \Leftrightarrow \bigwedge_{\Point \in
\Points} \Expr \models \Spec\downharpoonleft\Point$.

\begin{example}
  \label{ex:running:correctness_on_point}
  For the specification $\Spec$ of the running example, the function
  given by $\SynthFun(x, y) \equiv x + y$ is correct on the point $\{ x
      \mapsto 0, y \mapsto 3 \}$ and incorrect on the point $\{ x
  \mapsto 1, y \mapsto 2 \}$
\end{example}

The above restrictions make the SyGuS problem significantly easier.
However, a large number of problems do fall into this class (\arsays{x
out of y benchmarks in the SyGuS 2015 competition}).
Several previous works address this class of problem (see, for
example, \cite{ACR15,Madhu,xxx}).
The following example shows that a number of commonly occurring
specification mores are point-wise specifications.

\subsection{The Enumerative Solver}
\label{sec:enumeration}

\begin{wrapfigure}{l}{0.5\textwidth}
  \begin{minipage}{0.5\textwidth}
    \begin{algorithm}[H]
      \begin{algorithmic}[1]
        \Require Grammar $\Grammar = \tuple { \NonTerminals, \StartSymbol, \Rules }$
        \Require Specification $\Spec$
        \Ensure $\Expr$ s.t.  $\Expr \in \sem{\Grammar} \wedge \Expr \models \Spec$
        \State $\Point \gets \emptyset$ \label{line:basic:init}
        \While { $\True$ }
        \For {$\Expr \in \Call{Enumerate}{\Grammar,\Points}$ }\label{line:basic:enumerate}
        \If { $\Expr \models \Spec \downharpoonleft  \Points$ } \textbf{continue} \EndIf\label{line:basic:concrete_check}
        \State $\CexInput \gets \Verify(\Expr, \Spec)$ \label{line:basic:verify}
        \If { $\CexInput = \bot$ } \Return $\Expr$ \EndIf \label{line:basic:return}
        \State $\Points \gets \Points \cup \CexInput$ \label{line:basic:continue}
        \EndFor
        \EndWhile
      \end{algorithmic}
      \caption{Enumerative Solver}
      \label{algo:basic}
    \end{algorithm}
  \end{minipage}
\end{wrapfigure}
The principal idea behind the enumerative solver is to enumerate all
expressions from the given syntax with some pruning.
Only expressions that are distinct with respect to a set of concrete
input points are enumerated.

The full pseudo-code is given in Algorithm~\ref{algo:basic}.
Initially, the set of points is empty (line~\ref{line:basic:init}).
In each iteration, the algorithm calls the {\tt Enumerate} procedure
which returns a (possibly infinite) list of expressions that are all
distinct with respect to $\Points$ (line~\ref{line:basic:enumerate}).
This $\Expr$ is then verified, first on the set of points
(line~\ref{line:basic:concrete_check}) and then fully
(line~\ref{line:basic:verify}).
If the expression $\Expr$ is correct, it is returned
(line~\ref{line:basic:return}).
Otherwise, we pick a counter-example input point (i.e., an input on
which $\Expr$ is incorrect) and add it to the set of points and repeat
(line~\ref{line:basic:continue}).

For completeness, we recall the {\tt Enumerate} procedure here
(Algorithm~\ref{algo:enumerate}).
It maintains a map $\Productions : \NonTerminals \to
\Powerset{\Theory[\FormalParameters]}$ from non-terminals to
expressions they can produce.
The invariant maintained by the procedure is that every pair of
expressions in $\Productions[\NonTerminal]$ is distinct on $\Points$.

The algorithm starts by first accumulating into
$\Productions[\NonTerminal]$ the expressions that can be produced from
$\NonTerminal$ in one step
(lines~\ref{line:enumerate:level_one_iter}-\ref{line:enumerate:level_one}).
Then, for each possible expression size $\Size$, it attempts to
instantiate each production rule in the grammar with expressions already
generated and stored in $\Productions$, to generate new expressions of
size at most $\Size$.
These newly generated expression are checked for distinctness from
already generated ones, and if so, added to
$\Productions[\NonTerminal]$.
The algorithm returns all the expressions produced from the starting
non-terminal $\StartSymbol$.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Require Grammar $\Grammar = \tuple { \NonTerminals, \StartSymbol, \Rules }$ and a set of points $\Points$
    \Ensure Expressions $\tuple { \Expr_1, \Expr_2, \ldots }$ s.t. $\forall i < j : \vert \Expr_i \vert \leq \vert \Expr_j
    \vert \wedge \exists \Point \in \Points : \Expr_i[\Point] \neq \Expr_j[\Point]$
    \ForAll {$\NonTerminal \in \NonTerminals$} $\Productions[\NonTerminals] \gets \emptyset$ \EndFor
    \ForAll {$(\NonTerminal, \Expr) \in \Rules$}\label{line:enumerate:level_one_iter}
    \If { $\Expr \in \Theory[\FormalParameters]$ }
    $\Productions[\NonTerminal] \gets \Productions[\NonTerminal] \cup \{ \Expr \}$  \label{line:enumerate:level_one}
    \EndIf
    \EndFor
    \State $ \Size \gets 1 $
    \While { $\True$ }
    \ForAll {$(\NonTerminal, \Expr) \in \Rules$}
    \State $(\NonTerminal_1, \ldots, \NonTerminal_n) \gets \mbox{List of non-terminals occurring in $\Expr$ }$
    \ForAll { $(\Expr_1, \ldots, \Expr_n) \in \Productions[\NonTerminal_1] \times \cdots \times \Productions[\NonTerminal_n]$ }
    \State $\Expr^* \gets \Expr[\Expr_1/\NonTerminal_1,\ldots,\NonTerminal_n/\Expr_n]$
    \If { $\vert \Expr^* \vert \leq \Size \wedge \forall \Expr' \in
      \Productions[\NonTerminal] . \exists \Point \in \Points :
    \Expr'[\Point] \neq \Expr^*[\Point] $ }
    \State $\Productions[\NonTerminal] \gets \Productions[\NonTerminal] \cup \Expr^*$
    \If { $\NonTerminal = \StartSymbol$ } \textbf{yield} $\Expr^*$ \EndIf
    \EndIf
    \EndFor
    \EndFor
    \State $\Size \gets \Size + 1$
    \EndWhile
  \end{algorithmic}
  \label{algo:enumerate}
  \caption{Enumerating distinct expressions from a grammar}
\end{algorithm}

\begin{theorem}[\cite{Transit}]
  \label{thm:basic_enumeration}
  Given a SyGuS instance $(\Spec, \Grammar)$ with at least one solution
  expression, Algorithm~\ref{algo:basic} terminates and returns the
  smallest solution expression.
\end{theorem}

\paragraph{Features and Limitations}
The enumerative algorithm performs surprisingly well considering its
simplicity on small to medium sized benchmarks
(see~\cite{Transit,sygus_reports,etc}).
Further, due to the guarantee of Theorem~\ref{thm:basic_enumeration}
that the enumerative approach produces small solutions, the algorithm is
capable of generalizing from specifications that are input-output
examples.

However, enumeration quickly fails to scale with growing size of
solutions.
Figure~\ref{fig:scalability_graph} shows the time taken (in seconds) to
generate all distinct expressions up to a given size for the grammar
shown in Figure~\ref{fig:bitvec_grammar}.
As can be seen from the graph, the time taken grows exponentially with
the size.
\arsays{Need a exponential scalability graph. Pick one from the
experiments}



\section{The Divide-and-Conquer Enumeration Algorithm}
\label{sec:algo}

We first present some necessary definitions and concepts for the divide
and conquer enumeration algorithm.

\paragraph{Conditional Expression Grammars}
We introduce conditional expressions grammars that separate and
expression grammar into two grammars to generate:
\begin{inparaenum}[(a)]
\item the return value expression, and 
\item the conditionals that decide which return value is chosen
\end{inparaenum}
These generated return values (terms) and conditionals (predicates) are
then combined using if-then-else conditional operator.
Note that our techniques can be easily extended to use other conditional
operators such as switch statements.

A {\em conditional expression grammar} is a pair of grammars $\tuple{
\Grammar_T, \Grammar_P }$ where:
\begin{inparaenum}[(a)]
\item the {\em term grammar} $\Grammar_T$ is an expression grammar
  generating terms of type $\range(\SynthFun)$; and
\item the {\em predicate grammar} $\Grammar_P$ is an expression
  grammar generating boolean terms.
\end{inparaenum}
The set of expressions $\sem{\tuple{ \Grammar_T, \Grammar_P }}$
generated by the conditional expression grammar $\tuple{ G_T, G_P }$ is
the smallest set of expressions $\Theory[\FormalParameters]$ that
satisfy the following:
\begin{inparaenum}[(a)]
\item $\sem{\Grammar_T} \subseteq \sem{\tuple{ \Grammar_T, \Grammar_P
  }}$, and
\item $\Expr_1, \Expr_2 \in \sem{\tuple{ \Grammar_T, \Grammar_P }}
  \wedge \Pred \in \sem{\Grammar_P} \implies
  \ITE{\Pred}{\Expr_1}{\Expr_2} \in \sem{\tuple{ \Grammar_T, \Grammar_P }}$
\end{inparaenum}

Most commonly occurring SyGuS grammars in practice can be rewritten
as conditional expression grammars in a straightforward and automated
manner.
\arsays{In the \sygus 2015 competition, the grammars for x out of y
benchmarks were decomposable into conditional expression grammars}
However, it is easy to artificially construct a grammar that cannot be
decomposed into a conditional expression grammar.

\begin{example}
  For the running example, the grammar from
  Example~\ref{ex:running:grammar} is easily decomposed into a
  conditional expression grammar $\tuple{\Grammar_T, \Grammar_P}$ as
  follows:
  \begin{inparaenum}[(a)]
  \item the term grammar $\Grammar_T$ contains only the non-terminal
    $T$, and the rules for rewriting $T$.
  \item the predicate grammar $\Grammar_P$ contains the two
    non-terminals $\{ T, C \}$ and the rules associated with them.
  \end{inparaenum}
\end{example}

\paragraph{Decision Trees and Decision Tree Learning}
We use the concept of decision trees from machine learning literature to
model conditional expressions.
Informally, a decision tree $\DecisionTree$ computes a function
$\sem{\DecisionTree}$ that maps samples to {\em labels}.
Each internal node in a decision tree contains an {\em attribute} which
may either hold or not for each sample, and each leaf node contains a
label.
In our setting, labels are terms, attributes are predicates, and samples
are points.

To compute the $\sem{\DecisionTree}$ label for a given point, we follow
a path from the root of the decision tree to a leaf, taking the left
(resp. right) child at each internal node if the attribute holds (resp.
does not hold) for the sample.
The required label is the label at the leaf.

Formally, a {\em decision tree} $\DecisionTree$  is a tuple $\tuple{
\Nodes, (\NodesInternal, \NodesLeaf), \node_0, \Edges, (\EdgesYes,
\EdgesNo), \Attribute, \Label }$ where:
\begin{inparaenum}[(a)]
\item $(\Nodes, \Edges)$ form a rooted binary tree with root node
    $\node_0 \in \Nodes$;
\item The nodes $\Nodes$ are partitioned into a set of internal nodes
    $\NodesInternal$ and leaf nodes $\NodesLeaf$;
\item The attribute function $\Attribute : \NodesInternal \to
    \sem{\Grammar_P}$ maps internal nodes to predicates;
\item The label function $\Label : \NodesLeaf \to \sem{\Grammar_T}$ maps
    leaf nodes to terms;
\item The edges $\Edges$ are partitioned into positive edges $\EdgesYes$
  and negative edges $\EdgesNo$ with each internal node being the source
  of one positive and one negative edge.
  We denote the children of an internal node connected through a
  positive (resp. negative) edge the left (resp. right) child.
\end{inparaenum}

Intuitively, each internal node corresponds to an if-then-else
expression with the conditional given by the attribute of the node, and
each leaf node corresponds to a term given by its label.
Formally, the expression $\DTtoExpr{\DecisionTree}$ corresponding to a
decision tree $\DecisionTree$ is defined as follows:
\begin{inparaenum}[(a)]
\item If $\node_0 \in \NodesLeaf$, i.e., the tree is a single node, then
    $\DTtoExpr{\DecisionTree}$ is $\Label(\node_0)$, and
\item If $\node_0 \in \NodesInternal$, then $\DTtoExpr{\DecisionTree} =
  \ITE{\Attribute(\node_0)}{\DTtoExpr{\node_y}}{\DTtoExpr{\node_n}}$
  where $(\node_0, \node_y) \in \EdgesYes$ and $(\node_0, \node_n) \in \EdgesNo$.
\end{inparaenum}

Decision tree learning is a technique that learns a decision tree from a
given set of samples.
A decision tree learning procedure is given:
\begin{inparaenum}[(a)]
\item a set of samples (points),
\item a set of labels (terms), along with a function that maps a label to the
  subset of samples which it covers; and
\item a set of attributes (predicates).
\end{inparaenum}
A sound decision tree learning algorithm returns a decision tree
$\DecisionTree$ that classifies the points correctly, i.e., for every
sample $\Point$, the label $\sem{\DecisionTree}(\Point)$ covers the
point.
We use the notation {\tt LearnDT} to denote a generic sound
decision tree learning procedure.
The exact procedure we use for decision tree learning in
Section~\ref{sec:decision_trees}.

\subsection{Algorithm}
\label{sec:algo:main}

Algorithm~\ref{algo:main} presents the full divide-and-conquer
enumeration algorithm for synthesis.
Like Algorithm~\ref{algo:basic}, the divide-and-conquer algorithm
maintains a set of points $\Points$, and in each iteration:
\begin{inparaenum}[(a)]
\item computes a candidate solution expression $\Expr$
  (lines~\ref{line:main:start_iter}-\ref{line:main:compute_expr});
\item verifies and returns $\Expr$ if it is correct (lines~\ref{line:main:verify}
  and~\ref{line:main:return}); and
\item otherwise, adds the counter-example point into the set $\Points$
  (line~\ref{line:main:add_point}).
\end{inparaenum}

However, the key differences between Algorithm~\ref{algo:main} and
Algorithm~\ref{algo:basic} are in the way the candidate solution
expression $\Expr$ is generated.
The generation of candidate expressions is separated into two separate
steps.

\paragraph{Term solving.}
Instead of searching for a single candidate expression that is correct
on all points in $\Points$, Algorithm~\ref{algo:main} maintains a set of
candidate terms $\Terms$.
We say that a term $\Term$ covers a point $\Point \in \Points$ if $\Term
\models \Spec \downharpoonleft \Point$.
The set of points that a term covers is computed and stored in
$\Cover[\Term]$ (line~\ref{line:main:compute_cover}).
Note that the algorithm does not store terms that cover the same set of
points as already generated terms
(line~\ref{line:main:duplicate_cover}).
When the set of terms $\Terms$ covers all the points in $\Points$, i.e., for
each $\Point \in \Points$, there is at least one term that is correct on
$\Point$, the term enumeration is stopped.

\paragraph{Unification and Decision Tree Learning.}
In the next step (lines~\ref{line:main:xxx}-\ref{line:main:yyy}), we
generate a set of predicates $\Predicates$ that will be used as
conditionals to combine the terms from $\Terms$ into if-then-else
expressions.
In each iteration, we attempt to learn a decision tree that correctly
label each point $\Point \in \Points$ with a term $\Term$ such that
$\Point \in \Cover(\Term)$.
If such a decision tree $\DecisionTree$ exists, the conditional
expression $\DTtoExpr{\DecisionTree}$ is correct on all points, i.e.,
$\DTtoExpr{\DecisionTree} \models \Spec \downharpoonleft \Points$.
If a decision tree does not exist, we generate additional terms and
predicates and retry.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Require Conditional expression grammar $\Grammar = \tuple{ \Grammar_T, \Grammar_P }$
    \Require Specification $\Spec$
    \Ensure $\Expr$ s.t.  $\Expr \in \sem{\Grammar} \wedge \Expr \models \Spec$
    \State $\Point \gets \emptyset$ \label{line:basic:init}

    \While { $\True$ }
    \State $\Terms \gets \emptyset; \Predicates \gets \emptyset; \Cover \gets \emptyset$ \label{line:main:start_iter}

    \While { $\bigcup_{\Term \in \Terms} \Cover[\Term] \neq \Points$ } \Comment { Term solver }
    \State $\Term \gets \Call{Enumerate}{\Grammar_T,\Points};
    \Cover[\Term] \gets \{ \Point \mid \Point \in \Points \wedge \Term \models \Spec \downharpoonleft \Point \} $\label{line:main:compute_cover}
    \If { $\forall \Term' \in \Terms : \Cover[\Term] \neq \Cover[\Term']$ }
    $\Terms \gets \Terms \cup \{ \Term \}$ \label{line:main:duplicate_cover}
    \EndIf
    \EndWhile

    \While { $\True$ } \Comment { Unifier }
    \State $\DecisionTree \gets \Call{LearnDT}{\Terms, \Predicates}$
    \If  { $\DecisionTree = \bot$ }
    \State $\Terms \gets \Terms \cup \Call{Enumerate}{\Grammar_T,\Points}$ \label{line:main:more_terms}
    \State $\Predicates \gets \Predicates \cup \Call{Enumerate}{\Grammar_P,\Points}$\label{line:main:more_preds}
    \Else
    \State $\Expr \gets \DTtoExpr{\DecisionTree}$ \label{line:main:compute_expr}
    \State \textbf{break}
    \EndIf
    \EndWhile

    \State $\CexInput \gets \Verify(\Expr, \Spec)$ \Comment { Verifier } \label{line:main:verify}
    \If { $\CexInput = \bot$ } \Return $\Expr$ \EndIf \label{line:main:return}
    \State $\Points \gets \Points \cup \CexInput$ \label{line:main:add_point}
    \EndWhile
  \end{algorithmic}
  \caption{The divide-and-conquer enumeration algorithm}
  \label{algo:main}
\end{algorithm}

\begin{theorem}
  Algorithm~\ref{algo:main} is a sound and semi-complete algorithm for the
  Syntax Guided Synthesis problem.
\end{theorem}
\arsays{Proof idea. Also, what do we need from the decision tree
algorithm for completeness? This is a bit shady}

\begin{example}
  Full running of the algorithm
\end{example}

\begin{example}
  Incompleteness without line~\label{line:main:more_preds}
\end{example}

\subsection{Decision Tree Learning}
\label{sec:decision_trees}

The standard multi-label decision tree learning algorithm is presented
in Algorithm~\ref{algo:dt_learning}.
This algorithm is based on ID3~\cite{ID3}.
The algorithm first checks if there exists a single label (i.e., term)
$\Term$ that applies to all the points (line~\ref{line:dt:single}).
If so, it returns a decision tree with only one leaf whose label is
$\Term$ (line~\ref{line:dt:leaf}).
Otherwise, it picks the best predicate $\Pred$ to split on based on some
heuristic (line~\ref{line:dt:pick_pred}).
It the recursively computes the left and right sub-trees for the set of
points on which $\Pred$ holds and does not hold, respectively
(lines~\ref{line:dt:left} and~\ref{line:dt:right}).
The final decision tree is returned as a tree with a root (with
attribute $\Pred$), and positive and negative edges to the roots of the
left and right sub-trees, respectively.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Require $\Points$, ($\Terms$, $\Cover$), $\Predicates$
    \Ensure Decision tree $\DecisionTree$
    \If { $\exists \Term : \Cover[\Terms] \supseteq \Points$ }\label{line:dt:single}
    \Return $\mathit{LeafNode}[\Label \gets \Term]$ \label{line:dt:leaf}
    \EndIf
    \State $\Pred \gets \mbox{Pick predicate from $\Predicates$}$\label{line:dt:pick_pred}
    \State $L \gets \Call{LearnDT}{\{ \Point \mid \Predicates[\Point] \}, \Cover, \Predicates }$\label{line:dt:left})
    \State $R \gets \Call{LearnDT}{\{ \Point \mid \neg \Predicates[\Point] \}, \Cover, \Predicates }$\label{line:dt:right})
    \State \Return $\mathit{InternalNode}[\Attribute \gets \Pred,  left \gets L , right \gets R ]$
  \end{algorithmic}
  \label{algo:dt_learning}
  \caption{Learning Decision Trees}
\end{algorithm}

\begin{example}
  Simple algorithm
\end{example}

\paragraph{Frequency based priors}
The choice of the predicate at line~\ref{line:dt:pick_pred} influences
the size of the decision tree learned by
Algorithm~\ref{algo:dt_learning}, and hence, in our setting, the size of
the solution expression generated.
We now present the classical information gain heuristic we use for
picking the predicates to split.
Informally, the information gain heuristic attempts to maximize the
knowledge about the information \arsays{What the hell does it do
informally?}
We do not describe all aspects of computing information gain fully, but
refer the reader to any standard textbook on machine learning (see, for
example,~\cite{xxx}).

The information gain heuristic is based on defining prior
probabilities $\Prob{label(\Point) = \Term \mid \Point}$ that denotes 
the likelihood of a given point $\Point$ being labelled by a given term
$\Term$.
We define a prior that is based on the principle that the likelihood
that a point is labelled with a term $\Term$ grows with the number of
points covered by the $\Term$.
Informally, this is justified as the decision tree learning algorithm is
more likely to encounter a set of points can be labelled with a single
te are comp \arsays{What's happening here?}

Formally, we define:
\[
    \Prob{label(\Point) = t \mid \Point} = \left\{
    \begin{array}{l l}
      \frac{\Cover(\Term)}{\sum_{\Term' | \Point \in \Cover(\Term')} \Cover(\Term') } & \Point \in \Cover(\Term) \\
        0  & \mbox{otherwise}
    \end{array} \right.
\]
Now, the unconditional probability of an arbitrary point being labelled
with $\Term$ is given by $\Prob{label(\Point) = \Term} = \sum_{\Point}
\Prob{label(\Point) = \Term \mid \Point}\cdot\Prob{\Point}$.
Assuming a uniform distribution for picking points, we have that
\[
    \Prob{label(\Point) = t} =  \frac{1}{\vert \Points \vert} \cdot \sum_{\Point} \Prob{label(\Point) = \Term \mid \Point}
\]

\arsays{I don't even know what's happening here}

\subsection{Features and Optimizations}
\label{sec:optimizations}

We will now discuss several features and optimizations for
Algorithm~\ref{algo:main}.


\paragraph{Improving Synthesis Results}
In the presentation from Section~\ref{sec:algo:main}, the algorithm
stops enumeration of terms and predicates as soon as it finds a single
solution to the synthesis problem.
However, the algorithm could be made to continue executing even after a
single solution.
\arsays{HASDKLJ}
This could lead to different, potentially smaller decision trees and
solutions.

\begin{example}
  Give an example
\end{example}

\paragraph{Term and predicate substitution}
In Algorithm~\ref{algo:xxx}, we discard the terms and predicates that
are equivalent to other terms and predicates on the selected points
$\Points$ in lines~\ref{line:main:xxx},~\ref{line:main:yyy},
and~\ref{line:main:zzz}.
However, these discarded terms, though equivalent on the selected
points, may be \arsays{write something here} 

\begin{example}
  \arsays{Write DT substitution thing here}
\end{example}

\paragraph{Other optimizations}
\arsays{Bunching, branch level verification}
Instead of verifying the full 

branch level verification gives many more counter-examples


\section{Evaluation}
\label{sec:evaluation}
% \input{results_one_shot_table}
\input{results_anytime_table}

We built a prototype \sygus solver based on the ideas presented in
this paper. The algorithms for enumeration were implemented in
about 3000 lines of Python code, while the algorithms for decision
tree learning were implemented in about 3000 lines of
C$\raisebox{1pt}{+\!+}$ code. All experiments were executed on a
32-core machine with four Intel Xeon E7-4820 processors running at 2.0
GHz, and 128 GB of memory, with a one hour time limit.

We seek to empirically evaluate how our synthesis algorithm compares
to other state-of-the-art synthesis techniques along the following
dimensions:
\begin{inparaenum}[(a)]
\item
\emph{Performance}: How quickly can the algorithms arrive at a correct
solution?
\item
\emph{Quality}: How \emph{good} are the solutions produced by the
algorithms? We use compactness of solutions, in terms of syntactic
size, as a metric for the quality of solutions.
\item
\emph{Impact of allowing additional time}: How significant is the
improvement in the quality of the solutions generated, provided the
algorithm is given an additional (but fixed) time budget.
\end{inparaenum}
We now discuss how our algorithm matches up against the existing
techniques along each of these dimensions, using to data from Tables


\section{Related Work}
\label{sec:related_work}

\bibliographystyle{plainnat}
\setlength{\bibsep}{1pt}
\begin{small}
\bibliography{main}
\end{small}

\arsays{Madhu's work assumes that the predicate domain can distinguish
any two points?}

\end{document}
