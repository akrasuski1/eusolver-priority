\documentclass{llncs}

\sloppy

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{paralist}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup{compatibility=false}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\newcommand\comment[1]{}
\newcommand\arsays[1]{{\bf AR: #1}}
\newcommand\ausays[1]{{\bf AU: #1}}

\newcommand\tuple[1]{\langle #1 \rangle}

\begin{document}

\title{Scaling Enumeration for SyGuS through Divide-and-Conquer}
\author{Rajeev Alur \and Arjun Radhakrishna \and Abhishek Udupa}
\institute{University of Pennsylvania}
\maketitle

\begin{abstract}
  \arsays{What is the problem? What is the enumerative approach?}

  Given a functional specification and an expression syntax, the
  Syntax-Guided Synthesis (SyGuS) problem is to find an expression in
  the given syntax that satisfies the functional specification.
  The enumerative algorithm (proposed by Udupa et al) has proved to be
  very effective approach to the SyGuS problem winning the 2014 SyGuS
  competition and placing second in the 2015 SyGuS competition.

  \arsays{Where does enumeration fail?}
  The enumerative algorithm performs rather well on small to
  medium sized benchmarks, produces succinct expressions, and has the
  ability to generalize from input-output examples.
  However, its performance degrades drastically with the size of the
  smallest solution.

  \arsays{What do we do about it? Describe techniques in the paper}
  In this paper, we describe a synthesis technique that is based on
  extending the reach of the enumeration technique by separately
  enumerating
  \begin{inparaenum}[(a)]
  \item smaller expressions that are correct on subsets of inputs, and
  \item predicates on inputs that distinguish these subsets.
  \end{inparaenum}
  These smaller expressions and predicates are then combined into the
  final solution expression by using decision trees to obtain an
  expression that is correct on all inputs.
  We view the problem of combining expressions and predicates as a multi-label
  decision tree learning problem. We propose a novel technique of
  associating a probability distribution over the set of labels that a
  sample can be labeled with. This enables us to use standard
  information gain based heuristics to learn a compact decision tree.

  \arsays{Did our techniques work?}
  We implemented this technique in a prototype tool and evaluated it on
  the benchmarks from the SyGuS 2015 competition.
  Our tool is able to match the running times and the compactness of
  solutions of both standard enumerative solver as well as the latest
  white-box solvers in most cases.
  Further, our solver is able to solve a large number of instances from
  the ICFP suite of benchmarks, which were previously unsolved by all
  solvers including other decision tree based ones.
  Our evaluation shows that our approach improves the scalability of the
  enumerative algorithm without compromising on the solution quality.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\section{Illustrative Examples}
\label{sec:example}

\section{Problem Statement and Background}
\label{sec:problem}

An instance of the SyGuS problem is given by a pair $\tuple { \Phi, G }$
where $\Phi$ is an SMT formula over an unknown function $f$ and $G$ is a
grammer\dots
\arsays{Use standard definitions here.}
Note that we restrict ourselves to a version of the SyGuS problem where
there is exactly one unknown function to be synthesized, and the grammer
does not contain {\tt let} expressions.


\paragraph{Separability of specifications.}
The

\begin{remark}
  There have been many related notions of separability in the existing
  literature.
  The aim of all these notions is to categorize a class of SyGuS
  problems where the output a solver chooses for input does not affect
  the set of values the solver can choose for other inputs.

  Syntactic -- separability, single invocation, etc

  Semantic -- single point refuatable, etc
\end{remark}

\subsection{The Enumeration Sovler}
\label{sec:enumeration}

\section{What do we call the new algorithm?}
\label{sec:algo}

\subsection{Learning Decision Trees}
\label{sec:decision_trees}

\subsection{Optimizations}
\label{sec:optimizations}

\section{Evaluation}
\label{sec:evaluation}

\section{Related Work}
\label{sec:related_work}

\
\end{document}
