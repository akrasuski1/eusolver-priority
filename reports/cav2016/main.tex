\documentclass{llncs}

\sloppy

\usepackage{marginnote}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{alltt}
\usepackage{paralist}
\usepackage{soul}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{mdwtab}
\usepackage[titletoc,title]{appendix}
\usepackage{multirow}
\usepackage[square,numbers,sort&compress,comma,sectionbib]{natbib}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.12}
\usepackage{hyperref}

\newcommand\comment[1]{}
\newcommand\arsays[1]{{\bf AR: #1}}
\newcommand\ausays[1]{{\bf AU: #1}}

\renewcommand\subparagraph[1]{\smallskip\noindent{\em #1.}}
\makeatletter
\newcommand{\@chapapp}{\relax}
\makeatother
\newcommand{\ie}{\emph{i.e.}}
\renewcommand{\bibname}{References}
\newcommand\Integers{\mathbb{Z}}
\newcommand\tuple[1]{\langle #1 \rangle}
\newcommand\True{\mathit{True}}
\newcommand\None{\mathit{None}}
\newcommand\Equiv{\mathit{Eq}}
\newcommand\Points{\mathsf{pts}}
\newcommand\Point{\mathsf{pt}}
\newcommand\Verify{\mathsf{verify}}
\newcommand\CexInput{\mathsf{cexpt}}
\newcommand\Predicates{\mathsf{preds}}
\newcommand\Expr{e}
\newcommand\Pred{p}
\newcommand\Terms{\mathsf{terms}}
\newcommand\Term{t}
\newcommand\Cover{\mathsf{cover}}
\newcommand\Powerset[1]{\mathbf{2}^{#1}}
\newcommand\Spec{\Phi}
\newcommand\Size{K}
\newcommand\Grammar{G}
\newcommand\sem[1]{[\![ #1 ]\!]}
\newcommand\SynthFun{f}
\newcommand\range{\mathsf{range}}
\newcommand\FormalParameters{\mathsf{params}}
\newcommand\Productions{\mathsf{prodn}}
\newcommand\Prob[2]{\mathbb{P}_{#1}(#2)}
\newcommand\NonTerminals{\mathcal{N}}
\newcommand\NonTerminal{N}
\newcommand\StartSymbol{S}
\newcommand\Symbols{\mathsf{symbols}}
\newcommand\Rules{\mathcal{R}}
\newcommand\Rule{R}
\newcommand\Theory{\mathcal{T}}
\newcommand\RewritesTo{\rightarrow}
\newcommand\ITE[3]{\mathtt{if}~#1~\mathtt{then}~#2~\mathtt{else}~#3}

\newcommand\DecisionTree{\mathit{DT}}
\newcommand\DTtoExpr[1]{\mathsf{expr}(#1)}
\newcommand\NodesInternal{V_I}
\newcommand\Nodes{V}
\newcommand\node{v}
\newcommand\NodesLeaf{V_L}
\newcommand\EdgesYes{E_Y}
\newcommand\EdgesNo{E_N}
\newcommand\Edges{E}
\newcommand\Attribute{\mathcal{A}}
\newcommand\Label{\mathcal{L}}
\newcommand{\sygus}{{\sffamily\fontsize{8.5}{10}\selectfont
    SyGuS}\xspace}
\newcommand{\dcsolve}{{\sffamily\fontsize{8.5}{10}\selectfont
    DCSolve}\xspace}
\renewcommand{\paragraph}[1]{\par\noindent\textbf{#1}.}
\newcommand{\itparagraph}[1]{\par\noindent\textit{#1}.}
\newcommand{\esolver}{\textsc{esolver}\xspace}
\newcommand{\sketch}{\textsc{sketch}\xspace}
\newcommand{\pwsolve}{{\sffamily\fontsize{8.5}{10}\selectfont
    PWSolve}\xspace}

% Make floats and equations behave sensibly
\setlength{\intextsep}{1pt}
\setlength{\textfloatsep}{1pt}
\setlength{\floatsep}{1pt}
\abovedisplayskip=6pt plus 3pt minus 9pt
\abovedisplayshortskip=0pt plus 3pt
\belowdisplayskip=6pt plus 3pt minus 9pt
\belowdisplayshortskip=7pt plus 3pt minus 4pt
\setlength\abovecaptionskip{1pt}
\setlength\belowcaptionskip{1pt}

\begin{document}
% \arsays{TODO:}
% \begin{compactitem}
% \item Change main algo to check for distinctness on points in a
%   straightforward manner
% \item
% \end{compactitem}

% \newpage

\pagestyle{plain}
\title{Scaling Enumerative Program Synthesis via Divide and Conquer}
% \author{Rajeev Alur \and Arjun Radhakrishna \and Abhishek Udupa}
\author{}
% \institute{University of Pennsylvania}
\institute{}
\maketitle
\vspace*{-6ex}

\begin{abstract}
  Given a semantic constraint specified by a logical formula and
  syntactic constraint specified by a context-free grammar, the
  Syntax-Guided Synthesis (\sygus) problem is to find an expression
  that satisfies both the syntactic and semantic constraints.
  An enumerative approach to solve this problem is to systematically
  generates all expressions from the syntactic space with some pruning,
  and has proved to be surprisingly competitive in the newly started
  competition of \sygus solvers.  It performs well on small to medium sized
  benchmarks, produces succinct expressions, and has the ability to
  generalize from input-output examples.  However, its performance
  degrades drastically with the size of the smallest solution. To overcome
  this limitation, in this paper we propose an alternative approach to
  solve \sygus instances.

  The key idea  is to employ a divide-and-conquer approach by
  separately enumerating (a) smaller expressions that are correct on
  subsets of inputs, and (b) predicates on inputs that distinguish these
  subsets.  These smaller expressions and predicates are then combined
  using decision trees to obtain an expression that is correct on all
  inputs.  We view the problem of combining expressions and predicates as
  a multi-label decision tree learning problem. We propose a novel
  technique of associating a probability distribution over the set of
  labels that a sample can be labeled with. This enables us to use
  standard information-gain based heuristics to learn a compact decision
  tree.

  We report a prototype implementation and evaluate it on the benchmarks
  from the \sygus 2015 competition. Our tool is able to match the running
  times and the succinctness of solutions of both standard enumerative
  solver as well as the latest white-box solvers in most cases.  Further,
  our solver is able to solve a large number of instances from the ICFP
  class of benchmarks, which were previously unsolved by all existing
  solvers.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\marginnote{Para 1 {Synthesis background and \sygus as an enabling
technology}}
The field of program synthesis relates to automated techniques that
attempt to automatically generate programs from requirements that a
programmer writes.
Program synthesis is an increasingly popular area of research with the
introduction of counter-example guided inductive synthesis (CEGIS)
approach by Solar-Lezama et al, and has been applied to a wide array of
domains including invariant generation, automated repair of
programs, binary rewriting, etc~\cite{solar-lezama-05,garg-16}.
Recently, Syntax-Guided Synthesis (\sygus) has been proposed as a
back-end exchange format and enabling technology for program synthesis.
The aim is to allow experts from different domains to model their
synthesis problems as \sygus instances, and leverage general purpose
\sygus solvers.

\marginnote{Para 2 {\sygus and \sygus approaches; enumerative solver +
limitations}}
In the \sygus approach, a synthesis task is specified using
restrictions on both the form (syntax) and function (semantics) of the
program to be synthesized:
\begin{inparaenum}[(a)]
\item The syntactic restrictions are given in terms of a context-free
  grammar from which a solution program may be drawn.
\item The semantic restrictions are encoded into a specification as an
  SMT formula.
\end{inparaenum}
\sygus solvers can be broadly categorized into two kinds:
\begin{inparaenum}[(a)]
\item black-box solvers, which do not deal with the specification
  directly, but learn constraints on how a potential solution should
  behave on sample input points~\cite{udupa-transit,udupa-sygus,saha-15};
  and
\item white-box solvers, which attempt to either rewrite or solve the
  specification directly using constraint solving
  techniques~\cite{reynolds-15,alur-15}.
\end{inparaenum}
Of the black-box solvers, the most successful, despite its simplicity,
has been the enumerative solver which won the \sygus competition 2014
and placed second in \sygus competition 2014.
The enumerative solver maintains a set of concrete input points, and
in each iteration attempts to produce an expression that is correct on
these concrete inputs.
It does so by enumerating expressions from the grammar and checking if
they are correct on the input points, while pruning away expressions
that behave equivalently to already generated expressions.
If an expression that is correct on the input points is found, it is
verified against the full specification.
If it is incorrect, a counter-example point is found and added to the
set of input points.

\marginnote{Para 3 {Problem and our solution to this}}
While the enumerative strategy works well when the desired solutions
have small to medium sizes, it does not scale well with the solution
size.  In Figure~\ref{figure:random_exponential_graph}, the x-axis
represents expression size and the y-axis represents the time taken to
enumerate all expressions of size up to the expression size.  As can
be seen from the graph, the time taken to explore potential solutions
grows exponentially with size.  To overcome this scalability issue, we
introduce a divide-and-conquer enumerative algorithm.

The divide-and-conquer enumerative approach is based on this insight:
while the full solution expression to the synthesis problem may be
large, the important individual parts are small.
The individual parts we refer to here are:
\begin{inparaenum}[(a)]
\item {\em terms} which serve as the return value for the solution,
  and
\item {\em predicates} which serve as the conditionals that choose which
  term is the actual return value for a given input.
\end{inparaenum}
For example, in the expression $\ITE{x \leq y}{y}{x}$, the terms are $x$
and $y$, and the predicate is $x \leq y$.
In this example, although the full expression has size $6$,
the individual terms have size $1$ each, and the predicate size $3$.
Hence, the divide-and-conquer enumerative approach only enumerates terms
and predicates separately and attempts to combine them into a
conditional expression.
% \arsays{Do we need to mention the point-wise/separable restriction here,
% and also the restriction on grammars?}

For combining the different part of the solution in a full conditional
expression, we use the technique of learning decision
trees~\cite{quinlan-86,bishop-book}.
The input points maintained by the enumerative algorithm serve as the
samples, the predicates enumerated serve as the attributes, and the
terms serve as the labels.
A term $\Term$ is a valid label for a point $\Point$ if $\Term$ is
correct for $\Point$.
We use a simple multi-label decision tree learning algorithm returns a
sound decision tree that classifies the samples soundly, i.e., for each
point, following the edges corresponding to the attribute values (i.e.,
predicates) lead to a branch.

To enhance the quality of the solutions obtained, we extend the basic
divide-and-conquer algorithm to be an \emph{anytime}
algorithm, \ie, the algorithm does not stop when the first
solution is found, and instead continues enumerating terms and
predicates in an attempt to produce more compact
solutions. Decomposing the verification queries into
\emph{branch-level queries} helps in faster convergence of the
algorithm.

% \marginnote{Para 5 {Other extensions. \arsays{I don't like this
% paragraph. It sounds like a ``this too is in the paper'' paragraph}}}
% \paragraph{Extensions and Optimizations}
% We present two extensions and optimizations to resolve
% various issues that arise in a practical implementation of the
% divide-and-conquer algorithm, as well as to enhance to the quality of
% solutions produced.

% \subparagraph{Anytime extension}
% The anytime extension to the algorithm does not stop when the first
% solution is found, and instead continues enumerating terms and
% predicates in an attempt to produce more compact solutions.
% For example, suppose the synthesizer is attempting to produce a
% conditional equivalent to $x \geq 0 \wedge y \geq 0 \wedge x + y \leq
% 2$.
% Using predicates of size $3$, the most compact solution the synthesizer
% can produce is $y \geq 0 \wedge ((x = 0 \wedge y \leq 2) \vee (x = 1
% \wedge y \leq 1) \vee (x = 2 \wedge y \leq 0))$.
% However, allowing the synthesizer to continue and further enumerate
% predicates of size $4$ lets it to produce the more compact original
% conditional.

% \subparagraph{Decision tree repair}
% During the enumeration process, a term (say $\Term$) which is correct on
% the same set of points as an already generated term (say $\Term'$) is
% discarded.
% However, it may be the case that $\Term$ is correct on a larger set of
% inputs than $\Term'$.
% Hence, if a counter-example is generated when verifying an expression
% $\Expr$ that uses $\Term'$, it might be possible to repair $\Expr$ to
% make it behave correctly on the counter-example by replacing $\Term'$
% with $\Term$.

% \subparagraph{Branch level verification}
% Another more technique we use to generate additional counter-examples
% from a given decision tree is to verify each branch (path from root to
% leaf) separately.
% Each such verification call may return additional counter-examples,
% allowing the procedure to gain more information from a single incorrect
% solution.
% For example, to verify the proposed solution expression $\SynthFun(x, y)
% \equiv \ITE{x \leq y}{1}{0}$: instead of checking if there exists an
% input $(x_0, y_0)$ on which the expression behaves incorrectly, branch
% level verification checks separately if
% \begin{inparaenum}[(a)]
% \item there exists an input satisfying $x \leq y$ on which the
%   expression $1$ behaves incorrectly, and
% \item the exists an input satisfying $\neg x \leq y$ on which the
%   expression $0$ behaves incorrectly.
% \end{inparaenum}

\marginnote{Para 7 {Evaluation}}
\paragraph{Evaluation}
We implemented the proposed algorithm in a prototype tool and
evaluated it on benchmarks from the \sygus competition 2015, both in
the linear integer arithmetic and the bit-vector domain.  The tool was
able to perform on par or better than existing solvers in both
domains, and produced compact solutions.  In the bit-vector domain,
the tool was able to solve $47$ out of the $50$ problems in the ICFP
suite of benchmarks in less than $1$ hour.  To the best of our
knowledge, no other solver has been able to solve the ICFP benchmarks
satisfactorily.  Further, we also observed that the anytime extension
of the algorithm was able to produce more compact solutions in $22$ of
the $47$ solved ICFP benchmarks.  We were able to obtain upto a $90\%$
reduction in solution size in some cases.  In the linear integer
arithmetic domain, the tool performed much better than existing
black-box solvers scaling to larger instances.  However, in the larger
benchmarks, the size of the solution produced was larger than those
produced by white-box solvers due to the decision tree learning
algorithm producing larger than necessary decision trees.

\marginnote{Para 8 {Summary}}
\paragraph{Summary}
Overall, this paper presents a divide-and-conquer approach to
enumerative synthesis.
The main contributions are as follows:
\begin{inparaenum}[(a)]
\item The main divide-and-conquer algorithm that enumerates only the
  terms and predicates involved in the final solution and combines them
  using a decision tree learning technique;
\item Several novel extensions and optimizations leading to improved
  quality of solutions and performance; and
\item A prototype tool and experimental evaluation demonstrating the
  scalability of the approach and quality of solutions produced.
\end{inparaenum}

\section{Illustrative Examples}
\label{sec:example}

\begin{figure}[t]
  \vspace*{-1ex}
  \begin{alltt}
                      S ::= T | if (C) then T else T
                      T ::= 0 | 1 | x | y | T + T
                      C ::= T \(\leq\) T | C \(\wedge\) C | \(\neg\) C
  \end{alltt}
  \vspace*{-4ex}
  \caption{Grammar for linear integer expressions}
  \label{fig:lia_grammar}
\end{figure}
We explain the salient points of our approach on an example.
Consider an instance of the \sygus problem where the aim is to generate
an expression for the body of the function $f$, such that:
\begin{compactitem}
\item The expression generated by the grammar from Figure~\ref{fig:lia_grammar}.
\item The expression, when substituted for $f$, in the specification
  $\Spec$, renders it true, where $\Spec \equiv \forall x, y :
  \SynthFun(x, y) \geq x \wedge \SynthFun(x, y) \geq y \wedge
  (\SynthFun(x,y) = x \vee \SynthFun(x,y) = y)$.
\end{compactitem}
Note that the specification constrains the return value of $\SynthFun$
to be the maximum of the two inputs $x$ and $y$.
The smallest expression generated by the grammar that satisfies this
specification is $\ITE{x \leq y}{y}{x}$.

\paragraph{Na\"ive Enumeration}
We explain the running of the na\"ive enumerative
algorithm~\cite{udupa-transit} using
Table~\ref{table:enumerative_example}.
The enumerative algorithm maintains a set of concrete input points
$\Points$, and proceeds in rounds.
In the first round, $\Points$ is empty.
In each round the algorithm proposes a candidate solution expression
that is correct on all of $\Points$.
If this expression is correct on all inputs --- verified by an SMT
solver --- the algorithm returns it.
Otherwise, a counter-example input point, on which the expression is
incorrect, is added to $\Points$.

\begin{table}[!t]
  \setlength{\doublerulesep}{0.4pt}
  \centering
  \fontsize{8}{10}\selectfont
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}clcc}\\\hlx{hv}
      Round no. & \multicolumn{1}{c}{Enumerated expressions} & Candidate Expression & Point added\\\hlx{hvhv}
      1 & $0$ & $0$ & $\{ x \mapsto 1, y \mapsto 0 \}$ \\\hlx{h}
      2 & $0$, $1$ & $1$ & $\{ x \mapsto 0, y \mapsto 2 \}$ \\\hlx{h}
      3 & $0$, $1$, $x$, $y$, $\ldots$, $x + y$,  & $x+y$ & $\{ x \mapsto 1, y \mapsto 2 \}$ \\\hlx{h}
      \vdots & \vdots & \vdots & \vdots\\\hlx{h}
      $n$ & $0$, \ldots, $\ITE{x \leq y}{y}{x}$  & $\ITE{x \leq y}{y}{x}$ &  \\\hlx{hv}
  \end{tabular*}
  \caption{Example run of the na\"ive enumerative algorithm}
  \label{table:enumerative_example}
\end{table}

The algorithm generates the candidate solution expression by
enumerating expressions generated by the grammar in order of size.  In
the first round, the candidate expression proposed is the first
expression generated (the expression $0$) from the grammar as
$\Points$ is empty.  Attempting to verify the correctness of this
expression, yields a counter-example point $\{ x \mapsto 1, y \mapsto
0 \}$.  In the second round, the expression $0$ is incorrect on the
point, and the next expression to be correct on all of $\Points$ (the
expression $1$)
is proposed.  This fails to verify as well, and yields the
counter-example point $\{ x \mapsto 0, y \mapsto 2 \}$. In the third
round, all expressions of size $1$ are incorrect on at least one point
in $\Points$, and the algorithm moves on to enumerate larger
expressions.
% and the algorithm moves to expressions of size $3$
% (there are no expressions of size $2$).
% Here, the algorithm might
% generate the expression $0 + x$.  However, this expression is
% immediately discarded as it is equivalent to the expression $x$ (of
% size $1$) on all $\Points$.
After several rounds, the algorithm eventually generates the expression
$\ITE{x \leq y}{y}{x}$ which the SMT solver verifies to be correct.
Note that to get the correct expression, the algorithm had to generate
a large number (in this case, hundreds of expressions).  In general,
the number of expressions grows exponentially with size.  Thus, the
enumerative solver fails to scale to problems which only admit
solutions representable by large expressions.


\paragraph{Divide and Conquer Enumeration}
In the above run, though the solution expression is large, the
individual components (the terms $x$ and $y$, and the predicate $x \leq
y$) are rather small and can be quickly enumerated.
This difference between the sizes of the constituent terms and
predicates, and the complete solution only increases for larger
solutions.

The divide-and-conquer approach enumerates terms and predicates
separately, and attempts to combine them into a conditional expression.
Again, we explain this idea using an example run of the algorithm we
propose (see Table~\ref{table:dq_example}).

Similar to the na\"ive algorithm, the divide-and-conquer algorithm
maintains a set of points $\Points$, and works in rounds.
As before, in the first round, the term $0$ is generated and found to be
correct on $\Points$ (which is empty).
The second round is similar to the second round of the na\"ive
enumerative algorithm.
In the third round, the terms $0$, $1$, $x$ and $y$ are enumerated.
In contrast to the na\"ive algorithm, the enumeration stops here because
terms $1$ and $y$ are correct on the points $\{ x \mapsto 1, y \mapsto 0
\}$ and $\{ x \mapsto 0, y \mapsto 2 \}$, respectively, and thus
together ``cover'' all of $\Points$.
Now, to propose an expression, the algorithm starts enumerating
predicates.
The enumeration of predicates proceeds until the algorithm can find
a sufficient number of predicates to generate a conditional expression
using the previously enumerated terms.
The terms and predicates are combined into conditional expression by
learning decision trees (see Section~\ref{sec:decision_trees}).
The candidate expression proposed in the third round is
$\ITE{1 \leq y}{y}{x}$ and the counter-example generated is
$\{ x \mapsto 2, y \mapsto 0 \}$ (see table).
Proceeding further, in the fourth round, the correct expression is
generated.

Note that this approach only generates $4$ terms and $11$ predicates in
contrast to the na\"ive approach which generates hundreds of
expressions.

\begin{table}[!t]
  \setlength{\doublerulesep}{0.4pt}
  \centering
  \fontsize{8}{10}\selectfont
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}cllcc}\\\hlx{hv}

      \multirow{2}{*}{Round no.} & \multicolumn{1}{c}{Enumerated} & \multicolumn{1}{c}{Enumerated} & Candidate & \multirow{2}{*}{Point added} \\
      & \multicolumn{1}{c}{Terms} & \multicolumn{1}{c}{Predicates} & Expression & \\\hlx{hvhv}
      1 & $0$ & $0$ & $\emptyset$ & $\{ x \mapsto 1, y \mapsto 0 \}$ \\\hlx{hv}
      2 & $0$, $1$ & $1$ & $\emptyset$ &  $\{ x \mapsto 0, y \mapsto 2 \}$ \\\hlx{hv}
      \multirow{2}{*}{3} & \multirow{2}{*}{$0$, $1$, $x$, $y$} & $0 \leq 0$, \ldots $0 \leq y$, & \multirow{2}{*}{$\ITE{1 \leq y}{y}{1}$} & \multirow{2}{*}{$\{ x \mapsto 2, y \mapsto 0 \}$} \\
      & & $1 \leq 0$, \ldots $1 \leq y$ & & \\\hlx{hv}
      4 & $0$, $1$, $x$, $y$ & $0 \leq 0$, \ldots $x \leq y$ & $\ITE{x \leq y}{y}{x}$ & \\\hlx{hv}
  \end{tabular*}
  \caption{Example run of the divide-and-conquer enumerative algorithm}
  \label{table:dq_example}
\end{table}


\section{Problem Statement and Background}
\label{sec:problem}

Let us fix the function to be synthesized $\SynthFun$ and its formal
parameters $\FormalParameters$.
We write $\range(\SynthFun)$ to denote the range of $\SynthFun$.
The term {\em point} denotes a valuation of $\FormalParameters$, i.e., a
point is an input to $\SynthFun$.

\begin{example}
  For the running example in this section, we consider a function to be
  synthesized $\SynthFun$ of type $\Integers \times \Integers \to
  \Integers$ with the formal parameters $\FormalParameters = \{ x, y \}$.
  Points are valuations of $x$ and $y$.
  For example, $\{ x \mapsto 1, y \mapsto 2 \}$ is a point.
\end{example}

\paragraph{Specifications}
Satisfiability modulo theory formulae have become the standard formalism
for specifying semantic constraints for synthesis.
In the rest of this paper, we fix an arbitrary theory $\Theory$ and
denote by $\Theory[\Symbols]$, the set of $\Theory$ terms over the
symbols $\Symbols$.
A {\em specification} $\Spec$ is a logical formula in a theory $\Theory$
over standard theory symbols and the function to be synthesized
$\SynthFun$.
We say that an expression $\Expr$ satisfies a specification $\Spec$ if
instantiating the function to be synthesized $\SynthFun$ by $\Expr$
makes the formula $\Spec$ valid.

\begin{example}
  \label{ex:running:spec}
  Continuing the running example, we define a specification $\Spec
  \equiv \forall x, y : \SynthFun(x, y) \geq x \wedge \SynthFun(x, y)
  \geq y \wedge f(x, y) = x \vee f(x, y) = y$.
  The specification states that the function to be synthesized
  $\SynthFun$ maps each pair $x$ and $y$ to a value that is at least as
  great as each of them and equal to one of them.
  The function that returns the maximum of the two input values is the
  only possible function that satisfies this specification.
\end{example}

\paragraph{Grammars}
An {\em expression grammar} $\Grammar$ is a tuple $\tuple {
\NonTerminals, \StartSymbol, \Rules }$ where:
\begin{inparaenum}[(a)]
\item the set $\NonTerminals$ is a set of non-terminal symbols,
\item the non-terminal $\StartSymbol \in \NonTerminals$ is the initial non-terminal,
\item $\Rules \subseteq \NonTerminals \times
  \Theory[\NonTerminals \cup \FormalParameters]$ is a finite set
  of rewrite rules that map $\NonTerminals$ to $\Theory$-expressions
  over non-terminals and formal parameters.
\end{inparaenum}
We say that an expression $\Expr$ {\em rewrites to} an incomplete
expression $\Expr'$ (written as $\Expr \RewritesTo_\Grammar \Expr'$) if
there exists a rule $\Rule = (\NonTerminal, \Expr'') \in \Rules$ and
$\Expr'$ is obtained by replacing one occurrence of $\NonTerminal$ in
$\Expr$ by $\Expr''$.
Let $\RewritesTo_\Grammar^*$ be the transitive closure of $\RewritesTo$.
We say that an expression $\Expr \in \Theory[\FormalParameters]$
is {\em generated} by the grammar $\Grammar$ (written as $\Expr \in
\sem{\Grammar}$) if $\StartSymbol \RewritesTo_\Grammar^* \Expr$.
Note that we implicitly assume that all terms generated by the grammar
have the right type, i.e., are of the type $\range(\SynthFun)$.

\begin{example}
  \label{ex:running:grammar}
  For the running example, we choose the following grammar.
  The set of non-terminals is given by $\NonTerminals = \{ \StartSymbol, T, C \}$ with
  the initial non-terminal being $\StartSymbol$.
  The rules of this grammar are $\{ (\StartSymbol, T), (\StartSymbol,
      \ITE{C}{\StartSymbol}{\StartSymbol})
    \} \cup \{ (T, x), (T, y), (T, 1), (T, 0), (T, T + T) \} \cup
        \{ (C, T \leq T), (C, C \wedge C), (C, \neg C) \}$.
  This is the standard linear integer arithmetic grammar used for many
  \sygus problems.
  This grammar is equivalent to the one from
  Figure~\ref{fig:lia_grammar}.
\end{example}

\paragraph{The Syntax-Guided Synthesis Problem}
An instance of the \sygus problem is given by a pair $\tuple { \Spec,
\Grammar }$ of specification and grammar.
An expression $\Expr$ is a solution to the instance if $\Expr \models
\Spec$ and $\Expr \in \sem{\Grammar}$.

\begin{example}
  Continuing the running example, for the specification $\Spec$ from
  Example~\ref{ex:running:spec} and the grammar from
  Example~\ref{ex:running:grammar}, one of the solution expressions is
  given by $\SynthFun(x, y) \equiv \ITE{x \leq y}{y}{x}$.
\end{example}

\vspace*{-3mm}
From our definitions, it is clear that we restrict ourselves to a
version of the \sygus problem where there is exactly one unknown function
to be synthesized, and the grammar does not contain {\tt let} rules.
Further, we assume that our specifications are {\em
  point-wise}\footnote{For brief discussion on different syntactic and
semantic notions of point-wise specifications, see the appendix}.
Intuitively, if a specification is point-wise, if it just relates the
input point to its output, and not the outputs of different inputs.

Here, we use a simple syntactic notion of point-wise specifications for
convenience.
However, our techniques can be generalized to any notion of point-wise
specifications.
Formally, we say that a specification is {\em plainly separable} if can
be rewritten into a conjunctive normal form where each clause is either
\begin{inparaenum}[(a)]
\item a tautology, or
\item every appearing application of the function to be synthesized
  $\SynthFun$ have the same arguments.
\end{inparaenum}

\vspace*{-3mm}
\begin{example}
  The specification for our running example $\Spec \equiv \SynthFun(x,
  y) \geq x \wedge \SynthFun(x, y) \geq y \wedge \SynthFun(x, y) = x
  \vee \SynthFun(x, y) = y$ is plainly separable.
  For example, this implies that the value of $\SynthFun(1, 2)$ can be
  chosen irrespective of the value of $\SynthFun$ on any other point.
  On the other hand, a specification such as $\SynthFun(x, y) = 1
  \implies \SynthFun(x + 1, y) = 1$ is neither plainly separable nor
  point-wise.
  The value of $\SynthFun(1, 2)$ cannot be chosen independently of the
  value of $\SynthFun(0, 2)$.
\end{example}

\vspace*{-3mm}
Point-wise (and in turn, simply separable) specifications allow us to
define the notion of an expression $\Expr$ satisfying a specification
$\Spec$ on a point $\Point$.
Formally, we say that $\Expr \models \Spec \downharpoonleft
\Point$ if $\Expr$ satisfies the specification $\Spec \downharpoonleft
\Point$ given by replacing each clause $C$ in the plainly separable
specification by the formula $\mathit{Pre}_C(\Point) \implies C$.
Here, the premise $\mathit{Pre}_C(\Point)$ is given by $\bigwedge_{p \in
\FormalParameters} \mathit{Arg}_C(p) = \Point[p]$ where $\mathit{Arg}_C$ is the
actual argument corresponding to the formal parameter $p$ in the
unique invocation of $\SynthFun$ that occurs in $C$.
We extend this definition to sets of points as follows: $\Expr \models
\Spec \downharpoonleft \Points \Leftrightarrow \bigwedge_{\Point \in
\Points} \Expr \models \Spec\downharpoonleft\Point$.

\vspace*{-3mm}
\begin{example}
  \label{ex:running:correctness_on_point}
  For the specification $\Spec$ of the running example, the function
  given by $\SynthFun(x, y) \equiv x + y$ is correct on the point $\{ x
      \mapsto 0, y \mapsto 3 \}$ and incorrect on the point $\{ x
  \mapsto 1, y \mapsto 2 \}$
\end{example}

\vspace*{-3mm}
The above restrictions make the \sygus problem significantly easier.
However, a large number of problems do fall into this class (\arsays{x
out of y benchmarks in the \sygus 2015 competition}).
Several previous works address this class of problem (see, for
example, \cite{alur-15,madhusudan-16-pw,reynolds-15}).
The following example shows that a number of commonly occurring
specification mores are point-wise specifications.

\subsection{The Enumerative Solver}
\label{sec:enumeration}

The principal idea behind the enumerative solver is to enumerate all
expressions from the given syntax with some pruning.
Only expressions that are distinct with respect to a set of concrete
input points are enumerated.

\begin{wrapfigure}{l}{0.46\textwidth}
  \begin{minipage}{0.46\textwidth}
    \begin{algorithm}[H]
      \begin{algorithmic}[1]
        \fontsize{8}{10}\selectfont
        \Require Grammar $\Grammar = \tuple { \NonTerminals, \StartSymbol, \Rules }$
        \Require Specification $\Spec$
        \Ensure $\Expr$ s.t.  $\Expr \in \sem{\Grammar} \wedge \Expr \models \Spec$
        \State $\Points \gets \emptyset$ \label{line:basic:init}
        \While{$\mathtt{true}$}
        \For {$\Expr \in \Call{enumerate}{\Grammar,\Points}$ }\label{line:basic:enumerate}
        \If { $\Expr \not\models \Spec \downharpoonleft  \Points$ } \textbf{continue} \EndIf\label{line:basic:concrete_check}
        \State $\CexInput \gets \Verify(\Expr, \Spec)$ \label{line:basic:verify}
        \If { $\CexInput = \bot$ } \Return $\Expr$ \EndIf \label{line:basic:return}
        \State $\Points \gets \Points \cup \CexInput$ \label{line:basic:continue}
        \EndFor
        \EndWhile
      \end{algorithmic}
      \caption{Enumerative Solver}
      \label{algo:basic}
    \end{algorithm}
  \end{minipage}
\end{wrapfigure}
\noindent
The full pseudo-code is given in Algorithm~\ref{algo:basic}.
Initially, the set of points is set to be empty at
line~\ref{line:basic:init}.  In each iteration, the algorithm calls
the \textsc{enumerate} procedure\footnote{Note that \textsc{enumerate} is
a coprocedure.  Unfamiliar readers may assume that each call to \textsc{enumerate}
returns the next expression from an infinite list of
expressions.} which returns a (possibly infinite) list of expressions
such that no two expressions in this list evaluate to the same values
at every point $\Point \in \Points$ (line~\ref{line:basic:enumerate}).
Every expression $\Expr$ in this list is then verified, first on the
set of points (line~\ref{line:basic:concrete_check}) and then fully
(line~\ref{line:basic:verify}).  If the expression $\Expr$ is correct,
it is returned (line~\ref{line:basic:return}).  Otherwise, we pick a
counter-example input point (\ie, an input on which $\Expr$ is
incorrect) and add it to the set of points and repeat
(line~\ref{line:basic:continue}). A full description of the
\textsc{enumerate} procedure can be found in
\ref{section:appendix_esolver}.

\begin{theorem}
  \label{thm:basic_enumeration}
  Given a \sygus instance $(\Spec, \Grammar)$ with at least one solution
  expression, Algorithm~\ref{algo:basic} terminates and returns the
  smallest solution expression.
\end{theorem}

\begin{wrapfigure}{l}{0.5\textwidth}
\centering
\begin{tikzpicture}
\begin{semilogyaxis}[
width=5cm,
height=5cm,
xlabel=Expression Size,
ylabel=Time to enumerate (s),
tick align=outside,
grid=both,
xtick = {2, 4, 6, 8, 10},
ytick = {0.0001, 0.01, 1, 100, 10000, 1000000, 10000000}
]
\addplot[mark=*,color=black] table {enumeration.dat};
\end{semilogyaxis}
\end{tikzpicture}
\caption{Scalability of Enumeration}
\label{figure:random_exponential_graph}
\end{wrapfigure}
\paragraph{Features and Limitations}
The enumerative algorithm performs surprisingly well considering its
simplicity on small to medium sized benchmarks
(see~\cite{udupa-transit,udupa-sygus}).
Further, due to the guarantee of Theorem~\ref{thm:basic_enumeration}
that the enumerative approach produces small solutions, the algorithm is
capable of generalizing from specifications that are input-output
examples.

However, enumeration quickly fails to scale with growing size of
solutions.
Figure~\ref{figure:random_exponential_graph} shows the time taken (in seconds) to
generate all distinct expressions up to a given size for the grammar
shown in Figure~\ref{fig:lia_grammar}.
As can be seen from the graph, the time taken grows exponentially with
the size.



\section{The Divide-and-Conquer Enumeration Algorithm}
\label{sec:algo}

We first present some necessary definitions and concepts for the divide
and conquer enumeration algorithm.

\paragraph{Conditional Expression Grammars}
We introduce conditional expressions grammars that separate an
expression grammar into two grammars that generate:
\begin{inparaenum}[(a)]
\item the return value expression, and
\item the conditionals that decide which return value is chosen.
\end{inparaenum}
These generated return values (terms) and conditionals (predicates) are
combined using if-then-else conditional operators.

A {\em conditional expression grammar} is a pair of grammars $\tuple{
\Grammar_T, \Grammar_P }$ where:
\begin{inparaenum}[(a)]
\item the {\em term grammar} $\Grammar_T$ is an expression grammar
  generating terms of type $\range(\SynthFun)$; and
\item the {\em predicate grammar} $\Grammar_P$ is an expression
  grammar generating boolean terms.
\end{inparaenum}
The set of expressions $\sem{\tuple{ \Grammar_T, \Grammar_P }}$
generated by $\tuple{ G_T, G_P }$ is the smallest set of expressions
$\Theory[\FormalParameters]$ such that:
\begin{inparaenum}[(a)]
\item $\sem{\Grammar_T} \subseteq \sem{\tuple{ \Grammar_T, \Grammar_P
  }}$, and
\item $\Expr_1, \Expr_2 \in \sem{\tuple{ \Grammar_T, \Grammar_P }}
  \wedge \Pred \in \sem{\Grammar_P} \implies
  \ITE{\Pred}{\Expr_1}{\Expr_2} \in \sem{\tuple{ \Grammar_T, \Grammar_P }}$
\end{inparaenum}

Most commonly occurring \sygus grammars in practice can be rewritten
as conditional expression grammars in a straightforward and automated
manner.
However, it is easy to construct a grammar that cannot be decomposed
into a conditional expression grammar.

\begin{example}
  In the running example, the grammar from
  Example~\ref{ex:running:grammar} is easily decomposed into a
  conditional expression grammar $\tuple{\Grammar_T, \Grammar_P}$ where:
  \begin{inparaenum}[(a)]
  \item the term grammar $\Grammar_T$ contains only the non-terminal
    $T$, and the rules for rewriting $T$.
  \item the predicate grammar $\Grammar_P$ contains the two
    non-terminals $\{ T, C \}$ and the associated rules.
  \end{inparaenum}
\end{example}

\paragraph{Decision Trees and Decision Tree Learning}
We use the concept of decision trees from machine learning literature to
model conditional expressions.
Informally, a decision tree $\DecisionTree$ maps {\em samples} to {\em
labels}.
Each internal node in a decision tree contains an {\em attribute} which
may either hold or not for each sample, and each leaf node contains a
label.
In our setting, labels are terms, attributes are predicates, and samples
are points.

To compute the label for a given point, we follow a path from the root
of the decision tree to a leaf, taking the left (resp. right) child at
each internal node if the attribute holds (resp.  does not hold) for the
sample.
The required label is the label at the leaf.
We do not formally define decision trees, but instead refer the reader
to a standard text-book (see, for example,~\cite{bishop-book}).

\begin{example}
  Figure~\ref{fig:dt} contains a decision tree in our setting, i.e.,
  with attributes being predicates and labels being terms.
  To compute the associated label with the point $\Point \equiv \{ x
  \mapsto 2, y \mapsto 0 \}$:
  \begin{inparaenum}[(a)]
  \item we examine the predicate at the root node, i.e., $y \leq 0$ and
    follow the left child as the predicate hold for $\Point$;
  \item examine the predicate at the left child of the root node, i.e,
    $x \leq y$ and follow the right child as it does not hold; and
  \item return the label of the leaf $x + y$.
  \end{inparaenum}
\end{example}

\begin{wrapfigure}{l}{0.33\textwidth}
  \begin{tikzpicture}
    [every tree node/.style={draw}, level distance=18pt,
      level 1/.style={sibling distance=80pt},
      level 2/.style={sibling distance=50pt},
    level 3/.style={sibling distance=25pt}]
    \node[draw] {$y \leq 0$}
      child {node[draw] {$x \leq y$}
        child {node[fill=gray,draw] {$0$}}
        child {node[fill=gray,draw] {$x + y$}}
      }
      child {node[fill=gray,draw] {$y$}}
      ;
  \end{tikzpicture}
  \caption{Sample decision tree}
  \label{fig:dt}
\end{wrapfigure}
Every decision tree corresponds to an if-then-else expression with the
conditionals given by the attributes, and the terms given by the labels.
Formally, the expression $\DTtoExpr{\DecisionTree}$ corresponding to a
decision tree $\DecisionTree$ is defined as:
\begin{inparaenum}[(a)]
\item the label of the root node if the tree is a single leaf node; and
\item
  $\ITE{\Pred}{\DTtoExpr{\DecisionTree_L}}{\DTtoExpr{\DecisionTree_Y}}$
  where $\Pred$ is the attribute of the root node, and $\DecisionTree_L$ and
  $\DecisionTree_Y$ are the left and right children of the root node,
  otherwise
\end{inparaenum}

Decision tree learning is a technique that learns a decision tree from a
given set of samples.
A decision tree learning procedure is given:
\begin{inparaenum}[(a)]
\item a set of samples (points),
\item a set of labels (terms), along with a function that maps a label to the
  subset of samples which it covers; and
\item a set of attributes (predicates).
\end{inparaenum}
A sound decision tree learning algorithm returns a decision tree
$\DecisionTree$ that classifies the points correctly, i.e., for every
sample $\Point$, the label associated to it by the decision tree covers
the point.
We use the notation {\tt LearnDT} to denote a generic sound
decision tree learning procedure.
The exact procedure we use for decision tree learning in
Section~\ref{sec:decision_trees}.

\subsection{Algorithm}
\label{sec:algo:main}

Algorithm~\ref{algo:main} presents the full divide-and-conquer
enumeration algorithm for synthesis.
Like Algorithm~\ref{algo:basic}, the divide-and-conquer algorithm
maintains a set of points $\Points$, and in each iteration:
\begin{inparaenum}[(a)]
\item computes a candidate solution expression $\Expr$
  (lines~\ref{line:main:start_iter}-\ref{line:main:compute_expr});
\item verifies and returns $\Expr$ if it is correct (lines~\ref{line:main:verify}
  and~\ref{line:main:return}); and
\item otherwise, adds the counter-example point into the set $\Points$
  (line~\ref{line:main:add_point}).
\end{inparaenum}

However, the key differences between Algorithm~\ref{algo:main} and
Algorithm~\ref{algo:basic} are in the way the candidate solution
expression $\Expr$ is generated.
The generation of candidate expressions is separated into two separate
steps.

\paragraph{Term solving}
Instead of searching for a single candidate expression that is correct
on all points in $\Points$, Algorithm~\ref{algo:main} maintains a set of
candidate terms $\Terms$.
We say that a term $\Term$ covers a point $\Point \in \Points$ if $\Term
\models \Spec \downharpoonleft \Point$.
The set of points that a term covers is computed and stored in
$\Cover[\Term]$ (line~\ref{line:main:compute_cover}).
Note that the algorithm does not store terms that cover the same set of
points as already generated terms
(line~\ref{line:main:duplicate_cover}).
When the set of terms $\Terms$ covers all the points in $\Points$, i.e., for
each $\Point \in \Points$, there is at least one term that is correct on
$\Point$, the term enumeration is stopped (while-loop condition in
line~\ref{line:term_solver:while}).

\paragraph{Unification and Decision Tree Learning.}
In the next step
(lines~\ref{line:main:unify:start}-\ref{line:main:unify:end}), we
generate a set of predicates $\Predicates$ that will be used as
conditionals to combine the terms from $\Terms$ into if-then-else
expressions.
In each iteration, we attempt to learn a decision tree that correctly
label each point $\Point \in \Points$ with a term $\Term$ such that
$\Point \in \Cover[\Term]$.
If such a decision tree $\DecisionTree$ exists, the conditional
expression $\DTtoExpr{\DecisionTree}$ is correct on all points, i.e.,
$\DTtoExpr{\DecisionTree} \models \Spec \downharpoonleft \Points$.
If a decision tree does not exist, we generate additional terms and
predicates and retry.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Require Conditional expression grammar $\Grammar = \tuple{ \Grammar_T, \Grammar_P }$
    \Require Specification $\Spec$
    \Ensure Expression $\Expr$ s.t.  $\Expr \in \sem{\Grammar} \wedge \Expr \models \Spec$
    \State $\Points \gets \emptyset$ \label{line:main:init}

    \While { $\True$ }
    \State $\Terms \gets \emptyset; \Predicates \gets \emptyset; \Cover \gets \emptyset; \DecisionTree = \bot$ \label{line:main:start_iter}

    \While { $\bigcup_{\Term \in \Terms} \Cover[\Term] \neq \Points$ } \label{line:term_solver:while} \Comment { Term solver }
    \State $\Terms \gets \Terms \cup \Call{NextDistinctTerm}{\Points, \Terms, \Cover$}
    \EndWhile

    \While { $\DecisionTree = \bot$ }\label{line:main:unify:start} \Comment { Unifier }
    \State $\Terms \gets \Terms \cup \Call{NextDistinctTerm}{\Points, \Terms, \Cover$}\label{line:main:more_terms}
    \State $\Predicates \gets \Predicates \cup \Call{Enumerate}{\Grammar_P,\Points}$\label{line:main:more_preds}
    \State $\DecisionTree \gets \Call{LearnDT}{\Terms, \Predicates}$ \label{line:main:unify:end}
    \EndWhile

    \State $\Expr \gets \DTtoExpr{\DecisionTree}$; $\CexInput \gets \Verify(\Expr, \Spec)$  \Comment { Verifier } \label{line:main:compute_expr} \label{line:main:verify}
    \If { $\CexInput = \bot$ } \Return $\Expr$ \EndIf \label{line:main:return}
    \State $\Points \gets \Points \cup \CexInput$ \label{line:main:add_point}
    \EndWhile

    \Function{NextDistinctTerm}{$\Points, \Terms, \Cover$}
    \While { $\True$ }
    \State $\Term \gets \Call{Enumerate}{\Grammar_T,\Points}; \Cover[\Term] \gets \{ \Point \mid \Point \in \Points \wedge \Term \models \Spec \downharpoonleft \Point \} $\label{line:main:compute_cover}
    \If { $\forall \Term' \in \Terms : \Cover[\Term] \neq
    \Cover[\Term']$ } \Return $\Term$ \EndIf \label{line:main:duplicate_cover}
    \EndWhile
    \EndFunction
  \end{algorithmic}
  \caption{The divide-and-conquer enumeration algorithm}
  \label{algo:main}
\end{algorithm}

\begin{remark}
  In line~\ref{line:main:more_terms}, we generate additional terms even
  though $\Terms$ is guaranteed to contain terms that cover all points.
  This is required to achieve semi-completeness, i.e., without this, the
  algorithm might not find a solution even if one exists.
  See the appendix for an example.
\end{remark}

\begin{theorem}
  Algorithm~\ref{algo:main} is a sound for the \sygus problem.
  Further, assuming a sound and complete {\tt LearnDT} procedure, if
  there exists a solution expression, Algorithm~\ref{algo:main} is
  guaranteed to find it.
\end{theorem}
The proof of the above theorem is similar to the proof of soundness and
partial-completeness for the original enumerative solver.
The only additional assumption is that the {\tt LearnDT} decision tree
learning procedure will return a decision tree if one exists.
We present such a procedure in the next section.


\subsection{Decision Tree Learning}
\label{sec:decision_trees}

The standard multi-label decision tree learning algorithm (based on
ID3~\cite{quinlan-86}) is presented in Algorithm~\ref{algo:dt_learning}.
The algorithm first checks if there exists a single label (i.e., term)
$\Term$ that applies to all the points (line~\ref{line:dt:single}).
If so, it returns a decision tree with only one leaf whose label is
$\Term$ (line~\ref{line:dt:leaf}).
Otherwise, it picks the best predicate $\Pred$ to split on based on some
heuristic (line~\ref{line:dt:pick_pred}).
It the recursively computes the left and right sub-trees for the set of
points on which $\Pred$ holds and does not hold, respectively
(lines~\ref{line:dt:left} and~\ref{line:dt:right}).
The final decision tree is returned as a tree with a root (with
attribute $\Pred$), and positive and negative edges to the roots of the
left and right sub-trees, respectively.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Require $\Points$, $\Terms$, $\Cover$, $\Predicates$
    \Ensure Decision tree $\DecisionTree$
    \If { $\exists \Term : \Points \subseteq \Cover[\Term]$ }\label{line:dt:single}
    \Return $\mathit{LeafNode}[\Label \gets \Term]$ \label{line:dt:leaf}
    \EndIf
    \State $\Pred \gets \mbox{Pick predicate from $\Predicates$}$\label{line:dt:pick_pred}
    \State $L \gets \Call{LearnDT}{\{ \Point \mid \Pred[\Point] \},
      \Terms, \Cover, \Predicates }$\label{line:dt:left}
    \State $R \gets \Call{LearnDT}{\{ \Point \mid \neg \Pred[\Point]
      \}, \Terms, \Cover, \Predicates }$\label{line:dt:right}
    \State \Return $\mathit{InternalNode}[\Attribute \gets \Pred,  \mathit{left} \gets L , \mathit{right} \gets R ]$
  \end{algorithmic}
  \caption{Learning Decision Trees}
  \label{algo:dt_learning}
\end{algorithm}

\paragraph{Information-gain heuristic}
The choice of the predicate at line~\ref{line:dt:pick_pred} influences
the size of the decision tree learned by
Algorithm~\ref{algo:dt_learning}, and hence, in our setting, the size of
the solution expression generated by Algorithm~\ref{algo:main}.
We use the classical information gain heuristic to pick the predicates.
Informally, the information gain heuristic treats the label as a random
variable, and picks to split on the attribute whose value will reveal
the most information about the label.
We do not describe all aspects of computing information gain, but
refer the reader to any standard textbook on machine
learning~\cite{bishop-book}.
Given a set of points $\Points' \subseteq \Points$ the entropy
$H(\Points')$ is defined in terms of the probability
$\Prob{\Points'}{label(\Point) = t}$ of a point $\Point \in \Point'$ being
labeled with the term $\Term$ as
\[
H(\Points') = -\sum_{\Term} \Prob{\Points'}{label(\Point) = \Term} \cdot \log_2{\Prob{\Points'}{label(\Point) = \Term}}
\]
Further, given a predicate $\Pred \in \Predicates$, the information
gain of $\Pred$ is defined as
\[
G(\Pred) = \frac{|\Points_{Y}|}{|\Points|} \cdot
H(\Points_{Y}) + \frac{|\Points_{N}|}{|\Points|} \cdot
H(\Points_{N})
\]
where $\Points_{Y} = \{\Point \in \Points \mid \Pred[\Point]\}$ and
$\Points_{N} = \{\Point \in \Points \mid \neg\Pred[\Point]\}$.
Hence, at line~\ref{line:dt:pick_pred}, we compute the value $G(\Pred)$
for each predicate in $\Predicates$, and pick the one which maximizes
$G(\Pred)$.

We use conditional probabilities $\Prob{\Points'}{label(\Point) = \Term \mid
\Point}$ to compute $\Prob{\Points'}{label(\Point) = t}$.
The assumption we make about the prior distribution is that the
likelihood of a given point $\Point$ being labelled by a given term
$\Term$ is proportional to the number of points in $\Cover[\Term]$.
Formally, we define:
\[
    \Prob{\Points'}{label(\Point) = t \mid \Point} =
    \begin{cases}
        \qquad\qquad\quad 0  & \mathrm{if}\ \Point \notin \Cover[\Term]\\
      \displaystyle
      \frac{\strut\displaystyle\vert \Cover[\Term] \cap \Points' \vert}{\strut\displaystyle\sum_{\Term'
          | \Point \in \Cover[\Term']} \vert \Cover[\Term'] \cap \Points' \vert } & \mathrm{if}\
      \Point \in \Cover[\Term] \\
    \end{cases}
\]
Now, the unconditional probability of an arbitrary point being labelled
with $\Term$ is given by $\Prob{\Points'}{label(\Point) = \Term} = \sum_{\Point}
\Prob{\Points'}{label(\Point) = \Term \mid \Point}\cdot\Prob{\Points'}{\Point}$.
Assuming a uniform distribution for picking points, we have that
\[
    \Prob{\Points'}{label(\Point) = t} =  \frac{1}{\vert \Points \vert} \cdot \sum_{\Point} \Prob{\Points'}{label(\Point) = \Term \mid \Point}
\]


\subsection{Extensions and Optimizations}
\label{sec:optimizations}

We will now discuss several extensions and optimizations for
Algorithm~\ref{algo:main}.


\paragraph{Improving Synthesis Results}
In the presentation from Section~\ref{sec:algo:main}, the algorithm
stops enumeration of terms and predicates as soon as it finds a single
solution to the synthesis problem.
However, there are cases where due to the lack of sufficiently good
predicates, the decision tree and the resulting solution can be large
(see Example~\ref{ex:continuing}).
Instead, we can let the algorithm continue running by generating more
terms and predicates after the first solution is found.
This could lead to different, potentially smaller decision trees and
solutions.
Note that this is in contrast to divide-and-conquer techniques that use
constraint solving to generate predicates~\cite{alur-15,madhusudan-16-pw}.
In general, constraint solvers cannot be tuned to give alternate
solutions.

\begin{example}
  \label{ex:continuing}
  Consider the specification $(x \geq 0 \wedge y \geq 0) \implies (\SynthFun(x, y) = 1
  \Leftrightarrow x + y \leq 2)$ and a run of Algorithm~\ref{algo:main}
  where the terms $0$ and $1$ are generated.
  These terms fully cover any set of points for this specification.

  Over a sequence of iterations the predicates are generated in order of
  size.
  Now, for predicate size $3$, the predicates generated include $x = 0$,
  $x = 1$, $x = 2$, $y \leq 2$, $y \leq 1$, and $y \leq 0$.
  With these predicates, the decision tree depicted in
  Figure~\ref{fig:dt:large} is learned, and the corresponding
  conditional expression is correct for the specification.
  However, if the procedure continues to run after the first solution is
  generated, predicates of size $4$ are generated.
  Among these predicates, the predicate $x + y \leq 2$ is also
  generated.
  With this additional predicate, the decision tree in
  Figure~\ref{fig:dt:small} is generated, leading to the compact
  solution $\SynthFun(x, y) \equiv \mathtt{if}~x + y \leq
  2~\mathtt{then}~1~\mathtt{else}~0$.
\end{example}

\begin{figure}
  \begin{subfigure}{0.5\textwidth}
    \begin{tikzpicture}
      [every tree node/.style={draw}, level distance=18pt,
      level 1/.style={sibling distance=80pt},
      level 2/.style={sibling distance=50pt},
      level 3/.style={sibling distance=25pt}]
      \node[draw] {$x = 0$}
        child {node[draw] {$y \leq 2$}
          child {node[fill=gray,draw] {$1$}}
          child {node[fill=gray,draw] {$0$}}
        }
        child {node[draw] {$x = 1$}
          child {node[draw] {$y \leq 1$}
            child {node[fill=gray,draw] {$1$}}
            child {node[fill=gray,draw] {$0$}}
          }
          child {node[draw] {$x = 2$}
            child {node[draw] {$y \leq 0$}
              child {node[fill=gray,draw] {$1$}}
              child {node[fill=gray,draw] {$0$}}
            }
            child {node[fill=gray,draw] {$0$}}
          }
        }
        ;
    \end{tikzpicture}
    \caption{Decision tree for predicates of size $3$}
    \label{fig:dt:large}
  \end{subfigure}
  \begin{subfigure}{0.5\textwidth}
    \centering
    \begin{tikzpicture}
      \node[draw] {$x + y \leq 2$}
        child {node [fill=gray,draw] {$1$}}
        child {node [fill=gray,draw] {$0$}}
      ;
    \end{tikzpicture}
    \caption{Decision tree for predicates of size $4$}
    \label{fig:dt:small}
  \end{subfigure}
\end{figure}

\paragraph{Decision tree repair}
In Algorithm~\ref{algo:main}, we discard the terms and predicates that
are equivalent to already generated terms and predicates on the selected
points $\Points$ in line~\ref{line:main:duplicate_cover}.
However, these discarded terms may lead to better solutions than the
already generated ones.

\begin{example}
  \label{ex:dt_repair}
  Consider a run of the algorithm on the specification for the running
  example, where the set $\Points$ contains the points $\{ x \mapsto 1,
  y \mapsto 0 \}$ and $\{ x \mapsto -1, y \mapsto 0 \}$.
  Suppose the algorithm first generates the terms $0$ and $1$.
  These terms are each correct on one of the points and are added to
  $\Terms$.
  Next, the algorithm generates the terms $x$ and $y$.
  However, these are not added to $\Terms$ as $x$ (resp. $y$) is correct
  on exactly the same set of points as $1$ (resp. $0$).

  Suppose the algorithm also generates the predicate $x \leq y$ and
  learns the decision tree corresponding to the expression $\Expr \equiv
  \mathtt{if}~x \leq y~\mathtt{then}~0~\mathtt{else}~1$.
  Now, verifying this expression produces a counter-example point, say
  $\{ x \mapsto 1, y \mapsto 2 \}$.
  While the term $0$, and correspondingly, the expression $\Expr$ is
  incorrect on this point, the term $y$ which was discarded as an
  equivalent term to $0$, is correct.
\end{example}

Hence, for a practical implementation of the algorithm we do not discard
these terms and predicates, but store them separately in a map $\Equiv :
\Terms \to \sem{\Grammar_T}$ that maps the terms in
$\Terms$ to an additional set of equivalent terms.
At lines~\ref{line:main:duplicate_cover}, if the check for distinctness
fails, we instead add the term $\Term$ to the $\Equiv$ map.
Now, when the decision tree learning algorithm returns an expression
that fails to verify and returns a counter-example, we attempt to
replace terms and predicates in the decision tree with equivalent ones
from the $\Equiv$ map to make the decision tree behave correctly on the
counter-example.

\begin{example}
  Revisiting Example~\ref{ex:dt_repair}, instead of discarding the terms
  $x$ and $y$, we store them into the $\Equiv$ array, i.e., we set
  $\Equiv(0) = \{ y \}$ and $\Equiv(1) = \{ x \}$.
  Now, when the verification of the expression fails, with the
  counter-example point $\{ x \mapsto 1, y \mapsto 2 \}$, we check the
  term that is returned for the counter-example point--here, $0$.
  Now, we whether any term in $\Equiv(0)$ is correct on the
  counter-example point--here, the term $y$.
  If so, we replace the original term with the equivalent term that is
  additionally correct on the counter-example point and proceed with
  verification.
  Replacing $0$ with $y$ in the expression gives us $\mathtt{if}~x \leq
  y~\mathtt{then}~y~\mathtt{else}~1$.
  Another round of verification and decision tree repair will lead to
  replacing the term $1$ with $x$, giving us the final correct solution.
\end{example}

\paragraph{Branch-wise verification}
In Algorithm~\ref{algo:main}, and in most synthesis techniques, an
incorrect candidate solution is used to generate $1$ counter-example
point.
However, in the case of conditional expressions and point-wise
specifications, each branch (i.e., leaf of the decision tree) can be
verified separately.
Verifying each branch involves rewriting the specification as in the
point-wise verification defined in Section~\ref{sec:problem} -- but
instead of adding a premise to each clause asserting that the arguments
to the function are equal to a point, we add a premise that asserts that
the arguments satisfy all predicates along the path to the leaf.
This gives us two separate advantages:
\begin{compactitem}
\item We are able to generate multiple counter-examples from a single
  incorrect expression.
  This reduces the total number of iterations required, as well as the
  number of calls to the expensive decision tree learning algorithm.
\item It reduces the complexity of each call to the verifier in terms of
  the size of the SMT formula to be checked.
  As verification procedures generally scale exponentially with respect
  to the size of the SMT formula, multiple simpler verification calls
  are often faster than one more complex call.
\end{compactitem}
This optimization works very well along with the decision tree repair
described above as we can verify and repair each branch of the decision
tree separately.

\begin{example}
  Consider the verification of the expression $\ITE{x \leq y}{0}{1}$
  for the running example specification $\Spec$.
  Instead of running the full expression through the verifier and
  obtaining one counter-example point, we instead verify the two
  branches separately.
  This involves checking the satisfiability of the formulae $x \leq y
  \wedge f(x, y) = 0 \wedge \neg \Spec$ and $\neg (x \leq y) \wedge f(x,
  y) = 1 \wedge \neg \Spec$.
  This gives us two separate counter-example points as both branches are
  incorrect.
\end{example}

\section{Evaluation}
\label{sec:evaluation}
% \input{results_one_shot_table}
\input{results_anytime_table}
\input{results_one_shot_max_table}

We built a prototype \sygus solver that uses the divide-and-conquer
enumerative algorithm.
% based on the ideas presented in this paper.
The tool consists of 3000 lines of Python code implementing the
enumeration, and 3000 lines of C$\raisebox{1pt}{+\!+}$ code implementing
the decision tree learning.
% The algorithms for enumeration were implemented in
% about 3000 lines of Python code, while the algorithms for decision
% tree learning were implemented in about 3000 lines of
% C$\raisebox{1pt}{+\!+}$ code.
All experiments were executed on a
32-core machine with four Intel Xeon E7-4820 processors
% running at 2.0 GHz,
and 128 GB of memory, with a one hour time limit.

% \subsection{Goals, Benchmarks and the State-of-the-art}
\paragraph{Goals}
We seek to empirically evaluate how our synthesis algorithm compares
to other state-of-the-art synthesis techniques along the following
dimensions:
\begin{inparaenum}[(a)]
\item
\emph{Performance}: How quickly can the algorithms arrive at a correct
solution?
\item
\emph{Quality}: How \emph{good} are the solutions produced by the
algorithms? We use compactness of solutions
% , in terms of syntactic size,
as a metric for the quality of solutions.
\item
\emph{Effect of continued execution}: How significant is the
improvement in the quality of the solutions generated
% if the algorithm is
given an additional (but fixed) time budget.
\end{inparaenum}

% To compare and contrast the \dcsolve algorithm with contemporary
% techniques along each of these dimensions,
% We did not consider the other classes in the general-track \sygus
% benchmarks because:
% \begin{inparaenum}[(a)]
% \item the solutions were not conditional expressions, or
% \item the benchmarks were not plainly separable, or
% \item the benchmarks were similar to the ones that we considered.
% \end{inparaenum}

\paragraph{Benchmarks}
We considered two classes of \sygus benchmarks taken from the \sygus
competition 2015.
The first class that we considered consists of the ICFP
benchmarks. These are bit-twiddling problems with
\begin{inparaenum}[(a)]
\item very specific syntactic restrictions on the operations allowed,
  and
\item behaviour specified using only concrete input-output examples
\end{inparaenum}
To the best of our knowledge, no existing \sygus solver has been able to
solve these benchmarks.
The results of running our solver on these benchmarks is summarized in
Table~\ref{table:anytime_results}.
For each benchmark, we report
\begin{inparaenum}[(a)]
\item the size of the first solution discovered, the time at which it
  was discovered, and the associated decision tree size;
\item the same details for the minimal solution discovered; and
\item whether the first solution discovered was minimal.
\end{inparaenum}
% The first column of
% Table~\ref{table:anytime_results} gives the name of the benchmark, the
% second, third, and fourth columns give the size of the first solution
% discovered, the time at which it was discoverd by the algorithm, the time taken to discover the solution,
% and the size of the decision tree associated with the solution,
% respectively. The next three columns provide the same details, but for
% the \emph{minimal} sized solution discovered by the
% algorithm.
% Finally, the last column indicates whether the solution
% that was discovered earliest was also the solution with the minimal
% size.

The second class of benchmarks that we considered compute the
maximum of $n$ integers, where $n$ is a
parameter.
Table~\ref{table:max_results} demonstrates how our algorithm stacks up
against the state-of-the-art CVC4 solver.
The columns indicate the sizes of solutions generated, the time to arrive
at the solution, as well as the number of comparisons in the solution
obtained by each of the solvers, for different values of the parameter
$n$, indicated in the first column.

\comment{
One can broadly categorize the existing \sygus solvers into two
categories:\begin{inparaenum}[(a)]
\item
\emph{Black-box solvers}, which do not leverage the correctness
specification to guide the search for solutions. Some examples are
\esolver and the stochastic solver~\cite{udupa-sygus}, as well as the
\dcsolve algorithm.
\emph{White-box solvers}, which actively leverage the correctness
specification to generate terms and predicates for solutions. Examples
for white-box solvers include the solvers based on the \textsc{stun}
algorithms~\cite{alur-15}, and the CVC4 solver~\cite{reynolds-15}.
\end{inparaenum}
}

\paragraph{The State-of-the-art}
Empirically, black-box algorithms are able to generalize well from
underconstrained correctness specification, whereas it is not clear
how white-box solvers can generalize from such specifications, given
their heavy reliance on the specifications being complete.
We note that \esolver, which employs an optimized
version of the na\"ive enumerative algorithm discussed earlier won the
2014 \sygus competition and came in second in the 2015 \sygus
competition, losing to the CVC4 solver, which came in first.

\subsection{Discussion}
% We compare the \dcsolve algorithm compares to contemporary
% solvers along the three dimensions described earlier.

\paragraph{Performance}
As Table~\ref{table:anytime_results} demonstrates, our algorithm was
able to produce a solution for 47 out of the 50 ICFP benchmarks,
within an hour. We emphasize that we know of no other solver that has
been able to provide satisfactory solutions to these
benchmarks.
The solutions produced by our divide and conquer algorithm, while being
compact, are often of size larger than $20$, indicating that the
solutions are beyond the reach of a na\"ive enumerative strategy.
% Further, as Figure~\ref{figure:random_exponential_graph}
% indicates, it would be impossible to explore expressions of the size
% indicated in Table~\ref{table:anytime_results} --- which are often
% larger than 20 --- using a na\"ive enumeration strategy.
% such as the one used by \esolver.
% The solutions produced by our divide and
% conquer algorithm are quite compact, and
Of the $47$ solved instances, only $5$ benchmarks take longer than $10$
minutes.
% of them are computed in less than ten minutes, with only five
% benchmarks taking longer.

Table~\ref{table:max_results} indicates that our algorithm is competitive
with the state-of-the-art CVC4 solver on the parameterized ``max of
$n$ numbers'' benchmarks, for smaller values of $n$. For larger
values of $n$, our algorithm is not as performant. Our investigations
indicated that in the performance drops are largely due to the large
number of concrete counterexample points that the algorithm maintains,
which in turn leads to the decision tree learning algorithms taking
longer.

% To summarize, the \dcsolve algorithm demonstrates vastly better
% performance than other black-box solvers on both benchmark suites, and
% white-box solvers on the ICFP benchmarks.
% For the linear integer arithmetic benchmarks, its performance is
% competitive with white-box solvers.
% while also being able to
% generalize well from input-output examples as we will now discuss.

\paragraph{Quality of Solutions}
In the case of the ICFP benchmarks, Table~\ref{table:anytime_results}
indicates that most of the solutions that our algorithm generates are
rather compact, and are of a size that are comprehensible to a
human. We do not know if solutions smaller than these exist. The CVC4
solver has been able to synthesize solutions for some of the ICFP
benchmarks, {\em but only when syntactic restrictions are lifted}. However,
upon closer inspection, we discovered that the solutions generated are a
large case-split corresponding to the input-output examples that form
the constraints for the problem. Although technically correct, the
solutions are extremely large, perform no generalization and are
unlikely to be the intended or desired solutions.
% We emphasize that no solution generated by our algorithm is a similar large
% conditional expression that matches inputs to outputs.
The solutions that we report are all compact expressions that perform some
computation on the input, to return an output.

In the case of the ``max'' benchmarks, we again see from
Table~\ref{table:max_results} that our algorithm performs well for
small parameter values, often beating the solutions obtained from the
CVC4 solver in terms of size and number of comparisons.
At larger parameter values, the solutions are still reasonably compact
and not excessively large.
The reason for the somewhat larger solutions for
larger parameter values is that the decision tree learning algorithm
is not optimal, \ie, it does not guarantee the most compact decision
tree. Instead a greedy heuristic is used, which rapidly yields
suboptimal solutions as the number of samples (counterexample points)
increases.
% as with the ``max'' benchmarks for larger
% parameter values.
We intend to explore how we can adapt other
heuristics~\cite{madhusudan-16-pw}, into the setting of our problem, in
future work.

\paragraph{Effect of Continued Execution}
For about half the ICFP benchmarks, we were able to obtain a more
compact solution by letting the algorithm continue execution after the
first solution was discovered (last column in
Table~\ref{table:anytime_results}).
Also, the difference in the first and
smallest solutions is sometimes very significant, for example, in the
case of the ``icfp\_95\_100'' benchmark, we see the size of the
solution went down from 220 to 24.
% by almost an order of magnitude, from 220 to 24.
An interesting phenomenon that we observed was that while the size of
the decision tree almost always went down with time, the size of the
solutions sometimes increased.
% our algorithm would
% sometimes find \emph{larger} solutions when allowed to run
% longer.
% However, the size of the decision trees almost always reduced with time.
This is because the algorithm generated larger terms and predicates
over time, increasing the size of the labels and attributes in each node
of the decision tree.

% It turns out that although the decision trees learned were smaller,
% running the algorithm longer also meant that larger terms and predicates
% were enumerated, thus leading to the overall expression sizes of the
% solutions to sometimes increase with time.

Overall, our experimental evaluation suggests that:
% thus allows us to draw the following conclusions:
\begin{inparaenum}[(a)]
\item
The \dcsolve algorithm is able to quickly learn compact solutions,
% for a large fraction of the ICFP benchmarks,
and generalizes well from input-output examples.
\item
The anytime nature of the \dcsolve algorithm is often useful in
reducing the size of the computed solution;
% by allowing additional execution time.
\item
The \dcsolve algorithm works competently on problems from different
classes of benchmarks;
% domaother
% classes of benchmarks that admit conditional expressions as solutions,
% such as the parameterized ``max'' benchmarks.
\end{inparaenum}

\section{Concluding Remarks}
\label{sec:conclusion}

\subsection{Related Work}
Program synthesis has seen a revived interest in the last decade,
starting from the \sketch
framework~\cite{solar-lezama-05,solar-lezama-06}. \sketch uses an
unoptimized C program as a correctness specification, and a partial
program with \emph{generator} expressions to specify syntactic
restrictions, and proposed the counterexample guided inductive
synthesis (CEGIS) strategy for synthesis. Almost all synthesis
algorithms proposed in recent literature --- including the one
described in this paper --- can be viewed as an instantiation of the
CEGIS strategy.  More recently, constraint based synthesis approaches
have been applied to synthesize loop-free programs~\cite{jha-10,
gulwani-pldi-11}. Research in synthesis of string manipulating
programs using input-output examples has found applications in
Microsoft's FlashFill~\cite{gulwani-popl-11}, and the ideas have been
generalized for other domains in a meta-synthesis framework called
FlashMeta~\cite{polozov-15}.

In the area of \sygus solvers, solvers based on enumerative
search~\cite{udupa-transit}, stochastic
search~\cite{schkufza-13,udupa-sygus} and symbolic
search~\cite{gulwani-pldi-11} were among the first solvers
developed. The \sketch approach has also been used to develop \sygus
solvers~\cite{jeon-15}. Alchemist~\cite{saha-15} is another solver
that is quite competitive on benchmarks in the linear arithmetic
domains. More recently, white box solvers like the CVC4
solver~\cite{reynolds-15} and the unification based
solver~\cite{alur-15} have also been developed.

The enumerative synthesis algorithm used by
\esolver~\cite{udupa-transit, udupa-sygus} and the work on using decision
trees for piece-wise functions~\cite{madhusudan-16-pw} are perhaps the
most closely related to the work described in this paper. We have
already discussed at length the shortcomings of \esolver that our
algorithm overcomes. The approach for learning piece-wise
functions~\cite{madhusudan-16-pw}, which we refer to as \pwsolve, also
uses decision trees, but differs from our approach in several ways:
\begin{inparaenum}[(a)]
\item
\pwsolve uses a different set of heuristics based on hitting sets,
rather than the cover-based prior distribution that we use,
\item
\pwsolve presents a generic algorithm which uses decision trees to
learn a classifier, as well as thresholds for linear
inequalities. Although presented in a generic manner, the algorithm
has been instantiated only for the domain of linear arithmetic. It is
unclear how the approach can be instantiated for other
domains. \dcsolve on the other hand uses decision trees to strictly
learn a classifier, given some Boolean-valued attributes. Neither the
decision tree learning algorithm, nor the enumeration is
domain-specific, making \dcsolve a generic algorithm that can be
applied on any domain.
\item
Conversely, \pwsolve can easily learn constants in the linear
integer domain. This is something that enumerative approaches,
including \dcsolve struggle to do, often representing a constant like
``42'' as a sum of smaller constants that appear in the grammar.
\item
Finally, it is unclear how \pwsolve can handle syntactic restrictions
in a general manner, whereas \dcsolve can handle syntactic
restrictions of any form, being based on enumerative
approaches.
\end{inparaenum}

Other recent work in the area of program synthesis in general have
used type-theoretic approaches~\cite{gvero-13,osera-15,frankle-16} for
program completion and for generating code snippets. Synthesis of
recursive programs and data structure manipulating code is yet another
application of program synthesis that has been studied in recent
years~\cite{kuncak-10,kneuss-13,albarghouthi-13,feser-15}. Lastly,
decision trees have also been applied to learn program
invariants~\cite{garg-16}.

\subsection{Conclusion}
This paper has presented a new enumerative algorithm to solve
instances of the Syntax-Guided Synthesis (\sygus) problem. The
algorithm overcomes the shortcomings of a na\"ive enumerative
algorithm by using enumeration to only learn small expressions which
are correct on subsets of the inputs. These expressions are then used
to form a conditional expression using Boolean combinations of
enumerated predicates using decision trees. We have demonstrated the
ability of the algorithm to generalize from input-output examples by
solving most of the previously unsolved ICFP benchmarks, which are
part of the \sygus benchmark suite. The algorithm is generic and
competitive with the state-of-the-art techniques in other domains as
well. The algorithm produces compact solutions, and is \emph{anytime}
--- in that continued execution can potentially produce more compact
solutions than the initially discovered solution.


\bibliographystyle{plainnat}
\setlength{\bibsep}{1pt}
\begin{small}
\bibliography{main}
\end{small}

\input{appendix}

\arsays{Madhu's work assumes that the predicate domain can distinguish
any two points?}

\comment{
Formally, a {\em decision tree} $\DecisionTree$  is a tuple $\tuple{
\Nodes, (\NodesInternal, \NodesLeaf), \node_0, \Edges, (\EdgesYes,
\EdgesNo), \Attribute, \Label }$ where:
\begin{inparaenum}[(a)]
\item $(\Nodes, \Edges)$ form a rooted binary tree with root node
    $\node_0 \in \Nodes$;
\item The nodes $\Nodes$ are partitioned into a set of internal nodes
    $\NodesInternal$ and leaf nodes $\NodesLeaf$;
\item The attribute function $\Attribute : \NodesInternal \to
    \sem{\Grammar_P}$ maps internal nodes to predicates;
\item The label function $\Label : \NodesLeaf \to \sem{\Grammar_T}$ maps
    leaf nodes to terms;
\item The edges $\Edges$ are partitioned into positive edges $\EdgesYes$
  and negative edges $\EdgesNo$ with each internal node being the source
  of one positive and one negative edge.
  We denote the children of an internal node connected through a
  positive (resp. negative) edge the left (resp. right) child.
\end{inparaenum}
}

\end{document}
