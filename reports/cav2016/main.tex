\documentclass{llncs}

\sloppy

\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{paralist}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{mdwtab}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[square,numbers,sort&compress,comma,sectionbib]{natbib}
\renewcommand{\bibname}{References}
\captionsetup{compatibility=false}

\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\newcommand\comment[1]{}
\newcommand\arsays[1]{{\bf AR: #1}}
\newcommand\ausays[1]{{\bf AU: #1}}

\newcommand\tuple[1]{\langle #1 \rangle}
\newcommand\True{\mathit{True}}
\newcommand\None{\mathit{None}}
\newcommand\Points{\mathit{Pts}}
\newcommand\Point{\mathit{Pt}}
\newcommand\Verify{\mathit{Verify}}
\newcommand\CexInput{\mathit{CexPt}}
\newcommand\Predicates{\mathit{Preds}}
\newcommand\Expr{e}
\newcommand\Pred{p}
\newcommand\Terms{\mathit{Terms}}
\newcommand\Term{t}
\newcommand\Cover{\mathit{Cover}}
\newcommand\Powerset[1]{\mathbf{2}^{#1}}
\newcommand\Spec{\Phi}
\newcommand\Size{K}
\newcommand\Grammar{G}
\newcommand\sem[1]{[\![ #1 ]\!]}
\newcommand\SynthFun{f}
\newcommand\range{\mathit{range}}
\newcommand\FormalParameters{\mathit{Params}}
\newcommand\Productions{\mathit{Prodn}}
\newcommand\NonTerminals{\mathcal{N}}
\newcommand\NonTerminal{N}
\newcommand\StartSymbol{S}
\newcommand\Symbols{\mathit{Symbols}}
\newcommand\Rules{\mathcal{R}}
\newcommand\Rule{R}
\newcommand\Theory{\mathcal{T}}
\newcommand\RewritesTo{\rightarrow}
\newcommand\ITE[3]{\mathtt{if}~#1~\mathtt{then}~#2~\mathtt{else}~#3}

\newcommand\DecisionTree{\mathit{DT}}
\newcommand\DTtoExpr[1]{\mathit{Expr}(#1)}
\newcommand\NodesInternal{V_I}
\newcommand\Nodes{V}
\newcommand\node{v}
\newcommand\NodesLeaf{V_L}
\newcommand\EdgesYes{E_Y}
\newcommand\EdgesNo{E_N}
\newcommand\Edges{E}
\newcommand\Attribute{\mathcal{A}}
\newcommand\Label{\mathcal{L}}
\newcommand{\sygus}{{\sffamily\fontsize{8}{10}\selectfont SyGuS}\xspace}

% Make floats and equations behave sensibly
\setlength{\intextsep}{1pt}
\setlength{\textfloatsep}{1pt}
\setlength{\floatsep}{1pt}
\abovedisplayskip=6pt plus 3pt minus 9pt
\abovedisplayshortskip=0pt plus 3pt
\belowdisplayskip=6pt plus 3pt minus 9pt
\belowdisplayshortskip=7pt plus 3pt minus 4pt
\setlength\abovecaptionskip{1pt}
\setlength\belowcaptionskip{1pt}

\begin{document}

\title{Scaling Enumeration for SyGuS through Divide-and-Conquer}
% \author{Rajeev Alur \and Arjun Radhakrishna \and Abhishek Udupa}
\author{}
% \institute{University of Pennsylvania}
\institute{}
\maketitle
\vspace*{-6ex}

\begin{abstract}
  Given a semantic constraint specified by a logical formula and
  syntactic constraint specified by a context-free grammar, the
  Syntax-Guided Synthesis (SyGuS) problem is to find an expression
  that satisfies both the syntactic and semantic constraints.
  An enumerative approach to solve this problem is to systematically
  generates all expressions from the syntactic space with some pruning,
  and has proved to be surprisingly competitive in the newly started
  competition of SyGuS solvers.  It performs well on small to medium sized
  benchmarks, produces succinct expressions, and has the ability to
  generalize from input-output examples.  However, its performance
  degrades drastically with the size of the smallest solution. To overcome
  this limitation, in this paper we propose an alternative approach to
  solve SyGuS instances.

  The key idea  is to employ a divide-and-conquer approach by
  separately enumerating (a) smaller expressions that are correct on
  subsets of inputs, and (b) predicates on inputs that distinguish these
  subsets.  These smaller expressions and predicates are then combined
  using decision trees to obtain an expression that is correct on all
  inputs.  We view the problem of combining expressions and predicates as
  a multi-label decision tree learning problem. We propose a novel
  technique of associating a probability distribution over the set of
  labels that a sample can be labeled with. This enables us to use
  standard information-gain based heuristics to learn a compact decision
  tree.

  We report a prototype implementation and evaluate it on the benchmarks
  from the SyGuS 2015 competition. Our tool is able to match the running
  times and the succinctness of solutions of both standard enumerative
  solver as well as the latest white-box solvers in most cases.  Further,
  our solver is able to solve a large number of instances from the ICFP
  class of benchmarks, which were previously unsolved by all existing
  solvers.
\end{abstract}

\arsays{Madhu's work assumes that the predicate domain can distinguish
any two points?}

\section{Introduction}
\label{sec:intro}

\section{Illustrative Examples}
\label{sec:example}

\arsays{Make an example where all pieces are generated at a small size,
but the full expression takes much longer}


\section{Problem Statement and Background}\marginpar{2 pages}
\label{sec:problem}

Let us fix the function to be synthesized $\SynthFun$ and its formal
parameters $\FormalParameters$.
We write $\range(\SynthFun)$ to denote the range of $\SynthFun$.
The term {\em point} denotes a valuation of $\FormalParameters$, i.e., a
point is an input to $\SynthFun$.

\paragraph{Specifications}
Satisfiability modulo theory formulae have become the standard formalism
for specifying semantic constraints for synthesis.
In the rest of this paper, we fix an arbitrary theory $\Theory$.
We denote by $\Theory[\Symbols]$, the set of $\Theory$ terms over
the symbols $\Symbols$.
A {\em specification} $\Spec$ is a logical formula in a theory $\Theory$
over standard theory symbols and the function to be synthesized
$\SynthFun$.

We say that an expression $\Expr$ satisfies a specification $\Spec$ if
substituting the function to by synthesized by $\Expr$ makes $\Spec$
true.
Formally, $\Expr \models \Spec$ if $\Spec[\Expr/\SynthFun]$ is a valid
sentence.

\paragraph{Grammars}
An {\em expression grammar} $\Grammar$ is a tuple $\tuple {
\NonTerminals, \StartSymbol, \Rules }$ where:
\begin{inparaenum}[(a)]
\item the set $\NonTerminals$ is a set of non-terminal symbols,
\item the non-terminal $\StartSymbol \in \NonTerminals$ is the initial non-terminal,
\item $\Rules \subseteq \NonTerminals \times
  \Theory[\NonTerminals \cup \FormalParameters]$ is a finite set
  of rewrite rules that map $\NonTerminals$ to $\Theory$-expressions
  over non-terminals and formal parameters.
\end{inparaenum}
We say that an expression $\Expr$ {\em rewrites to} an incomplete
expression $\Expr'$ (written as $\Expr \RewritesTo_\Grammar \Expr'$) if
there exists a rule $\Rule = (\NonTerminal, \Expr'') \in \Rules$ and
$\Expr'$ is obtained by replacing one occurrence of $\NonTerminal$ in
$\Expr$ by $\Expr''$.
Let $\RewritesTo_\Grammar^*$ be the transitive closure of $\RewritesTo$.
We say that an expression $\Expr \in \Theory[\FormalParameters]$
is {\em generated} by the grammar $\Grammar$ (written as $\Expr \in
\sem{\Grammar}$) if $\StartSymbol \RewritesTo_\Grammar^* \Expr$.
Note that we implicitly assume that all terms generated by the grammar
have the right type, i.e., are of the type $\range(\SynthFun)$.

\paragraph{The Syntax-Guided Synthesis Problem}
An instance of the SyGuS problem is given by a pair $\tuple { \Spec,
\Grammar }$.
An expression $\Expr$ is a solution to the instance if $\Expr \models
\Spec$ and $\Expr \in \sem{\Grammar}$.

From our definitions, it is clear that we restrict ourselves to a
version of the SyGuS problem where there is exactly one unknown function
to be synthesized, and the grammar does not contain {\tt let} rules.
Further, we assume that our specifications are {\em
  point-wise}\footnote{For brief discussion on different syntactic and
semantic notions of point-wise specifications, see the appendix}.
Intuitively, if a specification is point-wise, if it just relates the
input point to its output, and not the outputs of different inputs.

Here, we use a simple syntactic notion of point-wise specifications for
convenience.
However, our techniques can be generalized to any notion of point-wise
specifications.
\arsays{Write about our definition\dots}

Point-wise specifications allow us to define the notion of an expression
$\Expr$ satisfying a specification $\Spec$ on a point $\Point$.
\arsays{Formally, we say that $\Expr \models \Spec \downharpoonleft
\Point$ if \dots}
We extend this definition to sets of points as follows: $\Expr \models
\Spec \downharpoonleft \Points \Leftrightarrow \bigwedge_{\Point \in
\Points} \Expr \models \Spec\downharpoonleft\Point$.

The above restrictions make the SyGuS problem significantly easier.
However, a large number of problems do fall into this class (\arsays{x
out of y benchmarks in the SyGuS 2015 competition}).
Several previous works address this class of problem (see, for
example, \cite{ACR15,Madhu,xxx}).
The following example shows that a number of commonly occurring
specification mores are point-wise specifications.

\begin{example}
  \label{ex:specifications}
  Show some separable and non-separable specifications

  Write about input-output specifications
\end{example}

\subsection{The Enumerative Solver}
\label{sec:enumeration}

\begin{wrapfigure}{l}{0.5\textwidth}
  \begin{minipage}{0.5\textwidth}
    \begin{algorithm}[H]
      \begin{algorithmic}[1]
        \Require Grammar $\Grammar = \tuple { \NonTerminals, \StartSymbol, \Rules }$
        \Require Specification $\Spec$
        \Ensure $\Expr$ s.t.  $\Expr \in \sem{\Grammar} \wedge \Expr \models \Spec$
        \State $\Point \gets \emptyset$ \label{line:basic:init}
        \While { $\True$ }
        \For {$\Expr \in \Call{Enumerate}{\Grammar,\Points}$ }\label{line:basic:enumerate}
        \If { $\Expr \models \Spec \downharpoonleft  \Points$ } \textbf{continue} \EndIf\label{line:basic:concrete_check}
        \State $\CexInput \gets \Verify(\Expr, \Spec)$ \label{line:basic:verify}
        \If { $\CexInput = \bot$ } \Return $\Expr$ \EndIf \label{line:basic:return}
        \State $\Points \gets \Points \cup \CexInput$ \label{line:basic:continue}
        \EndFor
        \EndWhile
      \end{algorithmic}
      \caption{Enumerative Solver}
      \label{algo:basic}
    \end{algorithm}
  \end{minipage}
\end{wrapfigure}
The principal idea behind the enumerative solver is to enumerate all
expressions from the given syntax with some pruning.
Only expressions that are distinct with respect to a set of concrete
input points are enumerated.

The full pseudo-code is given in Algorithm~\ref{algo:basic}.
Initially, the set of points is empty (line~\ref{line:basic:init}).
In each iteration, the algorithm calls the {\tt Enumerate} procedure
which returns a (possibly infinite) list of expressions that are all
distinct with respect to $\Points$ (line~\ref{line:basic:enumerate}).
This $\Expr$ is then verified, first on the set of points
(line~\ref{line:basic:concrete_check}) and then fully
(line~\ref{line:basic:verify}).
If the expression $\Expr$ is correct, it is returned
(line~\ref{line:basic:return}).
Otherwise, we pick a counter-example input point (i.e., an input on
which $\Expr$ is incorrect) and add it to the set of points and repeat
(line~\ref{line:basic:continue}).

For completeness, we recall the {\tt Enumerate} procedure here
(Algorithm~\ref{algo:enumerate}).
It maintains a map $\Productions : \NonTerminals \to
\Powerset{\Theory[\FormalParameters]}$ from non-terminals to
expressions they can produce.
The invariant maintained by the procedure is that every pair of
expressions in $\Productions[\NonTerminal]$ is distinct on $\Points$.

The algorithm starts by first accumulating into
$\Productions[\NonTerminal]$ the expressions that can be produced from
$\NonTerminal$ in one step
(lines~\ref{line:enumerate:level_one_iter}-\ref{line:enumerate:level_one}).
Then, for each possible expression size $\Size$, it attempts to
instantiate each production rule in the grammar with expressions already
generated and stored in $\Productions$, to generate new expressions of
size at most $\Size$.
These newly generated expression are checked for distinctness from
already generated ones, and if so, added to
$\Productions[\NonTerminal]$.
The algorithm returns all the expressions produced from the starting
non-terminal $\StartSymbol$.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Require Grammar $\Grammar = \tuple { \NonTerminals, \StartSymbol, \Rules }$ and a set of points $\Points$
    \Ensure Expressions $\tuple { \Expr_1, \Expr_2, \ldots }$ s.t. $\forall i < j : \vert \Expr_i \vert \leq \vert \Expr_j
    \vert \wedge \exists \Point \in \Points : \Expr_i[\Point] \neq \Expr_j[\Point]$
    \ForAll {$\NonTerminal \in \NonTerminals$} $\Productions[\NonTerminals] \gets \emptyset$ \EndFor
    \ForAll {$(\NonTerminal, \Expr) \in \Rules$}\label{line:enumerate:level_one_iter}
    \If { $\Expr \in \Theory[\FormalParameters]$ }
    $\Productions[\NonTerminal] \gets \Productions[\NonTerminal] \cup \{ \Expr \}$  \label{line:enumerate:level_one}
    \EndIf
    \EndFor
    \State $ \Size \gets 1 $
    \While { $\True$ }
    \ForAll {$(\NonTerminal, \Expr) \in \Rules$}
    \State $(\NonTerminal_1, \ldots, \NonTerminal_n) \gets \mbox{List of non-terminals occurring in $\Expr$ }$
    \ForAll { $(\Expr_1, \ldots, \Expr_n) \in \Productions[\NonTerminal_1] \times \cdots \times \Productions[\NonTerminal_n]$ }
    \State $\Expr^* \gets \Expr[\Expr_1/\NonTerminal_1,\ldots,\NonTerminal_n/\Expr_n]$
    \If { $\vert \Expr^* \vert \leq \Size \wedge \forall \Expr' \in
      \Productions[\NonTerminal] . \exists \Point \in \Points :
    \Expr'[\Point] \neq \Expr^*[\Point] $ }
    \State $\Productions[\NonTerminal] \gets \Productions[\NonTerminal] \cup \Expr^*$
    \If { $\NonTerminal = \StartSymbol$ } \textbf{yield} $\Expr^*$ \EndIf
    \EndIf
    \EndFor
    \EndFor
    \State $\Size \gets \Size + 1$
    \EndWhile
  \end{algorithmic}
  \label{algo:enumerate}
  \caption{Enumerating distinct expressions from a grammar}
\end{algorithm}

\begin{theorem}[\cite{Transit}]
  \label{thm:basic_enumeration}
  Given a SyGuS instance $(\Spec, \Grammar)$ with at least one solution
  expression, Algorithm~\ref{algo:basic} terminates and returns the
  smallest solution expression.
\end{theorem}

\paragraph{Features and Limitations}
The enumerative algorithm performs surprisingly well considering its
simplicity on small to medium sized benchmarks
(see~\cite{Transit,sygus_reports,etc}).
Further, due to the guarantee of Theorem~\ref{thm:basic_enumeration}
that the enumerative approach produces small solutions, the algorithm is
capable of generalizing from specifications that are input-output
examples.

However, enumeration quickly fails to scale with growing size of
solutions.
Figure~\ref{fig:scalability_graph} shows the time taken (in seconds) to
generate all distinct expressions (for $4$ points) up to a given size
for the grammar shown in Figure~\ref{fig:bitvec_grammar}.
As can be seen from the graph, the time taken grows exponentially with
the size.
\arsays{Need a exponential scalability graph. Pick one from the
experiments}



\section{The Divide-and-Conquer Enumeration Algorithm}
\label{sec:algo}

\subsection{Conditional Expressions}

\paragraph{Conditional Expression Grammars}
We introduce conditional expressions grammars separate the grammar that
generate the return value of the function to be synthesized $\SynthFun$,
and the grammar the generates conditionals to decide which return value
will be returned by $\SynthFun$.
The predicates and terms generated are combined using if-then-else
conditional operator.
Note that our techniques can be easily extended to use other conditional
operators such as switch statements.

A {\em conditional expression grammar} is a pair of grammars $\tuple{
\Grammar_T, \Grammar_P }$ where:
\begin{inparaenum}[(a)]
\item the {\em term grammar} $\Grammar_T$ is an expression grammar
  generating terms of type $\range(\SynthFun)$; and
\item the {\em predicate grammar} $\Grammar_P$ is an expression
  grammar generating boolean terms.
\end{inparaenum}
The set of expressions $\sem{\tuple{ \Grammar_T, \Grammar_P }}$
generated by the conditional expression grammar $\tuple{ G_T, G_P }$ is
the smallest set of expressions $\Theory[\FormalParameters]$ that
satisfy the following:
\begin{inparaenum}[(a)]
\item $\sem{\Grammar_T} \subseteq \sem{\tuple{ \Grammar_T, \Grammar_P
  }}$, and
\item $\Expr_1, \Expr_2 \in \sem{\tuple{ \Grammar_T, \Grammar_P }}
  \wedge \Pred \in \sem{\Grammar_P} \implies
  \ITE{\Pred}{\Expr_1}{\Expr_2} \in \sem{\tuple{ \Grammar_T, \Grammar_P }}$
\end{inparaenum}

\arsays{Note on going from standard SyGuS grammars to conditional
expression grammars}

\begin{example}
  \arsays{Give the ICFP and LIA benchmark grammars}
\end{example}

\paragraph{Decision Trees}
We use the concept of decision trees from machine learning literature to
model conditional expressions.
Given a conditional expressions, the predicates and terms act as
attributes and labels in the corresponding decision tree.
Formally, a {\em decision tree} $\DecisionTree$  is a tuple $\tuple{
\Nodes, (\NodesInternal, \NodesLeaf), \node_0, \Edges, (\EdgesYes,
\EdgesNo), \Attribute, \Label }$ where:
\begin{compactitem}
\item $(\Nodes, \Edges)$ form a rooted binary tree with root node
    $\node_0 \in \Nodes$;
\item The nodes $\Nodes$ are partitioned into a set of internal nodes
    $\NodesInternal$ and leaf nodes $\NodesLeaf$;
\item The attribute function $\Attribute : \NodesInternal \to
    \sem{\Grammar_P}$ maps internal nodes to predicates;
\item The label function $\Label : \NodesLeaf \to \sem{\Grammar_T}$ maps
    leaf nodes to terms;
\item The edges $\Edges$ are partitioned into positive edges $\EdgesYes$
  and negative edges $\EdgesNo$ with each internal node being the source
  of one positive and one negative edge.
\end{compactitem}
Intuitively, each internal node corresponds to an if-then-else
expression with the conditional given by the attribute of the node, and
each leaf node corresponds to a term given by its label.
Formally, the expression $\DTtoExpr{\DecisionTree}$ corresponding to a
decision tree $\DecisionTree$ is defined as follows:
\begin{inparaenum}[(a)]
\item If $\node_0 \in \NodesLeaf$, i.e., the tree is a single node, then
    $\DTtoExpr{\DecisionTree}$ is $\Label(\node_0)$, and
\item If $\node_0 \in \NodesInternal$, then $\DTtoExpr{\DecisionTree} =
  \ITE{\Attribute(\node_0)}{\DTtoExpr{\node_y}}{\DTtoExpr{\node_n}}$
  where $(\node_0, \node_y) \in \EdgesYes$ and $(\node_0, \node_n) \in \EdgesNo$.
\end{inparaenum}

\arsays{Intro about learning decision trees\dots}

\subsection{Algorithm}
\label{sec:algo}

Algorithm~\ref{algo:main} presents the full divide-and-conquer
enumeration algorithm for synthesis.
Like Algorithm~\ref{algo:basic}, the divide-and-conquer algorithm
maintains a set of points $\Points$, and in each iteration:
\begin{inparaenum}[(a)]
\item computes a candidate solution expression $\Expr$
  (lines~\ref{line:main:start_iter}-\ref{line:main:compute_expr});
\item verifies and returns $\Expr$ if it is correct (lines~\ref{line:main:verify}
  and~\ref{line:main:return}); and
\item otherwise, adds the counter-example point into the set $\Points$
  (line~\ref{line:main:add_point}).
\end{inparaenum}

However, the key differences between Algorithm~\ref{algo:main} and
Algorithm~\ref{algo:basic} are in the way the candidate solution
expression $\Expr$ is generated.
The generation of candidate expressions is separated into two separate
steps.

\paragraph{Term solving.}
Instead of searching for a single candidate expression that is correct
on all points in $\Points$, Algorithm~\ref{algo:main} maintains a set of
candidate terms $\Terms$.
We say that a term $\Term$ covers a point $\Point \in \Points$ if $\Term
\models \Spec \downharpoonleft \Point$.
The set of points that a term covers is computed and stored in
$\Cover[\Term]$ (line~\ref{line:main:compute_cover}).
Note that the algorithm does not store terms that cover the same set of
points as already generated terms
(line~\ref{line:main:duplicate_cover}).
When the set of terms $\Terms$ covers all the points in $\Points$, i.e., for
each $\Point \in \Points$, there is at least one term that is correct on
$\Point$, the term enumeration is stopped.

\paragraph{Unification and Decision Tree Learning.}
In the next step (lines~\ref{line:main:xxx}-\ref{line:main:yyy}), we
generate a set of predicates $\Predicates$ that will be used as
conditionals to combine the terms from $\Terms$ into if-then-else
expressions.
In each iteration, we attempt to learn a decision tree \dots
\arsays{Incomplete without adding terms}

\begin{algorithm}
  \begin{algorithmic}[1]
    \Require Conditional expression grammar $\Grammar = \tuple{ \Grammar_T, \Grammar_P }$
    \Require Specification $\Spec$
    \Ensure $\Expr$ s.t.  $\Expr \in \sem{\Grammar} \wedge \Expr \models \Spec$
    \State $\Point \gets \emptyset$ \label{line:basic:init}

    \While { $\True$ }
    \State $\Terms \gets \emptyset; \Predicates \gets \emptyset; \Cover \gets \emptyset$ \label{line:main:start_iter}

    \While { $\bigcup_{\Term \in \Terms} \Cover[\Term] \neq \Points$ } \Comment { Term solver }
    \State $\Term \gets \Call{Enumerate}{\Grammar_T,\Points};
    \Cover[\Term] \gets \{ \Point \mid \Point \in \Points \wedge \Term \models \Spec \downharpoonleft \Point \} $\label{line:main:compute_cover}
    \If { $\forall \Term' \in \Terms : \Cover[\Term] \neq \Cover[\Term']$ }
    $\Terms \gets \Terms \cup \{ \Term \}$ \label{line:main:duplicate_cover}
    \EndIf
    \EndWhile

    \While { $\True$ } \Comment { Unifier }
    \State $\DecisionTree \gets \Call{LearnDT}{\Terms, \Predicates}$
    \If  { $\DecisionTree = \bot$ }
    \State $\Terms \gets \Terms \cup \Call{Enumerate}{\Grammar_T,\Points}$ \label{line:main:more_terms}
    \State $\Predicates \gets \Predicates \cup \Call{Enumerate}{\Grammar_P,\Points}$\label{line:main:more_preds}
    \Else
    \State $\Expr \gets \DTtoExpr{\DecisionTree}$ \label{line:main:compute_expr}
    \State \textbf{break}
    \EndIf
    \EndWhile

    \State $\CexInput \gets \Verify(\Expr, \Spec)$ \Comment { Verifier } \label{line:main:verify}
    \If { $\CexInput = \bot$ } \Return $\Expr$ \EndIf \label{line:main:return}
    \State $\Points \gets \Points \cup \CexInput$ \label{line:main:add_point}
    \EndWhile
  \end{algorithmic}
  \caption{The divide-and-conquer enumeration algorithm}
  \label{algo:main}
\end{algorithm}

\begin{theorem}
  Algorithm~\ref{algo:main} is a sound and semi-complete algorithm for the
  Syntax Guided Synthesis problem.
\end{theorem}
\arsays{Proof idea. Also, what do we need from the decision tree
algorithm for completeness?}

\begin{example}
  Full running of the algorithm
\end{example}

\begin{example}
  Incompleteness without line~\label{line:main:more_preds}
\end{example}

\subsection{Decision Tree Learning}
\label{sec:decision_trees}

The standard multi-label decision tree learning algorithm is presented
in Algorithm~\ref{algo:dt_learning}.
This algorithm is based on ID3~\cite{ID3}.
The algorithm first checks if there exists a single label (i.e., term)
$\Term$ that applies to all the points (line~\ref{line:dt:single}).
If so, it returns a decision tree with only one leaf whose label is
$\Term$ (line~\ref{line:dt:leaf}).
Otherwise, it picks the best predicate $\Pred$ to split on based on some
heuristic (line~\ref{line:dt:pick_pred}).
It the recursively computes the left and right sub-trees for the set of
points on which $\Pred$ holds and does not hold, respectively
(lines~\ref{line:dt:left} and~\ref{line:dt:right}).
The final decision tree is returned as a tree with a root (with
attribute $\Pred$), and positive and negative edges to the roots of the
left and right sub-trees, respectively.

\begin{algorithm}
  \begin{algorithmic}
    \Require $\Points$, $\Cover$, $\Predicates$
    \Ensure Decision tree $\DecisionTree$
    \If { $\exists \Term : \Cover[\Terms] \supseteq \Points$ }\label{line:dt:single}
    \Return $\mathit{LeafNode}[\Label \gets \Term]$ \label{line:dt:leaf}
    \EndIf
    \State $\Pred \gets \mbox{Pick best predicate from $\Predicates$}$\label{line:dt:pick_pred}
    \State $L \gets \Call{LearnDT}{\{ \Point \mid \Predicates[\Point], \Cover, \Predicates \}}$\label{line:dt:left})
    \State $R \gets \Call{LearnDT}{\{ \Point \mid \neg \Predicates[\Point], \Cover, \Predicates \}}$\label{line:dt:right})
    \State \Return $\mathit{InternalNode}[\Attribute \gets \Pred,  left \gets L , right \gets R ]$
  \end{algorithmic}
\end{algorithm}

\begin{example}
  Simple algorithm
\end{example}

\paragraph{Frequency based priors}
We now present the information gain heuristic we use for picking the
best predicate to split on in the decision tree learning algorithm

\subsection{Features and Optimizations}
\label{sec:optimizations}

\paragraph{Anytime Synthesis}

\paragraph{Branch-level verification}

\paragraph{Label and attribute substitution}

\paragraph{Bunching enumerations}

\section{Evaluation}
\label{sec:evaluation}
We built a prototype \sygus solver based on the ideas presented in
this paper. The algorithms for enumeration were mainly implemented in
about 3000 lines of Python code, while the algorithms for decision
tree learning were implemented in about 3000 lines of
C$\raisebox{1pt}{+\!+}$ code.

Our experimental
evaluation focuses on the following aspects:
\begin{inparaenum}[(a)]
\item
time to find a solution,
\item
compactness of the discovered solutions,
\item
The most compact solution that the algorithm can discover within a
fixed time budget.
\end{inparaenum}
All experiments were executed on a 32-core machine with four Intel
Xeon E7-4820 processors running at 2.0 GHz, with a one hour time
limit.

\input{results_one_shot_table}

\section{Related Work}
\label{sec:related_work}



\bibliographystyle{plainnat}
\setlength{\bibsep}{1pt}
\begin{small}
\bibliography{main}
\end{small}
\end{document}
