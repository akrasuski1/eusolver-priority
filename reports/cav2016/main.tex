\documentclass{llncs}

\sloppy

\usepackage{marginnote}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{alltt}
\usepackage{paralist}
\usepackage{soul}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{mdwtab}
\usepackage{multirow}
\usepackage[square,numbers,sort&compress,comma,sectionbib]{natbib}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.12}
\usepackage{hyperref}

\newcommand\comment[1]{}
\newcommand\arsays[1]{{\bf AR: #1}}
\newcommand\ausays[1]{{\bf AU: #1}}

\renewcommand\subparagraph[1]{\smallskip\noindent{\em #1.}}

\newcommand{\ie}{\emph{i.e.}}
\renewcommand{\bibname}{References}
\newcommand\Integers{\mathbb{Z}}
\newcommand\tuple[1]{\langle #1 \rangle}
\newcommand\True{\mathit{True}}
\newcommand\None{\mathit{None}}
\newcommand\Equiv{\mathit{Eq}}
\newcommand\Points{\mathit{Pts}}
\newcommand\Point{\mathit{Pt}}
\newcommand\Verify{\mathit{Verify}}
\newcommand\CexInput{\mathit{CexPt}}
\newcommand\Predicates{\mathit{Preds}}
\newcommand\Expr{e}
\newcommand\Pred{p}
\newcommand\Terms{\mathit{Terms}}
\newcommand\Term{t}
\newcommand\Cover{\mathit{Cover}}
\newcommand\Powerset[1]{\mathbf{2}^{#1}}
\newcommand\Spec{\Phi}
\newcommand\Size{K}
\newcommand\Grammar{G}
\newcommand\sem[1]{[\![ #1 ]\!]}
\newcommand\SynthFun{f}
\newcommand\range{\mathit{range}}
\newcommand\FormalParameters{\mathit{Params}}
\newcommand\Productions{\mathit{Prodn}}
\newcommand\Prob[2]{\mathbb{P}_{#1}(#2)}
\newcommand\NonTerminals{\mathcal{N}}
\newcommand\NonTerminal{N}
\newcommand\StartSymbol{S}
\newcommand\Symbols{\mathit{Symbols}}
\newcommand\Rules{\mathcal{R}}
\newcommand\Rule{R}
\newcommand\Theory{\mathcal{T}}
\newcommand\RewritesTo{\rightarrow}
\newcommand\ITE[3]{\mathtt{if}~#1~\mathtt{then}~#2~\mathtt{else}~#3}

\newcommand\DecisionTree{\mathit{DT}}
\newcommand\DTtoExpr[1]{\mathit{Expr}(#1)}
\newcommand\NodesInternal{V_I}
\newcommand\Nodes{V}
\newcommand\node{v}
\newcommand\NodesLeaf{V_L}
\newcommand\EdgesYes{E_Y}
\newcommand\EdgesNo{E_N}
\newcommand\Edges{E}
\newcommand\Attribute{\mathcal{A}}
\newcommand\Label{\mathcal{L}}
\newcommand{\sygus}{{\sffamily\fontsize{8.5}{10}\selectfont
    SyGuS}\xspace}
\newcommand{\dcsolve}{{\sffamily\fontsize{8.5}{10}\selectfont
    DCSolve}\xspace}
\renewcommand{\paragraph}[1]{\par\noindent\textbf{#1}.}
\newcommand{\itparagraph}[1]{\par\noindent\textit{#1}.}
\newcommand{\esolver}{\textsc{esolver}\xspace}
\newcommand{\sketch}{\textsc{sketch}\xspace}
\newcommand{\pwsolve}{{\sffamily\fontsize{8.5}{10}\selectfont
    PWSolve}\xspace}

% Make floats and equations behave sensibly
\setlength{\intextsep}{1pt}
\setlength{\textfloatsep}{1pt}
\setlength{\floatsep}{1pt}
\abovedisplayskip=6pt plus 3pt minus 9pt
\abovedisplayshortskip=0pt plus 3pt
\belowdisplayskip=6pt plus 3pt minus 9pt
\belowdisplayshortskip=7pt plus 3pt minus 4pt
\setlength\abovecaptionskip{1pt}
\setlength\belowcaptionskip{1pt}

\begin{document}
% \arsays{TODO:}
% \begin{compactitem}
% \item Change main algo to check for distinctness on points in a
%   straightforward manner
% \item
% \end{compactitem}

% \newpage

\pagestyle{plain}
\title{Scaling Enumerative Program Synthesis via Divide and Conquer}
% \author{Rajeev Alur \and Arjun Radhakrishna \and Abhishek Udupa}
\author{}
% \institute{University of Pennsylvania}
\institute{}
\maketitle
\vspace*{-6ex}

\begin{abstract}
  Given a semantic constraint specified by a logical formula and
  syntactic constraint specified by a context-free grammar, the
  Syntax-Guided Synthesis (SyGuS) problem is to find an expression
  that satisfies both the syntactic and semantic constraints.
  An enumerative approach to solve this problem is to systematically
  generates all expressions from the syntactic space with some pruning,
  and has proved to be surprisingly competitive in the newly started
  competition of SyGuS solvers.  It performs well on small to medium sized
  benchmarks, produces succinct expressions, and has the ability to
  generalize from input-output examples.  However, its performance
  degrades drastically with the size of the smallest solution. To overcome
  this limitation, in this paper we propose an alternative approach to
  solve SyGuS instances.

  The key idea  is to employ a divide-and-conquer approach by
  separately enumerating (a) smaller expressions that are correct on
  subsets of inputs, and (b) predicates on inputs that distinguish these
  subsets.  These smaller expressions and predicates are then combined
  using decision trees to obtain an expression that is correct on all
  inputs.  We view the problem of combining expressions and predicates as
  a multi-label decision tree learning problem. We propose a novel
  technique of associating a probability distribution over the set of
  labels that a sample can be labeled with. This enables us to use
  standard information-gain based heuristics to learn a compact decision
  tree.

  We report a prototype implementation and evaluate it on the benchmarks
  from the SyGuS 2015 competition. Our tool is able to match the running
  times and the succinctness of solutions of both standard enumerative
  solver as well as the latest white-box solvers in most cases.  Further,
  our solver is able to solve a large number of instances from the ICFP
  class of benchmarks, which were previously unsolved by all existing
  solvers.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\marginnote{Para 1 {Synthesis background and SyGuS as an enabling
technology}}
The field of program synthesis relates to automated techniques that
attempt to automatically generate programs from requirements that a
programmer writes.
Program synthesis is an increasingly popular area of research with the
introduction of counter-example guided inductive synthesis (CEGIS)
approach by Solar-Lezama et al, and has been applied to a wide array of
domains including invariant generation, automated repair of
programs, binary rewriting, etc~\cite{xxx,yyy,zzz}.
Recently, Syntax-Guided Synthesis (\sygus) has been proposed as a
back-end exchange format and enabling technology for program synthesis.
The aim is to allow experts from different domains to model their
synthesis problems as \sygus instances, and leverage general purpose
\sygus solvers.

\marginnote{Para 2 {\sygus and \sygus approaches; enumerative solver +
limitations}}
In the \sygus approach, a synthesis task is specified using
restrictions on both the form (syntax) and function (semantics) of the
program to be synthesized:
\begin{inparaenum}[(a)]
\item The syntactic restrictions are given in terms of a context-free
  grammar from which a solution program may be drawn.
\item The semantic restrictions are encoded into a specification as an
  SMT formula.
\end{inparaenum}
\sygus solvers can be broadly categorized into two kinds:
\begin{inparaenum}[(a)]
\item black-box solvers, which do not deal with the specification
  directly, but learn constraints on how a potential solution should
  behave on sample input points~\cite{enumerative,stochastic,alchemist,etc};
  and
\item white-box solvers, which attempt to either rewrite or solve the
  specification directly using constraint solving
  techniques~\cite{cvc4,alur-2015}.
\end{inparaenum}
Of the black-box solvers, the most successful, despite its simplicity,
has been the enumerative solver which won the \sygus competition 2014
and placed second in \sygus competition 2014~\cite{xxx}.
The enumerative solver maintains a set of concrete input points, and
in each iteration attempts to produce an expression that is correct on
these concrete inputs.
It does so by enumerating expressions from the grammar and checking if
they are correct on the input points, while pruning away expressions
that behave equivalently to already generated expressions.
If an expression that is correct on the input points is found, it is
verified against the full specification.
If it is incorrect, a counter-example point is found and added to the
set of input points.

\marginnote{Para 3 {Problem and our solution to this}}
While the enumerative approach is very successful, it does not scale
well with the solution size.
In Figure~\ref{figure:random_exponential_graph}, the x-axis represents
expression size and the y-axis represents the time taken to enumerate
all expressions of size up to the expression size.
As can be seen from the graph, the time taken to explore potential
solutions grows exponentially with size.
To overcome this scalability issue, we introduce a divide-and-conquer
enumerative algorithm.

The divide-and-conquer enumerative approach is based on this insight:
while the full solution expression to the synthesis problem may be
large, the important individual parts are small.
The individual parts we refer to here are:
\begin{inparaenum}[(a)]
\item {\em terms} which serve as the return value for the solution,
  and
\item {\em predicates} which serve as the conditionals that choose which
  term is the actual return value for a given input.
\end{inparaenum}
For example, in the expression $\ITE{x \leq y}{y}{x}$, the terms are $x$
and $y$, and the predicate is $x \leq y$.
As can be seen in this example, though the full expression has size $6$,
the individual terms have size $1$ each, and the predicate size $3$.
Hence, the divide-and-conquer enumerative approach only enumerates terms
and predicates separately and attempts to combine them into a
conditional expression.
\arsays{Do we need to mention the point-wise/separable restriction here,
and also the restriction on grammars?}

For combining the different part of the solution in a full conditional
expression, we use the technique of learning decision
trees~\cite{ID3,probably_mitchell_book}.
The input points maintained by the enumerative algorithm serve as the
samples, the predicates enumerated serve as the attributes, and the
terms serve as the labels.
A term $\Term$ is a valid label for a point $\Point$ if $\Term$ is
correct for $\Point$.
We use a simple multi-label decision tree learning algorithm returns a
sound decision tree that classifies the samples soundly, i.e., for each
point, following the edges corresponding to the attribute values (i.e.,
predicates) lead to a branch.

\marginnote{Para 5 {Other extensions. \arsays{I don't like this
paragraph. It sounds like a ``this too is in the paper'' paragraph}}}
\paragraph{Extensions and Optimizations}
We present three separate extensions and optimizations to resolve
various issues that arise in a practical implementation of the
divide-and-conquer implementation.

\subparagraph{Anytime extension}
The anytime extension to the algorithm does not stop when the first
solution is found, and instead continues enumerating terms and
predicates in an attempt to produce more compact solutions.
For example, suppose the synthesizer is attempting to produce a
conditional equivalent to $x \geq 0 \wedge y \geq 0 \wedge x + y \leq
2$.
Using predicates of size $3$, the most compact solution the synthesizer
can produce is $y \geq 0 \wedge ((x = 0 \wedge y \leq 2) \vee (x = 1
\wedge y \leq 1) \vee (x = 2 \wedge y \leq 0))$.
However, allowing the synthesizer to continue and further enumerate
predicates of size $4$ lets it to produce the more compact original
conditional.

\subparagraph{Decision tree repair}
During the enumeration process, a term (say $\Term$) which is correct on
the same set of points as an already generated term (say $\Term'$) is
discarded.
However, it may be the case that $\Term$ is correct on a larger set of
inputs than $\Term'$.
Hence, if a counter-example is generated when verifying an expression
$\Expr$ that uses $\Term'$, it might be possible to repair $\Expr$ to
make it behave correctly on the counter-example by replacing $\Term'$
with $\Term$.

\subparagraph{Branch level verification}
Another more technique we use to generate additional counter-examples
from a given decision tree is to verify each branch (path from root to
leaf) separately.
Each such verification call may return additional counter-examples,
allowing the procedure to gain more information from a single incorrect
solution.
For example, to verify the proposed solution expression $\SynthFun(x, y)
\equiv \ITE{x \leq y}{1}{0}$: instead of checking if there exists an
input $(x_0, y_0)$ on which the expression behaves incorrectly, branch
level verification checks separately if
\begin{inparaenum}[(a)]
\item there exists an input satisfying $x \leq y$ on which the
  expression $1$ behaves incorrectly, and
\item the exists an input satisfying $\neg x \leq y$ on which the
  expression $0$ behaves incorrectly.
\end{inparaenum}

\marginnote{Para 7 {Evaluation}}
\paragraph{Evaluation}
We implemented the above mention techniques in a prototype tool and
evaluated it on benchmarks from the \sygus competition 2015, both in the
linear integer arithmetic and the bit-vector domain.
The tool was able to perform on par or better than existing solvers in
both domains, and produced compact solutions.
In the bit-vector domain, the tool was able to solve $47$ out of the
$50$ problems in the ICFP suite of benchmarks in less than $1$ hour.
To the best of our knowledge, no other solver has been able to solve the
ICFP benchmarks given the syntactic restrictions.
Further, we also evaluated if the anytime extension of the algorithm was
able to produce more compact solutions given more time.
In $22$ of the $47$ solved ICFP benchmarks, the anytime extension
produced smaller solutions than the earliest generated one, with the
smallest solution being $11\%$ of the earliest generated in the best
case.
In the linear integer arithmetic domain, the tool performed much better
than existing black-box solvers scaling to larger instances.
However, in the larger benchmarks, the size of the solution produced was
larger than those produced by white-box solvers due to the decision tree
learning algorithm producing larger than necessary decision trees.

\marginnote{Para 8 {Summary}}
\paragraph{Summary}
Overall, this paper presents a divide-and-conquer approach to
enumerative synthesis.
The main contributions are as follows:
\begin{inparaenum}[(a)]
\item The main divide-and-conquer algorithm that enumerates only the
  terms and predicates involved in the final solution and combines them
  using a decision tree learning technique;
\item Several novel extensions and optimizations leading to improved
  quality of solutions and performance; and
\item A prototype tool and experimental evaluation demonstrating the
  scalability of the approach and quality of solutions produced.
\end{inparaenum}

\section{Illustrative Examples}
\label{sec:example}

\begin{figure}[t]
  \vspace{-1ex}
  \begin{alltt}
    S ::= T | if (C) then T else T
    T ::= 0 | 1 | x | y | -T | T + T
    C ::= T \(\leq\) T | T < T | T = T | C \(\wedge\) C | \(\neg\) C
  \end{alltt}
  \vspace{-4ex}
  \caption{Grammar for linear integer expressions}
  \label{fig:grammar}
\end{figure}
We explain the salient points of our approach on an example.
Consider an instance of the \sygus problem where the aim is to generate
an expression for the function to be synthesized $\SynthFun$ which:
\begin{compactitem}
\item Is generated by the grammar from Figure~\ref{fig:lia_grammar}.
\item Satisfies the specification $\Spec \equiv \forall x, y :
  \SynthFun(x, y) \geq x \wedge \SynthFun(x, y) \geq y \wedge
  (\SynthFun(x,y) = x \vee \SynthFun(x,y) = y)$.
\end{compactitem}
Note that the specification constrains the return value of $\SynthFun$
to be the maximum of the two inputs $x$ and $y$.
The smallest expression generated by the grammar that satisfies this
specification is $\ITE{x \leq y}{y}{x}$.

\paragraph{Na\"ive Enumeration}
We explain the running of the na\"ive enumerative
algorithm~\cite{udupa-transit} using
Table~\ref{table:enumerative_example}.
The enumerative algorithm maintains a set of concrete input points
$\Points$, and proceeds in rounds.
In the first round, $\Points$ is empty. 
In each round the algorithm proposes a candidate solution expression
that is correct on all of $\Points$.
If this expression is correct on all inputs --- verified by an SMT
solver --- the algorithm returns it.
Otherwise, a counter-example input point, on which the expression is
incorrect, is added to $\Points$.

The algorithm generates the candidate solution expression by enumerating
expressions generated by the grammar in order of size.
In the first round, the candidate expression proposed is the first
expression generated ($0$) from the grammar as $\Points$ is empty.
Attempting to verify the correctness of this expression, yields a
counter-example point $\{ x \mapsto 1, y \mapsto 0 \}$.
In the second round, the expression $0$ is incorrect on the point, and
the next expression to be correct on all of $\Points$ ($1$) is proposed.
This fails to verify as well, and yields the counter-example point $\{ x
\mapsto 0, y \mapsto 2 \}$.
In the third round, all expressions of size $1$ are incorrect on at
least one point in $\Points$, and the algorithm moves to expressions of
size $3$ (there are no expressions of size $2$).
Here, the algorithm might generate the expression $0 + x$. 
However, this expression is immediately discarded as it is equivalent to
the expression $x$ (of size $1$) on all $\Points$.
Proceeding further, over several rounds, the algorithm eventually
generates the expression $\ITE{x \leq y}{y}{x}$ which the SMT solver
verifies to be correct.
Note that to get the correct expression, the algorithm had to generate a
large number (in this case, hundreds of expressions).
In general, the number of expressions grows exponentially with size.
Thus, the enumerative solver fails to scale to problems which only admit
large solution expressions.


\begin{table}[!t]
  \centering
  \fontsize{8}{10}\selectfont
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lllllllc}\\\hlx{hv}

      Round no. & Enumerated expressions & Candidate Expression & Point added \\
      1 & 0 & 0 & $\{ x \mapsto 1, y \mapsto 0 \}$ \\
      2 & 0, 1 & 1 & $\{ x \mapsto 0, y \mapsto 2 \}$ \\
      3 & 0, 1, x, y, \st{0 + x}, \ldots , x + y,  & 1 & $\{ x \mapsto 1, y \mapsto 2 \}$ \\
      \vdots \\
      n & 0, 1, \ldots, $\ITE{x \leq y}{y}{x}$  & $\ITE{x \leq y}{y}{x}$ &  \\

      \hlx{hv}
  \end{tabular*}
  \caption{Example run of the na\"ive enumerative algorithm}
  \label{table:enumerative_example}
\end{table}

\paragraph{Divide and Conquer Enumeration}
In the above run, though the solution expression is large, the
individual components (the terms $x$ and $y$, and the predicate $x \leq
y$) are rather small and can be quickly enumerated.
This difference between the sizes of the constituent terms and
predicates, and the complete solution only increases for larger
solutions. 

The divide-and-conquer approach enumerates terms and predicates
separately, and attempts to combine them into a conditional expression.
Again, we explain this idea using an example run of the algorithm we
propose (see Table~\ref{table:dq_example}).

Similar to the na\"ive algorithm, the divide-and-conquer algorithm
maintains a set of points $\Points$, and works in rounds.
As before, in the first round, the term $0$ is generated and found to be
correct on $\Points$ (which is empty).
The second round is similar to the second round of the na\"ive
enumerative algorithm.
In the third round, the terms $0$, $1$, $x$ and $y$ are enumerated.
In contrast to the na\"ive algorithm, the enumeration stops here because
terms $1$ and $y$ are correct on the points $\{ x \mapsto 1, y \mapsto 0
\}$ and $\{ x \mapsto 0, y \mapsto 2 \}$, respectively, and thus
together ``cover'' all of $\Points$.
Now, to propose an expression, the algorithm starts enumerating
predicates.
The enumeration of predicates proceeds until the algorithm can find
a sufficient number of predicates to generate a conditional expression
using the previously enumerated terms.
The terms and predicates are combined into conditional expression by
learning decision trees (see Section~\ref{sec:decision_trees}).
The candidate expression proposed in the third round is 
$\ITE{1 \leq y}{y}{x}$ and the counter-example generated is 
$\{ x \mapsto 2, y \mapsto 0 \}$ (see table).
Proceeding further, in the fourth round, the correct expression is
generated.

Note that this approach only generates $4$ terms and $11$ predicates in
contrast to the na\"ive approach which generates hundreds of
expressions.

\begin{table}[!t]
  \centering
  \fontsize{8}{10}\selectfont
  \begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lllllllc}\\\hlx{hv}

      Round no. & Enumerated Terms & Enumerated Predicates & Candidate Expression & Point added \\
      1 & 0 & 0 & $\emptyset$ & $\{ x \mapsto 1, y \mapsto 0 \}$ \\
      2 & 0, 1 & 1 & $\emptyset$ &  $\{ x \mapsto 0, y \mapsto 2 \}$ \\
      3 & 0, 1, x, y & $0 \leq 0$, \ldots $0 \leq y$, $1 \leq 0$, \ldots $1 \leq y$ & $\ITE{1 \leq y}{y}{1}$ & $\{ x \mapsto 2, y \mapsto 0 \}$ \\
      4 & 0, 1, x, y & $0 \leq 0$, \ldots $x \leq y$ & $\ITE{x \leq y}{y}{x}$ & \\
      \hlx{hv}
  \end{tabular*}
  \caption{Example run of the divide-and-conquer enumerative algorithm}
  \label{table:dq_example}
\end{table}


\section{Problem Statement and Background}
\label{sec:problem}

Let us fix the function to be synthesized $\SynthFun$ and its formal
parameters $\FormalParameters$.
We write $\range(\SynthFun)$ to denote the range of $\SynthFun$.
The term {\em point} denotes a valuation of $\FormalParameters$, i.e., a
point is an input to $\SynthFun$.

\begin{example}
  For the running example in this section, we consider a function to be
  synthesized $\SynthFun$ of type $\Integers \times \Integers \to
  \Integers$ with the formal parameters $\FormalParameters = \{ x, y \}$.
  Points are valuations of $x$ and $y$.
  For example, $\{ x \mapsto 1, y \mapsto 2 \}$ is a point.
\end{example}

\paragraph{Specifications}
Satisfiability modulo theory formulae have become the standard formalism
for specifying semantic constraints for synthesis.
In the rest of this paper, we fix an arbitrary theory $\Theory$ and
denote by $\Theory[\Symbols]$, the set of $\Theory$ terms over the
symbols $\Symbols$.
A {\em specification} $\Spec$ is a logical formula in a theory $\Theory$
over standard theory symbols and the function to be synthesized
$\SynthFun$.
We say that an expression $\Expr$ satisfies a specification $\Spec$ if
instantiating the function to be synthesized $\SynthFun$ by $\Expr$
makes the formula $\Spec$ valid.

\begin{example}
  \label{ex:running:spec}
  Continuing the running example, we define a specification $\Spec
  \equiv \forall x, y : \SynthFun(x, y) \geq x \wedge \SynthFun(x, y)
  \geq y \wedge f(x, y) = x \vee f(x, y) = y$.
  The specification states that the function to be synthesized
  $\SynthFun$ maps each pair $x$ and $y$ to a value that is at least as
  great as each of them and equal to one of them.
  The function that returns the maximum of the two input values is the
  only possible function that satisfies this specification.
\end{example}

\paragraph{Grammars}
An {\em expression grammar} $\Grammar$ is a tuple $\tuple {
\NonTerminals, \StartSymbol, \Rules }$ where:
\begin{inparaenum}[(a)]
\item the set $\NonTerminals$ is a set of non-terminal symbols,
\item the non-terminal $\StartSymbol \in \NonTerminals$ is the initial non-terminal,
\item $\Rules \subseteq \NonTerminals \times
  \Theory[\NonTerminals \cup \FormalParameters]$ is a finite set
  of rewrite rules that map $\NonTerminals$ to $\Theory$-expressions
  over non-terminals and formal parameters.
\end{inparaenum}
We say that an expression $\Expr$ {\em rewrites to} an incomplete
expression $\Expr'$ (written as $\Expr \RewritesTo_\Grammar \Expr'$) if
there exists a rule $\Rule = (\NonTerminal, \Expr'') \in \Rules$ and
$\Expr'$ is obtained by replacing one occurrence of $\NonTerminal$ in
$\Expr$ by $\Expr''$.
Let $\RewritesTo_\Grammar^*$ be the transitive closure of $\RewritesTo$.
We say that an expression $\Expr \in \Theory[\FormalParameters]$
is {\em generated} by the grammar $\Grammar$ (written as $\Expr \in
\sem{\Grammar}$) if $\StartSymbol \RewritesTo_\Grammar^* \Expr$.
Note that we implicitly assume that all terms generated by the grammar
have the right type, i.e., are of the type $\range(\SynthFun)$.

\begin{example}
  \label{ex:running:grammar}
  For the running example, we choose the following grammar.
  The non-terminals are given by $\NonTerminals = \{ \StartSymbol, T, C \}$ with
  the initial non-terminal bein $\StartSymbol$.
  The rules are $\{ (\StartSymbol, T), (\StartSymbol,
      \ITE{C}{\StartSymbol}{\StartSymbol})
    \} \cup \{ (T, x), (T, y), (T, 1), (T, 0), (T, - T), (T, T + T) \} \cup
        \{ (C, T \leq T), (C, C \wedge C), (C, \neg C) \}$.
  This is the standard linear integer arithmetic grammar used for many
  SyGuS problems.
  This grammar is equivalent to the one from
  Figure~\ref{fig:lia_grammar}.
\end{example}

\paragraph{The Syntax-Guided Synthesis Problem}
An instance of the SyGuS problem is given by a pair $\tuple { \Spec,
\Grammar }$ of specification and grammar.
An expression $\Expr$ is a solution to the instance if $\Expr \models
\Spec$ and $\Expr \in \sem{\Grammar}$.

\begin{example}
  Continuing the running example, for the specification $\Spec$ from
  Example~\ref{ex:running:spec} and the grammar from
  Example~\ref{ex:running:grammar}, one of the solution expressions is
  given by $\SynthFun(x, y) \equiv \ITE{x \leq y}{y}{x}$.
\end{example}

From our definitions, it is clear that we restrict ourselves to a
version of the SyGuS problem where there is exactly one unknown function
to be synthesized, and the grammar does not contain {\tt let} rules.
Further, we assume that our specifications are {\em
  point-wise}\footnote{For brief discussion on different syntactic and
semantic notions of point-wise specifications, see the appendix}.
Intuitively, if a specification is point-wise, if it just relates the
input point to its output, and not the outputs of different inputs.

Here, we use a simple syntactic notion of point-wise specifications for
convenience.
However, our techniques can be generalized to any notion of point-wise
specifications.
Formally, we say that a specification is {\em plainly separable} if can
be rewritten into a conjunctive normal form where each clause is either
\begin{inparaenum}[(a)]
\item a tautology, or
\item every appearing application of the function to be synthesized
  $\SynthFun$ have the same arguments.
\end{inparaenum}

\begin{example}
  The specification for our running example $\Spec \equiv \SynthFun(x,
  y) \geq x \wedge \SynthFun(x, y) \geq y \wedge \SynthFun(x, y) = x
  \vee \SynthFun(x, y) = y$ is plainly separable.
  For example, this implies that the value of $\SynthFun(1, 2)$ can be
  chosen irrespective of the value of $\SynthFun$ on any other point.
  On the other hand, a specification such as $\SynthFun(x, y) = 1
  \implies \SynthFun(x + 1, y) = 1$ is neither plainly separable nor
  point-wise.
  The value of $\SynthFun(1, 2)$ cannot be chosen independently of the
  value of $\SynthFun(0, 2)$.
\end{example}

Point-wise (and in turn, simply separable) specifications allow us to
define the notion of an expression $\Expr$ satisfying a specification
$\Spec$ on a point $\Point$.
Formally, we say that $\Expr \models \Spec \downharpoonleft
\Point$ if $\Expr$ satisfies the specification $\Spec \downharpoonleft
\Point$ given by replacing each clause $C$ in the plainly separable
specification by the formula $\mathit{Pre}_C(\Point) \implies C$.
Here, the premise $\mathit{Pre}_C(\Point)$ is given by $\bigwedge_{p \in
\FormalParameters} \mathit{Arg}_C(p) = \Point[p]$ where $\mathit{Arg}_C$ is the
actual argument corresponding to the formal parameter $p$ in the
unique invocation of $\SynthFun$ that occurs in $C$.
We extend this definition to sets of points as follows: $\Expr \models
\Spec \downharpoonleft \Points \Leftrightarrow \bigwedge_{\Point \in
\Points} \Expr \models \Spec\downharpoonleft\Point$.

\begin{example}
  \label{ex:running:correctness_on_point}
  For the specification $\Spec$ of the running example, the function
  given by $\SynthFun(x, y) \equiv x + y$ is correct on the point $\{ x
      \mapsto 0, y \mapsto 3 \}$ and incorrect on the point $\{ x
  \mapsto 1, y \mapsto 2 \}$
\end{example}

The above restrictions make the SyGuS problem significantly easier.
However, a large number of problems do fall into this class (\arsays{x
out of y benchmarks in the SyGuS 2015 competition}).
Several previous works address this class of problem (see, for
example, \cite{alur-15,madhusudan-16-pw,reynolds-15}).
The following example shows that a number of commonly occurring
specification mores are point-wise specifications.

\subsection{The Enumerative Solver}
\label{sec:enumeration}

\begin{wrapfigure}{l}{0.5\textwidth}
  \begin{minipage}{0.5\textwidth}
    \begin{algorithm}[H]
      \begin{algorithmic}[1]
        \Require Grammar $\Grammar = \tuple { \NonTerminals, \StartSymbol, \Rules }$
        \Require Specification $\Spec$
        \Ensure $\Expr$ s.t.  $\Expr \in \sem{\Grammar} \wedge \Expr \models \Spec$
        \State $\Point \gets \emptyset$ \label{line:basic:init}
        \While { $\True$ }
        \For {$\Expr \in \Call{Enumerate}{\Grammar,\Points}$ }\label{line:basic:enumerate}
        \If { $\Expr \models \Spec \downharpoonleft  \Points$ } \textbf{continue} \EndIf\label{line:basic:concrete_check}
        \State $\CexInput \gets \Verify(\Expr, \Spec)$ \label{line:basic:verify}
        \If { $\CexInput = \bot$ } \Return $\Expr$ \EndIf \label{line:basic:return}
        \State $\Points \gets \Points \cup \CexInput$ \label{line:basic:continue}
        \EndFor
        \EndWhile
      \end{algorithmic}
      \caption{Enumerative Solver}
      \label{algo:basic}
    \end{algorithm}
  \end{minipage}
\end{wrapfigure}
The principal idea behind the enumerative solver is to enumerate all
expressions from the given syntax with some pruning.
Only expressions that are distinct with respect to a set of concrete
input points are enumerated.

The full pseudo-code is given in Algorithm~\ref{algo:basic}.
Initially, the set of points is empty (line~\ref{line:basic:init}).
In each iteration, the algorithm calls the {\tt Enumerate} procedure
which returns a (possibly infinite) list of expressions that are all
distinct with respect to $\Points$ (line~\ref{line:basic:enumerate}).
This $\Expr$ is then verified, first on the set of points
(line~\ref{line:basic:concrete_check}) and then fully
(line~\ref{line:basic:verify}).
If the expression $\Expr$ is correct, it is returned
(line~\ref{line:basic:return}).
Otherwise, we pick a counter-example input point (i.e., an input on
which $\Expr$ is incorrect) and add it to the set of points and repeat
(line~\ref{line:basic:continue}).

For completeness, we recall the {\tt Enumerate} procedure here
(Algorithm~\ref{algo:enumerate}).
\footnote{Note that {\tt Enumerate} is a coprocedure.
  Unfamiliar readers may assume that each call to {\tt
  Enumerate} returns the next expression from an infinite list of
expressions}
It maintains a map $\Productions : \NonTerminals \to
\Powerset{\Theory[\FormalParameters]}$ from non-terminals to
expressions they can produce.
The invariant maintained by the procedure is that every pair of
expressions in $\Productions[\NonTerminal]$ is distinct on $\Points$.

The algorithm starts by first accumulating into
$\Productions[\NonTerminal]$ the expressions that can be produced from
$\NonTerminal$ in one step
(lines~\ref{line:enumerate:level_one_iter}-\ref{line:enumerate:level_one}).
Then, for each possible expression size $\Size$, it attempts to
instantiate each production rule in the grammar with expressions already
generated and stored in $\Productions$, to generate new expressions of
size at most $\Size$.
These newly generated expression are checked for distinctness from
already generated ones, and if so, added to
$\Productions[\NonTerminal]$.
The algorithm returns all the expressions produced from the starting
non-terminal $\StartSymbol$.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Require Grammar $\Grammar = \tuple { \NonTerminals, \StartSymbol, \Rules }$ and a set of points $\Points$
    \Ensure Expressions $\tuple { \Expr_1, \Expr_2, \ldots }$ s.t. $\forall i < j : \vert \Expr_i \vert \leq \vert \Expr_j
    \vert \wedge \exists \Point \in \Points : \Expr_i[\Point] \neq \Expr_j[\Point]$
    \ForAll {$\NonTerminal \in \NonTerminals$} $\Productions[\NonTerminals] \gets \emptyset$ \EndFor
    \ForAll {$(\NonTerminal, \Expr) \in \Rules$}\label{line:enumerate:level_one_iter}
    \If { $\Expr \in \Theory[\FormalParameters]$ }
    $\Productions[\NonTerminal] \gets \Productions[\NonTerminal] \cup \{ \Expr \}$  \label{line:enumerate:level_one}
    \EndIf
    \EndFor
    \State $ \Size \gets 1 $
    \While { $\True$ }
    \ForAll {$(\NonTerminal, \Expr) \in \Rules$}
    \State $(\NonTerminal_1, \ldots, \NonTerminal_n) \gets \mbox{List of non-terminals occurring in $\Expr$ }$
    \ForAll { $(\Expr_1, \ldots, \Expr_n) \in \Productions[\NonTerminal_1] \times \cdots \times \Productions[\NonTerminal_n]$ }
    \State $\Expr^* \gets \Expr[\Expr_1/\NonTerminal_1,\ldots,\NonTerminal_n/\Expr_n]$
    \If { $\vert \Expr^* \vert \leq \Size \wedge \forall \Expr' \in
      \Productions[\NonTerminal] . \exists \Point \in \Points :
    \Expr'[\Point] \neq \Expr^*[\Point] $ }
    \State $\Productions[\NonTerminal] \gets \Productions[\NonTerminal] \cup \Expr^*$
    \If { $\NonTerminal = \StartSymbol$ } \textbf{yield} $\Expr^*$ \EndIf
    \EndIf
    \EndFor
    \EndFor
    \State $\Size \gets \Size + 1$
    \EndWhile
  \end{algorithmic}
  \caption{Enumerating distinct expressions from a grammar}
  \label{algo:enumerate}
\end{algorithm}

\begin{theorem}
  \label{thm:basic_enumeration}
  Given a SyGuS instance $(\Spec, \Grammar)$ with at least one solution
  expression, Algorithm~\ref{algo:basic} terminates and returns the
  smallest solution expression.
\end{theorem}

\begin{wrapfigure}{l}{0.5\textwidth}
\centering
\begin{tikzpicture}
\begin{semilogyaxis}[
width=5cm,
height=5cm,
xlabel=Expression Size,
ylabel=Time to enumerate,
tick align=outside,
grid=both,
xtick = {2, 4, 6, 8, 10},
ytick = {0.0001, 0.01, 1, 100, 10000, 1000000, 10000000}
]
\addplot[mark=*,color=black] table {enumeration.dat};
\end{semilogyaxis}
\end{tikzpicture}
\caption{Scalability of Enumeration}
\label{figure:random_exponential_graph}
\end{wrapfigure}
\paragraph{Features and Limitations}
The enumerative algorithm performs surprisingly well considering its
simplicity on small to medium sized benchmarks
(see~\cite{udupa-transit,udupa-sygus}).
Further, due to the guarantee of Theorem~\ref{thm:basic_enumeration}
that the enumerative approach produces small solutions, the algorithm is
capable of generalizing from specifications that are input-output
examples.

However, enumeration quickly fails to scale with growing size of
solutions.
Figure~\ref{figure:random_exponential_graph} shows the time taken (in seconds) to
generate all distinct expressions up to a given size for the grammar
shown in Figure~\ref{fig:bitvec_grammar}.
As can be seen from the graph, the time taken grows exponentially with
the size.



\section{The Divide-and-Conquer Enumeration Algorithm}
\label{sec:algo}

We first present some necessary definitions and concepts for the divide
and conquer enumeration algorithm.

\paragraph{Conditional Expression Grammars}
We introduce conditional expressions grammars that separate and
expression grammar into two grammars to generate:
\begin{inparaenum}[(a)]
\item the return value expression, and
\item the conditionals that decide which return value is chosen
\end{inparaenum}
These generated return values (terms) and conditionals (predicates) are
then combined using if-then-else conditional operator.
Note that our techniques can be easily extended to use other conditional
operators such as switch statements.

A {\em conditional expression grammar} is a pair of grammars $\tuple{
\Grammar_T, \Grammar_P }$ where:
\begin{inparaenum}[(a)]
\item the {\em term grammar} $\Grammar_T$ is an expression grammar
  generating terms of type $\range(\SynthFun)$; and
\item the {\em predicate grammar} $\Grammar_P$ is an expression
  grammar generating boolean terms.
\end{inparaenum}
The set of expressions $\sem{\tuple{ \Grammar_T, \Grammar_P }}$
generated by the conditional expression grammar $\tuple{ G_T, G_P }$ is
the smallest set of expressions $\Theory[\FormalParameters]$ that
satisfy the following:
\begin{inparaenum}[(a)]
\item $\sem{\Grammar_T} \subseteq \sem{\tuple{ \Grammar_T, \Grammar_P
  }}$, and
\item $\Expr_1, \Expr_2 \in \sem{\tuple{ \Grammar_T, \Grammar_P }}
  \wedge \Pred \in \sem{\Grammar_P} \implies
  \ITE{\Pred}{\Expr_1}{\Expr_2} \in \sem{\tuple{ \Grammar_T, \Grammar_P }}$
\end{inparaenum}

Most commonly occurring SyGuS grammars in practice can be rewritten
as conditional expression grammars in a straightforward and automated
manner.
\arsays{In the \sygus 2015 competition, the grammars for x out of y
benchmarks were decomposable into conditional expression grammars}
However, it is easy to artificially construct a grammar that cannot be
decomposed into a conditional expression grammar.

\begin{example}
  For the running example, the grammar from
  Example~\ref{ex:running:grammar} is easily decomposed into a
  conditional expression grammar $\tuple{\Grammar_T, \Grammar_P}$ as
  follows:
  \begin{inparaenum}[(a)]
  \item the term grammar $\Grammar_T$ contains only the non-terminal
    $T$, and the rules for rewriting $T$.
  \item the predicate grammar $\Grammar_P$ contains the two
    non-terminals $\{ T, C \}$ and the rules associated with them.
  \end{inparaenum}
\end{example}

\paragraph{Decision Trees and Decision Tree Learning}
We use the concept of decision trees from machine learning literature to
model conditional expressions.
Informally, a decision tree $\DecisionTree$ computes a function
$\sem{\DecisionTree}$ that maps samples to {\em labels}.
Each internal node in a decision tree contains an {\em attribute} which
may either hold or not for each sample, and each leaf node contains a
label.
In our setting, labels are terms, attributes are predicates, and samples
are points.

To compute the $\sem{\DecisionTree}$ label for a given point, we follow
a path from the root of the decision tree to a leaf, taking the left
(resp. right) child at each internal node if the attribute holds (resp.
does not hold) for the sample.
The required label is the label at the leaf.

Formally, a {\em decision tree} $\DecisionTree$  is a tuple $\tuple{
\Nodes, (\NodesInternal, \NodesLeaf), \node_0, \Edges, (\EdgesYes,
\EdgesNo), \Attribute, \Label }$ where:
\begin{inparaenum}[(a)]
\item $(\Nodes, \Edges)$ form a rooted binary tree with root node
    $\node_0 \in \Nodes$;
\item The nodes $\Nodes$ are partitioned into a set of internal nodes
    $\NodesInternal$ and leaf nodes $\NodesLeaf$;
\item The attribute function $\Attribute : \NodesInternal \to
    \sem{\Grammar_P}$ maps internal nodes to predicates;
\item The label function $\Label : \NodesLeaf \to \sem{\Grammar_T}$ maps
    leaf nodes to terms;
\item The edges $\Edges$ are partitioned into positive edges $\EdgesYes$
  and negative edges $\EdgesNo$ with each internal node being the source
  of one positive and one negative edge.
  We denote the children of an internal node connected through a
  positive (resp. negative) edge the left (resp. right) child.
\end{inparaenum}

Intuitively, each internal node corresponds to an if-then-else
expression with the conditional given by the attribute of the node, and
each leaf node corresponds to a term given by its label.
Formally, the expression $\DTtoExpr{\DecisionTree}$ corresponding to a
decision tree $\DecisionTree$ is defined as follows:
\begin{inparaenum}[(a)]
\item If $\node_0 \in \NodesLeaf$, i.e., the tree is a single node, then
    $\DTtoExpr{\DecisionTree}$ is $\Label(\node_0)$, and
\item If $\node_0 \in \NodesInternal$, then $\DTtoExpr{\DecisionTree} =
  \ITE{\Attribute(\node_0)}{\DTtoExpr{\node_y}}{\DTtoExpr{\node_n}}$
  where $(\node_0, \node_y) \in \EdgesYes$ and $(\node_0, \node_n) \in \EdgesNo$.
\end{inparaenum}

Decision tree learning is a technique that learns a decision tree from a
given set of samples.
A decision tree learning procedure is given:
\begin{inparaenum}[(a)]
\item a set of samples (points),
\item a set of labels (terms), along with a function that maps a label to the
  subset of samples which it covers; and
\item a set of attributes (predicates).
\end{inparaenum}
A sound decision tree learning algorithm returns a decision tree
$\DecisionTree$ that classifies the points correctly, i.e., for every
sample $\Point$, the label $\sem{\DecisionTree}(\Point)$ covers the
point.
We use the notation {\tt LearnDT} to denote a generic sound
decision tree learning procedure.
The exact procedure we use for decision tree learning in
Section~\ref{sec:decision_trees}.

\subsection{Algorithm}
\label{sec:algo:main}

Algorithm~\ref{algo:main} presents the full divide-and-conquer
enumeration algorithm for synthesis.
Like Algorithm~\ref{algo:basic}, the divide-and-conquer algorithm
maintains a set of points $\Points$, and in each iteration:
\begin{inparaenum}[(a)]
\item computes a candidate solution expression $\Expr$
  (lines~\ref{line:main:start_iter}-\ref{line:main:compute_expr});
\item verifies and returns $\Expr$ if it is correct (lines~\ref{line:main:verify}
  and~\ref{line:main:return}); and
\item otherwise, adds the counter-example point into the set $\Points$
  (line~\ref{line:main:add_point}).
\end{inparaenum}

However, the key differences between Algorithm~\ref{algo:main} and
Algorithm~\ref{algo:basic} are in the way the candidate solution
expression $\Expr$ is generated.
The generation of candidate expressions is separated into two separate
steps.

\paragraph{Term solving.}
Instead of searching for a single candidate expression that is correct
on all points in $\Points$, Algorithm~\ref{algo:main} maintains a set of
candidate terms $\Terms$.
We say that a term $\Term$ covers a point $\Point \in \Points$ if $\Term
\models \Spec \downharpoonleft \Point$.
The set of points that a term covers is computed and stored in
$\Cover[\Term]$ (line~\ref{line:main:compute_cover}).
Note that the algorithm does not store terms that cover the same set of
points as already generated terms
(line~\ref{line:main:duplicate_cover}).
When the set of terms $\Terms$ covers all the points in $\Points$, i.e., for
each $\Point \in \Points$, there is at least one term that is correct on
$\Point$, the term enumeration is stopped.

\paragraph{Unification and Decision Tree Learning.}
In the next step (lines~\ref{line:main:xxx}-\ref{line:main:yyy}), we
generate a set of predicates $\Predicates$ that will be used as
conditionals to combine the terms from $\Terms$ into if-then-else
expressions.
In each iteration, we attempt to learn a decision tree that correctly
label each point $\Point \in \Points$ with a term $\Term$ such that
$\Point \in \Cover[\Term]$.
If such a decision tree $\DecisionTree$ exists, the conditional
expression $\DTtoExpr{\DecisionTree}$ is correct on all points, i.e.,
$\DTtoExpr{\DecisionTree} \models \Spec \downharpoonleft \Points$.
If a decision tree does not exist, we generate additional terms and
predicates and retry.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Require Conditional expression grammar $\Grammar = \tuple{ \Grammar_T, \Grammar_P }$
    \Require Specification $\Spec$
    \Ensure $\Expr$ s.t.  $\Expr \in \sem{\Grammar} \wedge \Expr \models \Spec$
    \State $\Point \gets \emptyset$ \label{line:basic:init}

    \While { $\True$ }
    \State $\Terms \gets \emptyset; \Predicates \gets \emptyset; \Cover \gets \emptyset$ \label{line:main:start_iter}

    \While { $\bigcup_{\Term \in \Terms} \Cover[\Term] \neq \Points$ } \Comment { Term solver }
    \State $\Term \gets \Call{Enumerate}{\Grammar_T,\Points};
    \Cover[\Term] \gets \{ \Point \mid \Point \in \Points \wedge \Term \models \Spec \downharpoonleft \Point \} $\label{line:main:compute_cover}
    \If { $\forall \Term' \in \Terms : \Cover[\Term] \neq \Cover[\Term']$ }
    $\Terms \gets \Terms \cup \{ \Term \}$ \label{line:main:duplicate_cover}
    \EndIf
    \EndWhile

    \While { $\True$ } \Comment { Unifier }
    \State $\DecisionTree \gets \Call{LearnDT}{\Terms, \Predicates}$
    \If  { $\DecisionTree = \bot$ }
    \State $\Terms \gets \Terms \cup \Call{Enumerate}{\Grammar_T,\Points}$ \label{line:main:more_terms}
    \State $\Predicates \gets \Predicates \cup \Call{Enumerate}{\Grammar_P,\Points}$\label{line:main:more_preds}
    \Else
    \State $\Expr \gets \DTtoExpr{\DecisionTree}$ \label{line:main:compute_expr}
    \State \textbf{break}
    \EndIf
    \EndWhile

    \State $\CexInput \gets \Verify(\Expr, \Spec)$ \Comment { Verifier } \label{line:main:verify}
    \If { $\CexInput = \bot$ } \Return $\Expr$ \EndIf \label{line:main:return}
    \State $\Points \gets \Points \cup \CexInput$ \label{line:main:add_point}
    \EndWhile
  \end{algorithmic}
  \caption{The divide-and-conquer enumeration algorithm}
  \label{algo:main}
\end{algorithm}

\begin{theorem}
  Algorithm~\ref{algo:main} is a sound for the \sygus problem.
  Further, assuming a sound and complete {\tt LearnDT} procedure, if
  there exists a solution expression, Algorithm~\ref{algo:main} is
  guaranteed to find it.
\end{theorem}
The proof of the above theorem is similar to the proof of soundness and
partial-completeness for the original enumerative solver.
The only additional assumption is that the {\tt LearnDT} decision tree
learning procedure will return a decision tree if one exists.
We present such a procedure in the next section.

\begin{remark}
  In line~\ref{line:main:more_terms}, we generate additional terms even
  though $\Terms$ is guaranteed to contain terms that cover all points.
  This is required to achieve semi-completeness, i.e., without this, the
  algorithm might not find a solution even if one exists.
  See the appendix for an example.
\end{remark}

\subsection{Decision Tree Learning}
\label{sec:decision_trees}

The standard multi-label decision tree learning algorithm (based on
ID3~\cite{quinlan-86}) is presented in Algorithm~\ref{algo:dt_learning}.
The algorithm first checks if there exists a single label (i.e., term)
$\Term$ that applies to all the points (line~\ref{line:dt:single}).
If so, it returns a decision tree with only one leaf whose label is
$\Term$ (line~\ref{line:dt:leaf}).
Otherwise, it picks the best predicate $\Pred$ to split on based on some
heuristic (line~\ref{line:dt:pick_pred}).
It the recursively computes the left and right sub-trees for the set of
points on which $\Pred$ holds and does not hold, respectively
(lines~\ref{line:dt:left} and~\ref{line:dt:right}).
The final decision tree is returned as a tree with a root (with
attribute $\Pred$), and positive and negative edges to the roots of the
left and right sub-trees, respectively.

\begin{algorithm}
  \begin{algorithmic}[1]
    \Require $\Points$, $\Terms$, $\Cover$, $\Predicates$
    \Ensure Decision tree $\DecisionTree$
    \If { $\exists \Term : \Points \subseteq \Cover[\Term]$ }\label{line:dt:single}
    \Return $\mathit{LeafNode}[\Label \gets \Term]$ \label{line:dt:leaf}
    \EndIf
    \State $\Pred \gets \mbox{Pick predicate from $\Predicates$}$\label{line:dt:pick_pred}
    \State $L \gets \Call{LearnDT}{\{ \Point \mid \Pred[\Point] \},
      \Terms, \Cover, \Predicates }$\label{line:dt:left}
    \State $R \gets \Call{LearnDT}{\{ \Point \mid \neg \Pred[\Point]
      \}, \Terms, \Cover, \Predicates }$\label{line:dt:right}
    \State \Return $\mathit{InternalNode}[\Attribute \gets \Pred,  \mathit{left} \gets L , \mathit{right} \gets R ]$
  \end{algorithmic}
  \caption{Learning Decision Trees}
  \label{algo:dt_learning}
\end{algorithm}

\paragraph{Information-gain heuristic}
The choice of the predicate at line~\ref{line:dt:pick_pred} influences
the size of the decision tree learned by
Algorithm~\ref{algo:dt_learning}, and hence, in our setting, the size of
the solution expression generated by Algorithm~\ref{algo:main}.
We now present the classical information gain heuristic that we use to
pick the predicates to split on.

\textbf{Informally, the information gain heuristic treats the label as a random
variable, and picks the attribute knowing whose value will maximize the
information gained about the variable.}
We do not describe all aspects of computing information gain, but
refer the reader to any standard textbook on machine
learning~\cite{bishop-book}.
Given a set of points $\Points' \subseteq \Points$ the entropy
$H(\Points')$ is defined in terms of the probability
$\Prob{\Points'}{label(\Point) = t}$ of a point $\Point \in \Point'$ being
labeled with the term $\Term$ as
\[
H(\Points') = -\sum_{\Term} \Prob{\Points'}{label(\Point) = \Term} \cdot \log_2{\Prob{\Points'}{label(\Point) = \Term}}
\]
Further, given a predicate $\Pred \in \Predicates$, the information
gain of $\Pred$ is defined as
\[
G(\Pred) = \frac{|\Points_{Y}|}{|\Points|} \cdot
H(\Points_{Y}) + \frac{|\Points_{N}|}{|\Points|} \cdot
H(\Points_{N})
\]
where $\Points_{Y} = \{\Point \in \Points \mid \Pred[\Point]\}$ and
$\Points_{N} = \{\Point \in \Points \mid \neg\Pred[\Point]\}$.
Hence, at line~\ref{line:dt:pick_pred}, we compute the value $G(\Pred)$
for each predicate in $\Predicates$, and pick the one which maximizes
$G(\Pred)$.

We use conditional probabilities $\Prob{\Points'}{label(\Point) = \Term \mid
\Point}$ to compute $\Prob{\Points'}{label(\Point) = t}$.
The assumption we make about the prior distribution is that the
likelihood of a given point $\Point$ being labelled by a given term
$\Term$ is proportional to the number of points in $\Cover[\Term]$.
Formally, we define:
\[
    \Prob{\Points'}{label(\Point) = t \mid \Point} =
    \begin{cases}
        \qquad\qquad\quad 0  & \mathrm{if}\ \Point \notin \Cover[\Term]\\
      \displaystyle
      \frac{\strut\displaystyle\vert \Cover[\Term] \cap \Points' \vert}{\strut\displaystyle\sum_{\Term'
          | \Point \in \Cover[\Term']} \vert \Cover[\Term'] \cap \Points' \vert } & \mathrm{if}\
      \Point \in \Cover[\Term] \\
    \end{cases}
\]
Now, the unconditional probability of an arbitrary point being labelled
with $\Term$ is given by $\Prob{\Points'}{label(\Point) = \Term} = \sum_{\Point}
\Prob{\Points'}{label(\Point) = \Term \mid \Point}\cdot\Prob{\Points'}{\Point}$.
Assuming a uniform distribution for picking points, we have that
\[
    \Prob{\Points'}{label(\Point) = t} =  \frac{1}{\vert \Points \vert} \cdot \sum_{\Point} \Prob{\Points'}{label(\Point) = \Term \mid \Point}
\]


\subsection{Extensions and Optimizations}
\label{sec:optimizations}

We will now discuss several extensions and optimizations for
Algorithm~\ref{algo:main}.


\paragraph{Improving Synthesis Results}
In the presentation from Section~\ref{sec:algo:main}, the algorithm
stops enumeration of terms and predicates as soon as it finds a single
solution to the synthesis problem.
However, there are cases where due to the lack of sufficiently good
predicates, the decision tree and the resulting solution can be large
(see Example~\ref{ex:continuing}).
Instead, we can let the algorithm continue running by generating more
terms and predicates after the first solution is found.
This could lead to different, potentially smaller decision trees and
solutions.
Note that this is in contrast to divide-and-conquer techniques that use
constraint solving to generate predicates~\cite{alur-15,madhusudan-16-pw}.
In general, constraint solvers cannot be tuned to give alternate
solutions.

\begin{example}
  \label{ex:continuing}
  Consider the specification $(x \geq 0 \wedge y \geq 0) \implies (\SynthFun(x, y) = 1
  \Leftrightarrow x + y \leq 2)$ and a run of Algorithm~\ref{algo:main}
  where the terms $0$ and $1$ are generated.
  These terms fully cover any set of points for this specification.

  Over a sequence of iterations the predicates are generated in order of
  size.
  Now, for predicate size $3$, the predicates generated include $x = 0$,
  $x = 1$, $x = 2$, $y \leq 2$, $y \leq 1$, and $y \leq 0$.
  With these predicates, the decision tree depicted in
  Figure~\ref{fig:dt:large} is learned, and the corresponding
  conditional expression is correct for the specification.
  However, if the procedure continues to run after the first solution is
  generated, predicates of size $4$ are generated.
  Among these predicates, the predicate $x + y \leq 2$ is also
  generated.
  With this additional predicate, the decision tree in
  Figure~\ref{fig:dt:small} is generated, leading to the compact
  solution $\SynthFun(x, y) \equiv \mathtt{if}~x + y \leq
  2~\mathtt{then}~1~\mathtt{else}~0$.
\end{example}

\begin{figure}
  \begin{subfigure}{0.5\textwidth}
    \begin{tikzpicture}
      [every tree node/.style={draw}, level distance=18pt,
      level 1/.style={sibling distance=80pt},
      level 2/.style={sibling distance=50pt},
      level 3/.style={sibling distance=25pt}]
      \node[draw] {$x = 0$}
        child {node[draw] {$y \leq 2$}
          child {node[fill=gray,draw] {$1$}}
          child {node[fill=gray,draw] {$0$}}
        }
        child {node[draw] {$x = 1$}
          child {node[draw] {$y \leq 1$}
            child {node[fill=gray,draw] {$1$}}
            child {node[fill=gray,draw] {$0$}}
          }
          child {node[draw] {$x = 2$}
            child {node[draw] {$y \leq 0$}
              child {node[fill=gray,draw] {$1$}}
              child {node[fill=gray,draw] {$0$}}
            }
            child {node[fill=gray,draw] {$0$}}
          }
        }
        ;
    \end{tikzpicture}
    \caption{Decision tree for predicates of size $3$}
    \label{fig:dt:large}
  \end{subfigure}
  \begin{subfigure}{0.5\textwidth}
    \centering
    \begin{tikzpicture}
      \node[draw] {$x + y \leq 2$}
        child {node [fill=gray,draw] {$1$}}
        child {node [fill=gray,draw] {$0$}}
      ;
    \end{tikzpicture}
    \caption{Decision tree for predicates of size $4$}
    \label{fig:dt:small}
  \end{subfigure}
\end{figure}

\paragraph{Decision tree repair}
In Algorithm~\ref{algo:xxx}, we discard the terms and predicates that
are equivalent to already generated terms and predicates on the selected
points $\Points$ in lines~\ref{line:main:xxx},~\ref{line:main:yyy},
and~\ref{line:main:zzz}.
However, these discarded terms and predicates, may lead to better
solutions than the already generated ones.

\begin{example}
  Consider a run of the algorithm on the specification for the running
  example, where the set $\Points$ contains the points $\{ x \mapsto 1,
  y \mapsto 0 \}$ and $\{ x \mapsto -1, y \mapsto 0 \}$.
  Suppose the algorithm first generates the terms $0$ and $1$.
  These terms are each correct on one of the points and are added to
  $\Terms$.
  Next, the algorithm generates the terms $x$ and $y$.
  However, these are not added to $\Terms$ as $x$ (resp. $y$) is correct
  on exactly the same set of points as $1$ (resp. $0$).

  Suppose the algorithm also generates the predicate $x \leq y$ and
  learns the decision tree corresponding to the expression $\Expr \equiv
  \mathtt{if}~x \leq y~\mathtt{then}~0~\mathtt{else}~1$.
  Now, verifying this expression produces a counter-example point, say
  $\{ x \mapsto 1, y \mapsto 2 \}$.
  While the term $0$, and correspondingly, the expression $\Expr$ is
  incorrect on this point, the term $y$ which was discarded as an
  equivalent term to $0$, is correct.
\end{example}

Hence, for a practical implementation of the algorithm we do not discard
these terms and predicates, but store them separately in a map $\Equiv :
\Terms \to \Powerset{\Theory[\FormalParameters]}$ that maps the terms in
$\Terms$ to an additional set of equivalent terms.
At lines~\ref{line:main:xxx},~\ref{line:main:yyy},
and~\ref{line:main:zzz}, if the check for distinctness fails, we
instead add the term $\Term$ to the $\Equiv$ map.
Now, when the decision tree learning algorithm returns an expression
that fails to verify and returns a counter-example, we attempt to
replace terms and predicates in the decision tree with equivalent ones
from the $\Equiv$ map to make the decision tree behave correctly on the
counter-example.

\begin{example}
  Revisiting Example~\ref{ex:dt_repair}, instead of discarding the terms
  $x$ and $y$, we store them into the $\Equiv$ array, i.e., we set
  $\Equiv(0) = \{ y \}$ and $\Equiv(1) = \{ x \}$.
  Now, when the verification of the expression fails, with the
  counter-example point $\{ x \mapsto 1, y \mapsto 2 \}$, we check the
  term that is returned for the counter-example point--here, $0$.
  Now, we whether any term in $\Equiv(0)$ is correct on the
  counter-example point--here, the term $y$.
  If so, we replace the original term with the equivalent term that is
  additionally correct on the counter-example point and proceed with
  verification.
  Replacing $0$ with $y$ in the expression gives us $\mathtt{if}~x \leq
  y~\mathtt{then}~y~\mathtt{else}~1$.
  Another round of verification and decision tree repair will lead to
  replacing the term $1$ with $x$, giving us the final correct solution.
\end{example}

\paragraph{Branch-wise verification}
In Algorithm~\ref{algo:main}, and in most synthesis techniques, an
incorrect candidate solution is used to generate $1$ counter-example
point.
However, in the case of conditional expressions and point-wise
specifications, each branch can be verified separately.
Verifying each branch involves rewriting the specification as in the
point-wise verification defined in Section~\ref{sec:defs} -- but with the
added formula enfor
This gives us two separate advantages:
\begin{compactitem}
\item We are able to generate multiple counter-examples from a single
  incorrect expression.
  This reduces the total number of iterations required, as well as the
  number of calls to the expensive decision tree learning algorithm.
\item It reduces the complexity of each call to the verifier in terms of
  the size of the SMT formula to be checked.
  As verification procedures generally scale exponentially with respect
  to the size of the SMT formula, multiple simpler verification calls
  are often faster than one more complex call.
\end{compactitem}
This optimization works very well along with the decision tree repair
described above as we can verify and repair each branch of the decision
tree separately.

\begin{example}
  Consider the verification of the expression $mathtt{if}~x \leq
  y~\mathtt{then}~0~\mathtt{else}~1$ for the running example.
  Instead of running the full expression through the verifier and
  obtaining one counter-example point, we instead verify the two
  branches separately.
  This involves checking the satisfiability of the formulae $x \leq y
  \wedge f(x, y) = 0 \wedge \neg \Spec$ and $\neg (x \leq y) \wedge f(x,
  y) = 1 \wedge \neg \Spec$.
  This gives us two separate counter-example points as both branches are
  incorrect.
\end{example}

% \paragraph{Other optimizations}
% Bunching\dots
% Anything else?

\section{Evaluation}
\label{sec:evaluation}
% \input{results_one_shot_table}
\input{results_anytime_table}
\input{results_one_shot_max_table}

We built a prototype \sygus solver based on the ideas presented in
this paper. The algorithms for enumeration were implemented in
about 3000 lines of Python code, while the algorithms for decision
tree learning were implemented in about 3000 lines of
C$\raisebox{1pt}{+\!+}$ code. All experiments were executed on a
32-core machine with four Intel Xeon E7-4820 processors running at 2.0
GHz, and 128 GB of memory, with a one hour time limit.

This section first sets up the questions that we would like for our
experimental evaluation to answer, We then briefly describe
the benchmarks used and comment on the state-of-the-art in solving
these benchmarks, to provide some context to the reader.
We then conclude this section with a discussion about our
experimental observations.

\subsection{Goals, Benchmarks and the State-of-the-art}
\paragraph{Goals}
We seek to empirically evaluate how our synthesis algorithm compares
to other state-of-the-art synthesis techniques along the following
dimensions:
\begin{inparaenum}[(a)]
\item
\emph{Performance}: How quickly can the algorithms arrive at a correct
solution?
\item
\emph{Quality}: How \emph{good} are the solutions produced by the
algorithms? We use compactness of solutions, in terms of syntactic
size, as a metric for the quality of solutions.
\item
\emph{Effect of continued execution}: How significant is the
improvement in the quality of the solutions generated if the
algorithm is given an additional (but fixed) time budget.
\end{inparaenum}

\paragraph{Benchmarks}
To compare and contrast the \dcsolve algorithm with contemporary
techniques along each of these dimensions, we considered two classes
of \sygus benchmarks. We did not consider the other classes in the
general-track \sygus benchmarks because:
\begin{inparaenum}[(a)]
\item the solutions were not conditional expressions, as is the case
with the bit-vector benchmarks, or
\item the benchmarks were not plainly separable, or
\item the benchmarks were very similar to the ones that we have
considered.
\end{inparaenum}
The first class that we considered consists of the ICFP
benchmarks. These are bit-twiddling problems, with very specific
restrictions on the bitwise operations that are allowed. The behavior
of the function to be synthesized is specified only using concrete
input-output examples. To the best of our knowledge, no existing
\sygus solver has been able to solve these benchmarks. The results of
running our solver on these benchmarks is summarized in
Table~\ref{table:anytime_results}. The first column of
Table~\ref{table:anytime_results} gives the name of the benchmark, the
second and third columns give the size of the earliest solution
discovered by the algorithm, the time taken to discover the solution,
and the size of the decision tree associated with the solution,
respectively. The next three columns provide the same details, but for
the \emph{minimal} sized solution discovered by the
algorithm. Finally, the last column indicates whether the solution
that was discovered earliest was also the solution with the minimal
size. The second class of benchmarks that we considered compute the
maximum of $n$ integers, where $n$ is a
parameter. Table~\ref{table:max_results} demonstrates how our
algorithm stacks up against the state-of-the-art CVC4 solver. The
columns indicate the sizes of solutions generated, the time to arrive
at the solution, as well as the number of comparisons in the solution
obtained by each of the solvers, for different values of the parameter
$n$, indicated in the first column.

\paragraph{The State-of-the-art}
One can broadly categorize the existing \sygus solvers into two
categories:\begin{inparaenum}[(a)]
\item
\emph{Black-box solvers}, which do not leverage the correctness
specification to guide the search for solutions. Some examples are
\esolver and the stochastic solver~\cite{udupa-sygus}, as well as the
\dcsolve algorithm.
\emph{White-box solvers}, which actively leverage the correctness
specification to generate terms and predicates for solutions. Examples
for white-box solvers include the solvers based on the \textsc{stun}
algorithms~\cite{alur-15}, and the CVC4 solver~\cite{reynolds-15}.
\end{inparaenum}
Empirically, black-box algorithms are able to generalize well from
underconstrained correctness specification, whereas it is not clear
how white-box solvers can generalize from such specifications, given
their heavy reliance on the specifications being complete.
We note that \esolver, which employs an optimized
version of the na\"ive enumerative algorithm discussed earlier won the
2014 \sygus competition and came in second in the 2015 \sygus
competition, losing to the CVC4 solver, which came in first.

\subsection{Discussion}
Having set the context with respect to the benchmarks and contemporary
solvers, we now proceed to a discussion on how the
\dcsolve algorithm compares to contemporary solvers along the three
dimensions described earlier.

\paragraph{Performance}
As Table~\ref{table:anytime_results} demonstrates, our algorithm was
able to produce a solution for 47 out of the 50 ICFP benchmarks,
within an hour. We emphasize that we know of no other solver that has
been able to provide satisfactory solutions to these
benchmarks. Further, as Figure~\ref{figure:random_exponential_graph}
indicates, it would be impossible to explore expressions of the size
indicated in Table~\ref{table:anytime_results} --- which are often
larger than 20 --- using a na\"ive enumeration strategy, such as the
one used by \esolver.  The solutions produced by our divide and
conquer algorithm are quite compact, and most of them are computed in
less than ten minutes, with only five benchmarks taking longer.

Table~\ref{table:max_results} indicates that our algorithm is competitive
with the state-of-the-art CVC4 solver on the parameterized ``max of
$n$ numbers'' benchmarks, for smaller values of $n$. For larger
values of $n$, our algorithm is not as performant. Our investigations
indicated that in the performance drops are largely due to the large
number of concrete counterexample points that the algorithm maintains,
which in turn leads to the decision tree learning algorithms taking
longer.

To summarize, the \dcsolve algorithm demonstrates vastly better
performance than other black-box solvers. Its performance is
competitive with white-box solvers, while also being able to
generalize well from input-output examples as we will now discuss.

\paragraph{Quality of Solutions}
In the case of the ICFP benchmarks, Table~\ref{table:anytime_results}
indicates that most of the solutions that our algorithm generates are
rather compact, and are of a size that are comprehensible to a
human. We do not know if solutions smaller than these exist. The CVC4
solver has been able to synthesize solutions for some of the ICFP
benchmarks, but only when syntactic restrictions are lifted. However,
upon closer inspection, we discovered that the solutions were just a
large case-split corresponding to the input-output examples that form
the constraints for the problem. Although technically correct, the
solutions are extremely large, perform no generalization and are
unlikely to be the intended or desired solutions. We emphasize
that no solution generated by our algorithm is a similar large
conditional expression that matches inputs to outputs. The solutions
that we report are all compact expressions that perform some
computation on the input, to return an output.

In the case of the ``max'' benchmarks, we again see from
Table~\ref{table:max_results} that our algorithm performs well for
small parameter values, often beating the solutions obtained from the
CVC4 solver in terms of size and number of comparisons. At larger
parameter values, the solutions are still reasonably compact and not
excessively large. The reason for the somewhat larger solutions for
larger parameter values is that the decision tree learning algorithm
is not optimal, \ie, it does not guarantee the most compact decision
tree. Instead a greedy heuristic is used, which rapidly yields
suboptimal solutions as the number of samples (counterexample points)
increases, as with the ``max'' benchmarks for larger
parameter values. We intend to explore how we can adapt other
heuristics~\cite{madhusudan-16-pw}, into the setting of our problem, in future
work.

\paragraph{Effect of Continued Execution}
We observe from the last column in Table~\ref{table:anytime_results}
that for about half the ICFP benchmarks, we were able to obtain a more
compact solution by continuing to execute our algorithm even after the
first solution was discovered. Also, the difference in the first and
smallest solutions is sometimes very significant, for example, in the
case of the ``icfp\_95\_100'' benchmark, we see the size of the
solutions go down by almost an order of magnitude, from 220 to 24. An
interesting phenomenon that we observed was that our algorithm would
sometimes find \emph{larger} solutions when allowed to run
longer. However, we observed that the size of the decision trees
almost always reduced with time. It turns out that although the
decision trees learned were smaller, running the algorithm longer also
meant that larger terms and predicates were enumerated, thus leading
to the overall expression sizes of the solutions to sometimes increase
with time.

Our experimental evaluation thus allows us to draw the following
conclusions:
\begin{inparaenum}[(a)]
\item
The \dcsolve algorithm is able to quickly learn compact solutions for
a large fraction of the ICFP benchmarks, and generalizes well from
input-output examples.
\item
The anytime nature of the \dcsolve algorithm is often useful in
reducing the size of the computed solution by allowing additional
execution time.
\item
The \dcsolve algorithm works competently on problems from other
classes of benchmarks that admit conditional expressions as solutions,
such as the parameterized ``max'' benchmarks.
\end{inparaenum}

\section{Concluding Remarks}
\label{sec:conclusion}

\subsection{Related Work}
Program synthesis has seen a revived interest in the last decade,
starting from the \sketch
framework~\cite{solar-lezama-05,solar-lezama-06}. \sketch uses an
unoptimized C program as a correctness specification, and a partial
program with \emph{generator} expressions to specify syntactic
restrictions, and proposed the counterexample guided inductive
synthesis (CEGIS) strategy for synthesis. Almost all synthesis
algorithms proposed in recent literature --- including the one
described in this paper --- can be viewed as an instantiation of the
CEGIS strategy.  More recently, constraint based synthesis approaches
have been applied to synthesize loop-free programs~\cite{jha-10,
gulwani-pldi-11}. Research in synthesis of string manipulating
programs using input-output examples has found applications in
Microsoft's FlashFill~\cite{gulwani-popl-11}, and the ideas have been
generalized for other domains in a meta-synthesis framework called
FlashMeta~\cite{polozov-15}.

In the area of \sygus solvers, solvers based on enumerative
search~\cite{udupa-transit}, stochastic
search~\cite{schkufza-13,udupa-sygus} and symbolic
search~\cite{gulwani-pldi-11} were among the first solvers
developed. The \sketch approach has also been used to develop \sygus
solvers~\cite{jeon-15}. Alchemist~\cite{saha-15} is another solver
that is quite competitive on benchmarks in the linear arithmetic
domains. More recently, white box solvers like the CVC4
solver~\cite{reynolds-15} and the unification based
solver~\cite{alur-15} have also been developed.

The enumerative synthesis algorithm used by
\esolver~\cite{udupa-transit, udupa-sygus} and the work on using decision
trees for piece-wise functions~\cite{madhusudan-16-pw} are perhaps the
most closely related to the work described in this paper. We have
already discussed at length the shortcomings of \esolver that our
algorithm overcomes. The approach for learning piece-wise
functions~\cite{madhusudan-16-pw}, which we refer to as \pwsolve, also
uses decision trees, but differs from our approach in several ways:
\begin{inparaenum}[(a)]
\item
\pwsolve uses a different set of heuristics based on hitting sets,
rather than the cover-based prior distribution that we use,
\item
\pwsolve presents a generic algorithm which uses decision trees to
learn a classifier, as well as thresholds for linear
inequalities. Although presented in a generic manner, the algorithm
has been instantiated only for the domain of linear arithmetic. It is
unclear how the approach can be instantiated for other
domains. \dcsolve on the other hand uses decision trees to strictly
learn a classifier, given some Boolean-valued attributes. Neither the
decision tree learning algorithm, nor the enumeration is
domain-specific, making \dcsolve a generic algorithm that can be
applied on any domain.
\item
Conversely, \pwsolve can easily learn constants in the linear
integer domain. This is something that enumerative approaches,
including \dcsolve struggle to do, often representing a constant like
``42'' as a sum of smaller constants that appear in the grammar.
\item
Finally, it is unclear how \pwsolve can handle syntactic restrictions
in a general manner, whereas \dcsolve can handle syntactic
restrictions of any form, being based on enumerative
approaches.
\end{inparaenum}

Other recent work in the area of program synthesis in general have
used type-theoretic approaches~\cite{gvero-13,osera-15,frankle-16} for
program completion and for generating code snippets. Synthesis of
recursive programs and data structure manipulating code is yet another
application of program synthesis that has been studied in recent
years~\cite{kuncak-10,kneuss-13,albarghouthi-13,feser-15}. Lastly,
decision trees have also been applied to learn program
invariants~\cite{garg-16}.

\subsection{Conclusion}
This paper has presented a new enumerative algorithm to solve
instances of the Syntax-Guided Synthesis (\sygus) problem. The
algorithm overcomes the shortcomings of a na\"ive enumerative
algorithm by using enumeration to only learn small expressions which
are correct on subsets of the inputs. These expressions are then used
to form a conditional expression using Boolean combinations of
enumerated predicates using decision trees. We have demonstrated the
ability of the algorithm to generalize from input-output examples by
solving most of the previously unsolved ICFP benchmarks, which are
part of the \sygus benchmark suite. The algorithm is generic and
competitive with the state-of-the-art techniques in other domains as
well. The algorithm produces compact solutions, and is \emph{anytime}
--- in that continued execution can potentially produce more compact
solutions than the initially discovered solution.


\bibliographystyle{plainnat}
\setlength{\bibsep}{1pt}
\begin{small}
\bibliography{main}
\end{small}

\arsays{Madhu's work assumes that the predicate domain can distinguish
any two points?}

\end{document}
